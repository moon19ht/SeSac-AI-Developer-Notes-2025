# ğŸ“Š Machine Learning ì´ë¡ 

##### ğŸ—“ï¸ 2025.07.08
##### ğŸ“ Writer : Moon19ht

---

## ğŸ“š ëª©ì°¨

1. [íšŒê·€ ë¶„ì„ ëª¨ë¸ ë¹„êµ](#1-íšŒê·€-ë¶„ì„-ëª¨ë¸-ë¹„êµ)
2. [ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ë¹„êµ](#2-ì´ë¯¸ì§€-ë¶„ë¥˜-ëª¨ë¸-ë¹„êµ)
3. [ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ë°ì´í„° ë³€í™˜](#3-ì´ë¯¸ì§€-ì²˜ë¦¬-ë°-ë°ì´í„°-ë³€í™˜)
4. [ì‹¤ì œ í”„ë¡œì íŠ¸: ê½ƒ ë¶„ë¥˜ ì‹œìŠ¤í…œ](#4-ì‹¤ì œ-í”„ë¡œì íŠ¸-ê½ƒ-ë¶„ë¥˜-ì‹œìŠ¤í…œ)
5. [ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„](#5-ì„±ëŠ¥-ë¹„êµ-ë°-ë¶„ì„)
6. [í•µì‹¬ í¬ì¸íŠ¸](#6-í•µì‹¬-í¬ì¸íŠ¸)
7. [ë‹¤ìŒ í•™ìŠµ ê³¼ì œ](#7-ë‹¤ìŒ-í•™ìŠµ-ê³¼ì œ)

---

## 1. íšŒê·€ ë¶„ì„ ëª¨ë¸ ë¹„êµ

### ë°ì´í„°ì…‹: ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡

#### ë°ì´í„° íŠ¹ì„±
- **ìƒ˜í”Œ ìˆ˜**: 442ê°œ
- **íŠ¹ì„± ìˆ˜**: 10ê°œ (ë‚˜ì´, ì„±ë³„, BMI, í˜ˆì••, í˜ˆì²­ ìˆ˜ì¹˜ ë“±)
- **ëª©í‘œ**: 1ë…„ í›„ ë‹¹ë‡¨ë³‘ ì§„í–‰ë„ ì˜ˆì¸¡
- **íŠ¹ì§•**: ëª¨ë“  íŠ¹ì„±ì´ ì •ê·œí™”ë˜ì–´ ì œê³µë¨

#### íŠ¹ì„± ì„¤ëª…
- **age**: ë‚˜ì´
- **sex**: ì„±ë³„
- **bmi**: ì²´ì§ˆëŸ‰ ì§€ìˆ˜
- **bp**: í‰ê·  í˜ˆì••
- **s1**: tc, ì´ í˜ˆì²­ ì½œë ˆìŠ¤í…Œë¡¤
- **s2**: ldl, ì €ë°€ë„ ì§€ë‹¨ë°±ì§ˆ
- **s3**: hdl, ê³ ë°€ë„ ì§€ë‹¨ë°±ì§ˆ
- **s4**: tch, ì´ ì½œë ˆìŠ¤í…Œë¡¤ / HDL ë¹„ìœ¨
- **s5**: ltg, í˜ˆì²­ ì¤‘ì„±ì§€ë°© ë¡œê·¸ ê°’
- **s6**: glu, í˜ˆë‹¹ ìˆ˜ì¹˜

### íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

#### 1. Linear Regression (ì„ í˜• íšŒê·€)
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
```
- **íŠ¹ì§•**: ê°€ì¥ ê¸°ë³¸ì ì¸ íšŒê·€ ëª¨ë¸
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ RÂ² = 0.5304, í…ŒìŠ¤íŠ¸ì…‹ RÂ² = 0.4694
- **ì¥ì **: í•´ì„ì´ ì‰½ê³  ë¹ ë¦„
- **ë‹¨ì **: ë³µì¡í•œ ê´€ê³„ í¬ì°© í•œê³„

#### 2. Ridge Regression (L2 ì •ê·œí™”)
```python
from sklearn.linear_model import Ridge
model = Ridge(alpha=0.1)
```
- **íŠ¹ì§•**: L2 ì •ê·œí™”ë¡œ ê³¼ì í•© ë°©ì§€
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ RÂ² = 0.5225, í…ŒìŠ¤íŠ¸ì…‹ RÂ² = 0.4723
- **ì¥ì **: ê³„ìˆ˜ í¬ê¸° ì œí•œìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: alpha (ì •ê·œí™” ê°•ë„)

#### 3. Lasso Regression (L1 ì •ê·œí™”)
```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=0.1)
```
- **íŠ¹ì§•**: L1 ì •ê·œí™”ë¡œ íŠ¹ì„± ì„ íƒ íš¨ê³¼
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ RÂ² = 0.5202, í…ŒìŠ¤íŠ¸ì…‹ RÂ² = 0.4752
- **ì¥ì **: ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì˜ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¦
- **íŠ¹ì§•**: ìë™ íŠ¹ì„± ì„ íƒ ê¸°ëŠ¥

#### 4. Decision Tree Regressor (ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ íšŒê·€)
```python
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
```
- **íŠ¹ì§•**: ë¹„ì„ í˜• ê´€ê³„ í¬ì°© ê°€ëŠ¥
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ RÂ² = 1.0000, í…ŒìŠ¤íŠ¸ì…‹ RÂ² = -0.0732
- **ë¬¸ì œì **: ì‹¬ê°í•œ ê³¼ì í•© ë°œìƒ
- **ì£¼ì˜**: í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ì´ ìŒìˆ˜ (ë§¤ìš° ìœ„í—˜)

#### 5. Random Forest Regressor (ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€)
```python
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(random_state=0, n_estimators=300, max_depth=3)
```
- **íŠ¹ì§•**: ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ì˜ ì•™ìƒë¸”
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ RÂ² = 0.5810, í…ŒìŠ¤íŠ¸ì…‹ RÂ² = 0.4800
- **ì¥ì **: ê³¼ì í•© ìœ„í—˜ ê°ì†Œ, íŠ¹ì„± ì¤‘ìš”ë„ ì œê³µ
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: n_estimators, max_depth

#### 6. Gradient Boosting Regressor (ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…)
```python
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(random_state=0, n_estimators=10, max_depth=3, learning_rate=0.1)
```
- **íŠ¹ì§•**: ìˆœì°¨ì  í•™ìŠµìœ¼ë¡œ ì˜¤ë¥˜ ë³´ì •
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ RÂ² = 0.5086, í…ŒìŠ¤íŠ¸ì…‹ RÂ² = 0.4397
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: learning_rate (í•™ìŠµ ì†ë„ ì¡°ì ˆ)

#### 7. XGBoost Regressor
```python
from xgboost import XGBRegressor
model = XGBRegressor(random_state=0, n_estimators=10, max_depth=3, learning_rate=0.1)
```
- **íŠ¹ì§•**: ê³ ì„±ëŠ¥ ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… êµ¬í˜„
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ RÂ² = 0.4964, í…ŒìŠ¤íŠ¸ì…‹ RÂ² = 0.4179
- **ì¥ì **: Kaggle ëŒ€íšŒì—ì„œ ìì£¼ ìš°ìŠ¹í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

---

## 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ë¹„êµ

### ë°ì´í„°ì…‹: ì†ê¸€ì”¨ ìˆ«ì ë¶„ë¥˜

#### ë°ì´í„° íŠ¹ì„±
- **ìƒ˜í”Œ ìˆ˜**: 1,797ê°œ (ìˆ«ì 0-9)
- **ì´ë¯¸ì§€ í¬ê¸°**: 8Ã—8 í”½ì…€ (64ê°œ íŠ¹ì„±)
- **ìƒ‰ìƒ**: í‘ë°± (grayscale)
- **ëª©í‘œ**: 0-9 ìˆ«ì ë¶„ë¥˜ (10ê°œ í´ë˜ìŠ¤)

#### ë°ì´í„° ì „ì²˜ë¦¬
```python
# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)
# í›ˆë ¨ ë°ì´í„°: (1257, 64), í…ŒìŠ¤íŠ¸ ë°ì´í„°: (540, 64)
```

### ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

#### 1. Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(solver='lbfgs', max_iter=5000, random_state=0)
```
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ = 1.0000, í…ŒìŠ¤íŠ¸ì…‹ = 0.9685
- **íŠ¹ì§•**: í™•ë¥  ê¸°ë°˜ ì„ í˜• ë¶„ë¥˜
- **ì¥ì **: ë¹ ë¥´ê³  í•´ì„ ê°€ëŠ¥
- **ë‹¤ì¤‘ ë¶„ë¥˜**: One-vs-Rest ë°©ì‹ ì‚¬ìš©

#### 2. K-Nearest Neighbors (KNN)
```python
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)
```
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ = 0.9889, í…ŒìŠ¤íŠ¸ì…‹ = 0.9889
- **íŠ¹ì§•**: ê±°ë¦¬ ê¸°ë°˜ ë¶„ë¥˜
- **ì¥ì **: ë‹¨ìˆœí•˜ê³  ì§ê´€ì 
- **ë‹¨ì **: ê³„ì‚° ë¹„ìš©ì´ ë†’ìŒ

#### 3. Decision Tree (ì˜ì‚¬ê²°ì •íŠ¸ë¦¬)
```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth=5)
```
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ = 0.6866, í…ŒìŠ¤íŠ¸ì…‹ = 0.6796
- **íŠ¹ì§•**: ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
- **ë¬¸ì œì **: ì„±ëŠ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ
- **ê°œì„ **: max_depthë¡œ ê³¼ì í•© ë°©ì§€

#### 4. Random Forest (ëœë¤í¬ë ˆìŠ¤íŠ¸)
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(max_depth=4, n_estimators=100, random_state=0)
```
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ = 0.9387, í…ŒìŠ¤íŠ¸ì…‹ = 0.9370
- **íŠ¹ì§•**: ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ì˜ ì•™ìƒë¸”
- **ì¥ì **: ê³¼ì í•© ë°©ì§€, ì•ˆì •ì  ì„±ëŠ¥

#### 5. Gradient Boosting (ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…)
```python
from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier(max_depth=4, n_estimators=100, random_state=0, learning_rate=0.1)
```
- **ì„±ëŠ¥**: í›ˆë ¨ì…‹ = 1.0000, í…ŒìŠ¤íŠ¸ì…‹ = 0.9574
- **íŠ¹ì§•**: ìˆœì°¨ì  í•™ìŠµìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
- **ì£¼ì˜**: ê³¼ì í•© ê²½í–¥ (í›ˆë ¨ì…‹ 100% ì •í™•ë„)

---

## 3. ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ë°ì´í„° ë³€í™˜

### ê¸°ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬

#### PILì„ ì´ìš©í•œ ì´ë¯¸ì§€ ì½ê¸°
```python
import PIL.Image as pilimg
import numpy as np

# ì´ë¯¸ì§€ ì½ê¸° ë° ë³€í™˜
img = pilimg.open("./img/1.jpg")
pix = np.array(img)  # numpy ë°°ì—´ë¡œ ë³€í™˜
print(f"ì´ë¯¸ì§€ í˜•íƒœ: {pix.shape}")  # (ë†’ì´, ë„ˆë¹„, ì±„ë„)
```

#### ì´ë¯¸ì§€ í˜•íƒœ ì´í•´
- **ì»¬ëŸ¬ ì´ë¯¸ì§€**: (ë†’ì´, ë„ˆë¹„, 3) - RGB 3ì±„ë„
- **í‘ë°± ì´ë¯¸ì§€**: (ë†’ì´, ë„ˆë¹„) - ë‹¨ì¼ ì±„ë„
- **í”½ì…€ ê°’ ë²”ìœ„**: 0-255 (8bit)

### ë‹¤ì¤‘ ì´ë¯¸ì§€ ì²˜ë¦¬

#### í´ë”ë³„ ë°°ì¹˜ ì²˜ë¦¬
```python
import os

path = "./img/animal"
filenameList = os.listdir(path)
imageList = []

for filename in filenameList:
    img = pilimg.open(path + "/" + filename)
    img = img.resize((80, 80))  # í¬ê¸° í†µì¼
    img = np.array(img)
    imageList.append(img)

# NPZ íŒŒì¼ë¡œ ì €ì¥
np.savez("./data/data_animal.npz", data=imageList)
```

### ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° ë³€í™˜

#### ì°¨ì› ë³€í™˜
```python
# 4ì°¨ì› â†’ 2ì°¨ì› ë³€í™˜ (ë¨¸ì‹ ëŸ¬ë‹ìš©)
# ì›ë³¸: (ì´ë¯¸ì§€ê°œìˆ˜, ë†’ì´, ë„ˆë¹„, ì±„ë„)
# ë³€í™˜: (ì´ë¯¸ì§€ê°œìˆ˜, ë†’ì´Ã—ë„ˆë¹„Ã—ì±„ë„)
X_reshaped = np.array(imageList).reshape(len(imageList), -1)
```

#### ì •ê·œí™”
```python
# í”½ì…€ ê°’ì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
X_normalized = X_reshaped / 255.0
```

#### ë¼ë²¨ë§
```python
# í´ë”ëª… ê¸°ë°˜ ìë™ ë¼ë²¨ë§
# daisy=0, dandelion=1, rose=2, sunflower=3, tulip=4
```

---

## 4. ì‹¤ì œ í”„ë¡œì íŠ¸: ê½ƒ ë¶„ë¥˜ ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ ê°œìš”
- **ëª©í‘œ**: 5ê°€ì§€ ê½ƒ ì¢…ë¥˜ ìë™ ë¶„ë¥˜
- **ë°ì´í„°**: ì´ 4,317ê°œ ì´ë¯¸ì§€
- **ê½ƒ ì¢…ë¥˜**: daisy(764ê°œ), dandelion(1052ê°œ), sunflower(733ê°œ), rose(784ê°œ), tulip(984ê°œ)

### ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

#### 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
```python
def makeData(folder, label):
    data = []
    labels = []
    path = "./img/flowers" + "/" + folder
    
    for filename in os.listdir(path):
        kind = imghdr.what(path + "/" + filename)
        if kind in ["gif", "png", "jpg", "jpeg"]:
            img = pilimg.open(path + "/" + filename)
            resize_img = img.resize((80, 80))
            pixel = np.array(resize_img)
            
            if pixel.shape == (80, 80, 3):
                data.append(pixel)
                labels.append(label)
    
    np.savez(f"./data/{folder}.npz", data=data, targets=labels)
```

#### 2. ë°ì´í„° í†µí•©
```python
def loadData():
    # ê° ê½ƒ ì¢…ë¥˜ë³„ NPZ íŒŒì¼ ë¡œë“œ
    daisy = np.load("./data/daisy.npz")
    dandelion = np.load("./data/dandelion.npz")
    # ... ë‹¤ë¥¸ ê½ƒë“¤ë„ ë¡œë“œ
    
    # ëª¨ë“  ë°ì´í„° í†µí•©
    data = np.concatenate((daisy["data"], dandelion["data"], ...))
    target = np.concatenate((daisy["targets"], dandelion["targets"], ...))
    
    return data, target
```

#### 3. ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ
```python
# ì°¨ì› ì¶•ì†Œ ë° ì •ê·œí™”
data = data.reshape(data.shape[0], 80*80*3)  # (4317, 19200)
data = data / 255.0  # ì •ê·œí™”

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.5)

# KNN ëª¨ë¸ í•™ìŠµ
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
```

### ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼

#### ê¸°ë³¸ ì„±ëŠ¥
- **í›ˆë ¨ì…‹ ì •í™•ë„**: 45.32%
- **í…ŒìŠ¤íŠ¸ì…‹ ì •í™•ë„**: 32.93%
- **ë¬¸ì œì **: ê³¼ì í•© ë°œìƒ (ì„±ëŠ¥ ì°¨ì´ê°€ í¼)

#### Kê°’ ìµœì í™” ê²°ê³¼
| Kê°’ | í…ŒìŠ¤íŠ¸ì…‹ ì •í™•ë„ |
|-----|----------------|
| 3   | 32.10%         |
| 5   | 32.93%         |
| 7   | **34.09%**     |
| 9   | 34.04%         |
| 11  | 33.67%         |

- **ìµœì  Kê°’**: 7 (ì •í™•ë„ 34.09%)

#### ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥
| ê½ƒ ì¢…ë¥˜     | Precision | Recall | F1-Score |
|-------------|-----------|---------|----------|
| daisy       | 0.18      | 0.07    | 0.10     |
| dandelion   | 0.30      | 0.86    | 0.44     |
| sunflower   | 0.54      | 0.34    | 0.42     |
| rose        | 0.45      | 0.14    | 0.22     |
| tulip       | 0.47      | 0.09    | 0.15     |

---

## 5. ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„

### íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„

| ìˆœìœ„ | ëª¨ë¸ëª…               | í…ŒìŠ¤íŠ¸ì…‹ RÂ² | íŠ¹ì§•                    |
|------|---------------------|-------------|-------------------------|
| 1    | Random Forest       | 0.4800      | ì•™ìƒë¸”, ê³¼ì í•© ë°©ì§€      |
| 2    | Lasso              | 0.4752      | L1 ì •ê·œí™”, íŠ¹ì„± ì„ íƒ     |
| 3    | Ridge              | 0.4723      | L2 ì •ê·œí™”, ì•ˆì •ì„±       |
| 4    | Linear Regression  | 0.4694      | ê¸°ë³¸ ëª¨ë¸               |
| 5    | Gradient Boosting  | 0.4397      | ìˆœì°¨ì  í•™ìŠµ             |
| 6    | XGBoost            | 0.4179      | ê³ ì„±ëŠ¥ ë¶€ìŠ¤íŒ…           |
| 7    | Decision Tree      | -0.0732     | ì‹¬ê°í•œ ê³¼ì í•©           |

### ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„

| ìˆœìœ„ | ëª¨ë¸ëª…               | í…ŒìŠ¤íŠ¸ì…‹ ì •í™•ë„ | íŠ¹ì§•                    |
|------|---------------------|----------------|-------------------------|
| 1    | KNN                | 98.89%         | ê±°ë¦¬ ê¸°ë°˜, ì•ˆì •ì         |
| 2    | Logistic Regression| 96.85%         | í™•ë¥  ê¸°ë°˜, ë¹ ë¦„         |
| 3    | Gradient Boosting  | 95.74%         | ìˆœì°¨ì  í•™ìŠµ             |
| 4    | Random Forest      | 93.70%         | ì•™ìƒë¸” ë°©ë²•             |
| 5    | Decision Tree      | 67.96%         | ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜          |

### ì£¼ìš” ê´€ì°° ì‚¬í•­

#### íšŒê·€ ë¶„ì„
- **Random Forest**ê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥
- **Decision Tree**ëŠ” ê³¼ì í•©ìœ¼ë¡œ ì‹¤íŒ¨
- **ì •ê·œí™” ê¸°ë²•**(Ridge, Lasso)ì´ ì•ˆì •ì 
- **ì•™ìƒë¸” ë°©ë²•**ì´ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜

#### ì´ë¯¸ì§€ ë¶„ë¥˜
- **KNN**ì´ ìµœê³  ì„±ëŠ¥ (ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì )
- **ì†ê¸€ì”¨ ìˆ«ì**ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ë¬¸ì œ
- **ê½ƒ ë¶„ë¥˜**ëŠ” í›¨ì”¬ ì–´ë ¤ìš´ ë¬¸ì œ (34% vs 99%)

---

## 6. í•µì‹¬ í¬ì¸íŠ¸

### íšŒê·€ vs ë¶„ë¥˜

#### íšŒê·€ ë¶„ì„ íŠ¹ì§•
- **ëª©í‘œ**: ì—°ì†ì ì¸ ìˆ˜ì¹˜ ì˜ˆì¸¡
- **í‰ê°€ ì§€í‘œ**: RÂ², MSE, MAE
- **í•´ì„**: ê²°ì •ê³„ìˆ˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
- **ì£¼ì˜**: ìŒìˆ˜ RÂ²ëŠ” ë§¤ìš° ìœ„í—˜í•œ ì‹ í˜¸

#### ë¶„ë¥˜ ë¶„ì„ íŠ¹ì§•
- **ëª©í‘œ**: ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡
- **í‰ê°€ ì§€í‘œ**: ì •í™•ë„, Precision, Recall, F1-Score
- **ë‹¤ì¤‘ ë¶„ë¥˜**: ì—¬ëŸ¬ í´ë˜ìŠ¤ ë™ì‹œ ì²˜ë¦¬
- **ë¶ˆê· í˜•**: í´ë˜ìŠ¤ë³„ ë°ì´í„° ê°œìˆ˜ ì°¨ì´ ì£¼ì˜

### ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬

#### ì „ì²˜ë¦¬ ì¤‘ìš”ì„±
1. **í¬ê¸° í†µì¼**: ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ í¬ê¸°ë¡œ ì¡°ì •
2. **ì •ê·œí™”**: í”½ì…€ ê°’ì„ 0-1 ë²”ìœ„ë¡œ ë³€í™˜
3. **ì°¨ì› ë³€í™˜**: 4ì°¨ì› â†’ 2ì°¨ì› (ë¨¸ì‹ ëŸ¬ë‹ìš©)
4. **ë°ì´í„° í˜•ì‹**: NPZ íŒŒì¼ë¡œ íš¨ìœ¨ì  ì €ì¥

#### ì„±ëŠ¥ í–¥ìƒ ë°©ë²•
- **í•´ìƒë„ ì¦ê°€**: 80Ã—80 â†’ ë” ë†’ì€ í•´ìƒë„
- **ë°ì´í„° ì¦ê°•**: íšŒì „, ë’¤ì§‘ê¸°, ë°ê¸° ì¡°ì •
- **ë”¥ëŸ¬ë‹ í™œìš©**: CNN ëª¨ë¸ ì‚¬ìš©
- **ì „ì´ í•™ìŠµ**: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í™œìš©

### ê³¼ì í•© íƒì§€ ë° ë°©ì§€

#### ê³¼ì í•© ì‹ í˜¸
- í›ˆë ¨ì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ ì°¨ì´ê°€ í¼
- í›ˆë ¨ì…‹ ì„±ëŠ¥ì´ ë¹„í˜„ì‹¤ì ìœ¼ë¡œ ë†’ìŒ (100%)
- í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë‚®ìŒ

#### ë°©ì§€ ë°©ë²•
1. **ì •ê·œí™”**: Ridge (L2), Lasso (L1)
2. **ì•™ìƒë¸”**: Random Forest, Gradient Boosting
3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œí•œ**: max_depth, min_samples_leaf
4. **êµì°¨ ê²€ì¦**: K-Fold Cross Validation
5. **ë” ë§ì€ ë°ì´í„°**: ë°ì´í„° ìˆ˜ì§‘ ë˜ëŠ” ì¦ê°•

---

## 7. ë‹¤ìŒ í•™ìŠµ ê³¼ì œ

### ë‹¨ê¸° ê³¼ì œ (1-2ì£¼)
1. **ë” ë§ì€ ì•Œê³ ë¦¬ì¦˜ ì‹œë„**
   - SVM (Support Vector Machine)
   - Naive Bayes
   - Neural Networks

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**
   - GridSearchCV í™œìš©
   - RandomizedSearchCV ì‚¬ìš©
   - Optunaë¥¼ ì´ìš©í•œ ë² ì´ì§€ì•ˆ ìµœì í™”

3. **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§**
   - PCAë¥¼ ì´ìš©í•œ ì°¨ì› ì¶•ì†Œ
   - LDAë¥¼ ì´ìš©í•œ íŠ¹ì„± ì„ íƒ
   - ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ (HOG, SIFT)

### ì¤‘ê¸° ê³¼ì œ (1ê°œì›”)
1. **ë”¥ëŸ¬ë‹ ë„ì…**
   - CNN (Convolutional Neural Network) í•™ìŠµ
   - TensorFlow/PyTorch ì‚¬ìš©ë²•
   - ì „ì´ í•™ìŠµ (Transfer Learning)

2. **ì‹¤ì œ í”„ë¡œì íŠ¸ í™•ì¥**
   - ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ (Flask/Django)
   - ëª¨ë°”ì¼ ì•± ì—°ë™
   - ì‹¤ì‹œê°„ ì´ë¯¸ì§€ ë¶„ë¥˜

3. **ì„±ëŠ¥ ê°œì„ **
   - ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©
   - ì•™ìƒë¸” ë°©ë²• ì‹¬í™”
   - ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìµœì í™”

### ì¥ê¸° ê³¼ì œ (3ê°œì›”)
1. **ì „ë¬¸ ë¶„ì•¼ íŠ¹í™”**
   - ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„
   - ììœ¨ì£¼í–‰ ê°ì²´ ì¸ì‹
   - ìì—°ì–´ ì²˜ë¦¬ì™€ ê²°í•©

2. **MLOps êµ¬ì¶•**
   - ëª¨ë¸ ë°°í¬ ìë™í™”
   - ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•
   - A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„

3. **ì—°êµ¬ ë° ë…¼ë¬¸**
   - ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ
   - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
   - í•™íšŒ ë°œí‘œ ì¤€ë¹„
