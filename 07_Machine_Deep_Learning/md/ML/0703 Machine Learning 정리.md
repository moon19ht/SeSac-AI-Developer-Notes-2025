# ğŸ“Š Machine Learning ì´ë¡ 

##### ğŸ—“ï¸ 2025.07.03
##### ğŸ“ Writer : Moon19ht

---

## ğŸ“š ëª©ì°¨

1. [ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬](#ëˆ„ë½-ë°ì´í„°-ì²˜ë¦¬)
2. [ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬](#ì¤‘ë³µ-ë°ì´í„°-ì²˜ë¦¬)
3. [ë°ì´í„° ì •ê·œí™”](#ë°ì´í„°-ì •ê·œí™”)
4. [ë°ì´í„° íƒ€ì… ë³€í™˜](#ë°ì´í„°-íƒ€ì…-ë³€í™˜)
5. [êµ¬ê°„ ë‚˜ëˆ„ê¸°](#êµ¬ê°„-ë‚˜ëˆ„ê¸°)
6. [ì›í•« ì¸ì½”ë”©](#ì›í•«-ì¸ì½”ë”©)
7. [ì‹¤ìŠµ ì˜ˆì œ](#ì‹¤ìŠµ-ì˜ˆì œ)
8. [í•µì‹¬ ìš”ì•½](#í•µì‹¬-ìš”ì•½)

---

## ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬

### ëˆ„ë½ ë°ì´í„°ì˜ ì˜í–¥
ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì—ì„œ ë°ì´í„° ëˆ„ë½ì€ ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë³´ë‹¤ ì˜ˆì¸¡ë ¥ì´ ë†’ê³  ì‹ ë¢°ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ëˆ„ë½ëœ ë°ì´í„°ì— ëŒ€í•œ ì ì ˆí•œ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

### ëˆ„ë½ ë°ì´í„° í™•ì¸ í•¨ìˆ˜

#### 1. isnull() í•¨ìˆ˜
ë°ì´í„°ê°€ NaNì´ë©´ True, ì•„ë‹ˆë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
import pandas as pd
import numpy as np

# NaNì€ numpyë¥¼ ì‚¬ìš©í•´ ì§ì ‘ ì…ë ¥ ê°€ëŠ¥
s = pd.Series([1, 2, 3, 4, np.nan])
print(s.isnull())
```

#### 2. sum() í•¨ìˆ˜ì™€ ì¡°í•©
isnull() ê²°ê³¼ì— sum() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ NaN ê°’ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
print("NaN ê°’ ê°œìˆ˜:", s.isnull().sum())
```

### ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ë°©ë²•

#### 1. ì‚­ì œ ë°©ë²• (dropna)
```python
# íŒŒì¼ëª…: exam11_1.py
import pandas as pd

data = pd.read_csv("./data/data.csv")
print("ì»¬ëŸ¼ëª…:", data.columns)
print("ì¸ë±ìŠ¤:", data.index)
print(data.info())

# ëˆ„ë½ëœ ë°ì´í„° í™•ì¸
print(data['height'].value_counts(dropna=False))
print(data['height'].isnull())    # NaNì´ë©´ True
print(data['height'].notnull())   # NaNì´ ì•„ë‹ˆë©´ True

# null ê°’ ê°œìˆ˜ í™•ì¸
print("height í•„ë“œ NaN ê°œìˆ˜:", data['height'].isnull().sum())

# ë°ì´í„° ì‚­ì œ
print("ì‚­ì œ ì „ ë°ì´í„° ê°œìˆ˜:", data.shape)
data = data.dropna(subset=['height'], how='any', axis=0)
print("ì‚­ì œ í›„ ë°ì´í„° ê°œìˆ˜:", data.shape)

# ì¸ë±ìŠ¤ ì¬ì„¤ì •
data = data.reset_index(drop=True)
print(data)
```

#### dropna í•¨ìˆ˜ ì£¼ìš” ë§¤ê°œë³€ìˆ˜
- `subset`: íŠ¹ì • í•„ë“œë“¤ì—ì„œ NaN ê°’ì´ ìˆëŠ” í–‰ ì‚­ì œ
- `how`: 
  - `'any'`: í•˜ë‚˜ë¼ë„ NaNì´ ìˆìœ¼ë©´ ì‚­ì œ
  - `'all'`: ëª¨ë‘ NaNì¼ ë•Œë§Œ ì‚­ì œ
- `axis`: 
  - `0`: í–‰ ì‚­ì œ
  - `1`: ì—´ ì‚­ì œ
- `thresh`: ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ NaNì´ ì—†ëŠ” í–‰/ì—´ ì‚­ì œ
- `inplace`: Trueë¡œ ì„¤ì •í•˜ë©´ ì›ë³¸ ë°ì´í„° ìˆ˜ì •

#### 2. ëŒ€ì²´ ë°©ë²• (fillna)
ëˆ„ë½ ë°ì´í„°ê°€ ë§ì•„ì„œ ì‚­ì œ ì‹œ ë¶„ì„ì´ ì–´ë ¤ìš´ ê²½ìš°, ëŒ€ì²´ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ë°©ë²•ì…ë‹ˆë‹¤.

```python
# íŒŒì¼ëª…: exam11_2.py
import pandas as pd

data = pd.read_csv("./data/data.csv")

# ëˆ„ë½ ë°ì´í„° í™•ì¸
print("height í•„ë“œ NaN ê°œìˆ˜:", data['height'].isnull().sum())
print("weight í•„ë“œ NaN ê°œìˆ˜:", data['weight'].isnull().sum())

# í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
mean_height = data['height'].mean()
mean_weight = data['weight'].mean()

data['height'].fillna(mean_height, inplace=True)
data['weight'].fillna(mean_weight, inplace=True)

print("ëŒ€ì²´ í›„ height í•„ë“œ NaN ê°œìˆ˜:", data['height'].isnull().sum())
print("ëŒ€ì²´ í›„ weight í•„ë“œ NaN ê°œìˆ˜:", data['weight'].isnull().sum())
print(data)
```

#### ëŒ€ì²´ê°’ ì„ íƒ ê¸°ì¤€
- **í‰ê· ê°’**: ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš°
- **ì¤‘ê°„ê°’**: ì´ìƒì¹˜ê°€ ë§ê±°ë‚˜ í¸í–¥ëœ ë¶„í¬ë¥¼ ê°€ì§„ ê²½ìš°
- **ìµœë¹ˆê°’**: ë²”ì£¼í˜• ë°ì´í„°ì˜ ê²½ìš°

---

## ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬

### ì¤‘ë³µ ë°ì´í„°ì˜ ë¬¸ì œ
- ë¶„ì„ ê²°ê³¼ ì™œê³¡
- ëª¨ë¸ ì„±ëŠ¥ ì €í•˜
- ì²˜ë¦¬ ì‹œê°„ ì¦ê°€

### ì¤‘ë³µ ë°ì´í„° í™•ì¸ ë° ì œê±°

```python
# íŒŒì¼ëª…: exam11_3.py
import pandas as pd

data = {
    'passenger_code': ['A101', 'A102', 'A103', 'A101', 'A104', 'A101', 'A103'],
    'target': ['ê´‘ì£¼', 'ì„œìš¸', 'ë¶€ì‚°', 'ê´‘ì£¼', 'ëŒ€êµ¬', 'ê´‘ì£¼', 'ë¶€ì‚°'],
    'price': [25000, 27000, 45000, 25000, 35000, 27000, 45000]
}

df = pd.DataFrame(data)
print("ì›ë³¸ ë°ì´í„°:")
print(df)

# ì¤‘ë³µ ë°ì´í„° í™•ì¸
print("\nì¤‘ë³µ ë°ì´í„° í™•ì¸:")
duplicated_data = df['passenger_code'].duplicated()
print(duplicated_data)

# ì „ì²´ í–‰ì´ ì¤‘ë³µì¸ ê²½ìš° ì œê±°
print("\nì „ì²´ í–‰ ì¤‘ë³µ ì œê±°:")
df2 = df.drop_duplicates()
print(df2)

# íŠ¹ì • ì»¬ëŸ¼ê°’ì´ ì¤‘ë³µì¸ ê²½ìš° ì œê±°
print("\níŠ¹ì • ì»¬ëŸ¼ ì¤‘ë³µ ì œê±°:")
df3 = df.drop_duplicates(subset=['passenger_code'])
print(df3)

# ë‘ ê°œ ì»¬ëŸ¼ ì¡°í•©ì´ ì¤‘ë³µì¸ ê²½ìš° ì œê±°
print("\në‘ ì»¬ëŸ¼ ì¡°í•© ì¤‘ë³µ ì œê±°:")
df4 = df.drop_duplicates(subset=['passenger_code', 'target'])
print(df4)
```

### ì¤‘ë³µ ì²˜ë¦¬ í•¨ìˆ˜
- `duplicated()`: ì¤‘ë³µ ì—¬ë¶€ í™•ì¸ (ì²« ë²ˆì§¸ëŠ” False, ë‚˜ë¨¸ì§€ëŠ” True)
- `drop_duplicates()`: ì¤‘ë³µ í–‰ ì œê±°
- `subset`: íŠ¹ì • ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°

---

## ë°ì´í„° ì •ê·œí™”

### ì •ê·œí™”ì˜ í•„ìš”ì„±
ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì—ì„œ ë°ì´í„°ì˜ ë‹¨ìœ„ë‚˜ ë²”ìœ„ê°€ í¬ê²Œ ë‹¤ë¥¼ ê²½ìš° ì˜ˆì¸¡ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ê°™ì€ ìŠ¤ì¼€ì¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì •ê·œí™” ê³µì‹
```
ì •ê·œí™” ê°’ = (ì›ë³¸ ê°’ - ìµœì†Ÿê°’) / (ìµœëŒ“ê°’ - ìµœì†Ÿê°’)
```

### ì •ê·œí™” ë° ë‹¨ìœ„ ë³€í™˜ ì˜ˆì œ

```python
# íŒŒì¼ëª…: exam11_4.py
import pandas as pd

data = pd.read_csv('./data/auto-mpg.csv')
print(data.info())
print(data.head())

# ì»¬ëŸ¼ëª… ë³€ê²½
data.columns = ['mpg', 'cyl', 'disp', 'power', 'weight', 'acce', 'model']
print(data.head())

# ì •ê·œí™” ì ìš©
data['mpg2'] = (data['mpg'] - data['mpg'].min()) / (data['mpg'].max() - data['mpg'].min())
print(data.head())

# ë‹¨ìœ„ í™˜ì‚° (MPGë¥¼ KPLë¡œ ë³€í™˜)
mpg_unit = 1.60934 / 3.78541  # ë§ˆì¼ì„ kmë¡œ, ê°¤ëŸ°ì„ ë¦¬í„°ë¡œ
data['kpl'] = (data['mpg'] * mpg_unit).round(2)
print(data.head())
```

### ì •ê·œí™” ë°©ë²•
1. **Min-Max ì •ê·œí™”**: 0~1 ì‚¬ì´ë¡œ ë³€í™˜
2. **Z-score ì •ê·œí™”**: í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜
3. **Robust ì •ê·œí™”**: ì¤‘ê°„ê°’ê³¼ IQR ì‚¬ìš©

---

## ë°ì´í„° íƒ€ì… ë³€í™˜

### íƒ€ì… ë³€í™˜ì˜ í•„ìš”ì„±
- ìˆ«ìí˜• ë°ì´í„°ê°€ ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°
- ë¬¸ìì—´ ë°ì´í„°ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•˜ëŠ” ê²½ìš°
- ì˜ëª»ëœ ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš°

### íƒ€ì… ë³€í™˜ ì˜ˆì œ

```python
# íŒŒì¼ëª…: exam11_5.py
import pandas as pd
import numpy as np

data = pd.read_csv('./data/auto-mpg.csv')
data.columns = ['mpg', 'cyl', 'disp', 'power', 'weight', 'acce', 'model']

print("ë°ì´í„° íƒ€ì… í™•ì¸:")
print(data.dtypes)
print("ê³ ìœ ê°’ í™•ì¸:")
print(data['disp'].unique())

# ì˜ëª»ëœ ë°ì´í„°ë¥¼ NaNìœ¼ë¡œ ë³€í™˜
data['disp'].replace('?', np.nan, inplace=True)
print("ì˜ëª»ëœ ë°ì´í„° ë³€í™˜ í›„:")
print(data.head())

# NaN í¬í•¨ í–‰ ì‚­ì œ
data.dropna(subset=['disp'], axis=0, inplace=True)

# íƒ€ì… ë³€í™˜
print("ë³€í™˜ ì „ íƒ€ì…:")
print(data.dtypes)

data['disp'] = data['disp'].astype('float')
data['model'] = data['model'].astype('category')

print("ë³€í™˜ í›„ íƒ€ì…:")
print(data.dtypes)
```

### ì£¼ìš” íƒ€ì… ë³€í™˜ í•¨ìˆ˜
- `astype()`: ë°ì´í„° íƒ€ì… ë³€í™˜
- `replace()`: íŠ¹ì • ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ëŒ€ì²´
- `'category'`: ë²”ì£¼í˜• ë°ì´í„°ë¡œ ë³€í™˜

---

## êµ¬ê°„ ë‚˜ëˆ„ê¸°

### êµ¬ê°„ ë‚˜ëˆ„ê¸°ì˜ í•„ìš”ì„±
ì—°ì†ì  ë°ì´í„°ë¥¼ ë¶ˆì—°ì† ë°ì´í„°ë¡œ ë³€í™˜í•´ì•¼ í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤:
- ë“±ê¸‰ ì‹œìŠ¤í…œ êµ¬ì¶•
- ë²”ì£¼ë³„ ë¶„ì„
- ë³µì¡í•œ ì—°ì† ë°ì´í„° ë‹¨ìˆœí™”

### êµ¬ê°„ ë‚˜ëˆ„ê¸° ì˜ˆì œ

```python
# íŒŒì¼ëª…: exam11_6.py
import pandas as pd
import numpy as np

data = pd.read_csv('./data/auto-mpg.csv')
data.columns = ['mpg', 'cyl', 'disp', 'power', 'weight', 'acce', 'model']

# ë°ì´í„° ì „ì²˜ë¦¬
data['disp'].replace('?', np.nan, inplace=True)
data.dropna(subset=['disp'], axis=0, inplace=True)
data['disp'] = data['disp'].astype('float')
data['model'] = data['model'].astype('category')

# power í•„ë“œ ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬
print("power í•„ë“œ NaN ê°œìˆ˜:", data['power'].isnull().sum())
data.dropna(subset=['power'], inplace=True)

# êµ¬ê°„ ë‚˜ëˆ„ê¸°
count, bin_dividers = np.histogram(data['power'], bins=4)
print("êµ¬ê°„ ê²½ê³„ê°’:", bin_dividers)

bin_names = ["D", "C", "B", "A"]
data["grade"] = pd.cut(x=data['power'], 
                       bins=bin_dividers,
                       labels=bin_names, 
                       include_lowest=True)

print("êµ¬ê°„ ë‚˜ëˆ„ê¸° ê²°ê³¼:")
print(data[['power', 'grade']].head(10))
```

### êµ¬ê°„ ë‚˜ëˆ„ê¸° í•¨ìˆ˜
- `np.histogram()`: êµ¬ê°„ ê²½ê³„ê°’ ê³„ì‚°
- `pd.cut()`: ì—°ì† ë°ì´í„°ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
- `include_lowest=True`: ìµœì†Ÿê°’ í¬í•¨
- `bins`: êµ¬ê°„ ê°œìˆ˜ ë˜ëŠ” ê²½ê³„ê°’ ë°°ì—´
- `labels`: ê° êµ¬ê°„ì— ë¶™ì¼ ë¼ë²¨ëª…

---

## ì›í•« ì¸ì½”ë”©

### ì›í•« ì¸ì½”ë”©ì˜ í•„ìš”ì„±
ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì›í•« ì¸ì½”ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ì›í•« ì¸ì½”ë”© ê°œë…
4ê°œì˜ ì¹´í…Œê³ ë¦¬ A, B, C, Dê°€ ìˆì„ ë•Œ:
- A: [1, 0, 0, 0]
- B: [0, 1, 0, 0]
- C: [0, 0, 1, 0]
- D: [0, 0, 0, 1]

### ì›í•« ì¸ì½”ë”© êµ¬í˜„

```python
# íŒŒì¼ëª…: exam11_7.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder

# ì´ì „ ì˜ˆì œì—ì„œ êµ¬ê°„ ë‚˜ëˆ„ê¸° ì™„ë£Œëœ ë°ì´í„° ì‚¬ìš©
data = pd.read_csv('./data/auto-mpg.csv')
data.columns = ['mpg', 'cyl', 'disp', 'power', 'weight', 'acce', 'model']

# ë°ì´í„° ì „ì²˜ë¦¬ (ì´ì „ ë‹¨ê³„ë“¤)
data['disp'].replace('?', np.nan, inplace=True)
data.dropna(subset=['disp'], axis=0, inplace=True)
data['disp'] = data['disp'].astype('float')
data.dropna(subset=['power'], inplace=True)

# êµ¬ê°„ ë‚˜ëˆ„ê¸°
count, bin_dividers = np.histogram(data['power'], bins=4)
bin_names = ["D", "C", "B", "A"]
data["grade"] = pd.cut(x=data['power'], 
                       bins=bin_dividers,
                       labels=bin_names, 
                       include_lowest=True)

# ì›í•« ì¸ì½”ë”©
Y_class = np.array(data['grade']).reshape(-1, 1)
enc = OneHotEncoder()
enc.fit(Y_class)
Y_class_onehot = enc.transform(Y_class).toarray()

print("ì›í•« ì¸ì½”ë”© ê²°ê³¼:")
print(Y_class_onehot[:10])

# ì›í•« ì¸ì½”ë”© ë³µì›
Y_class_recovery = np.argmax(Y_class_onehot, axis=1).reshape(-1, 1)
print("ë³µì›ëœ ì¸ë±ìŠ¤:")
print(Y_class_recovery[:10])
```

### ì›í•« ì¸ì½”ë”© ê³¼ì •
1. **reshape(-1, 1)**: 1ì°¨ì› ë°°ì—´ì„ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜
2. **OneHotEncoder()**: ì¸ì½”ë” ê°ì²´ ìƒì„±
3. **fit()**: ì¸ì½”ë” í›ˆë ¨
4. **transform()**: ì›í•« ì¸ì½”ë”© ìˆ˜í–‰
5. **toarray()**: í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ í–‰ë ¬ë¡œ ë³€í™˜

### ì›í•« ì¸ì½”ë”© ë³µì›
- `np.argmax()`: ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤ ë°˜í™˜
- `axis=1`: í–‰ ë°©í–¥ìœ¼ë¡œ ìµœëŒ“ê°’ ì°¾ê¸°

---

## ì‹¤ìŠµ ì˜ˆì œ

### ê³¼ì œ: Iris ë°ì´í„°ì…‹ ì¢…í•© ì²˜ë¦¬
`data/iris.csv` íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. ëˆ„ë½ëœ ë°ì´í„° í™•ì¸ ë° í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
2. sepal.length, sepal.width, petal.width í•„ë“œ ì •ê·œí™”
3. petal.length í•„ë“œë¥¼ 3ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ A, B, C ë“±ê¸‰ ë¶€ì—¬
4. ë“±ê¸‰ì„ ì›í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜

### í•´ë‹µ

```python
# íŒŒì¼ëª…: homework11.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder

# 1. ë°ì´í„° ë¡œë“œ ë° ëˆ„ë½ ë°ì´í„° í™•ì¸
data = pd.read_csv('./data/iris.csv')
print("=== 1. ëˆ„ë½ ë°ì´í„° í™•ì¸ ===")
print(data.isnull().sum())

# 2. ëˆ„ë½ ë°ì´í„° í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
print("\n=== 2. ëˆ„ë½ ë°ì´í„° ëŒ€ì²´ ===")
mean_values = {
    'sepal.length': data['sepal.length'].mean(),
    'sepal.width': data['sepal.width'].mean(),
    'petal.length': data['petal.length'].mean(),
    'petal.width': data['petal.width'].mean()
}

for column, mean_value in mean_values.items():
    data[column].fillna(mean_value, inplace=True)

print("ëŒ€ì²´ í›„ ëˆ„ë½ ë°ì´í„° í™•ì¸:")
print(data.isnull().sum())

# 3. ì •ê·œí™” í•¨ìˆ˜ ì •ì˜ ë° ì ìš©
def normalize(column_name):
    max_val = data[column_name].max()
    min_val = data[column_name].min()
    return (data[column_name] - min_val) / (max_val - min_val)

print("\n=== 3. ì •ê·œí™” ì ìš© ===")
data['sepal.length'] = normalize('sepal.length')
data['sepal.width'] = normalize('sepal.width')
data['petal.width'] = normalize('petal.width')
print("ì •ê·œí™” ì™„ë£Œ")
print(data.head())

# 4. êµ¬ê°„ ë‚˜ëˆ„ê¸°
print("\n=== 4. êµ¬ê°„ ë‚˜ëˆ„ê¸° ===")
count, bin_dividers = np.histogram(data['petal.length'], bins=3)
print("êµ¬ê°„ ê²½ê³„ê°’:", bin_dividers)

bin_names = ["A", "B", "C"]
data["petal_grade"] = pd.cut(x=data['petal.length'], 
                             bins=bin_dividers,
                             labels=bin_names, 
                             include_lowest=True)

print("êµ¬ê°„ ë‚˜ëˆ„ê¸° ê²°ê³¼:")
print(data[['petal.length', 'petal_grade']].head())

# 5. ì›í•« ì¸ì½”ë”©
print("\n=== 5. ì›í•« ì¸ì½”ë”© ===")
Y_class = np.array(data['petal_grade']).reshape(-1, 1)
enc = OneHotEncoder()
enc.fit(Y_class)
Y_class_onehot = enc.transform(Y_class).toarray()

print("ì›í•« ì¸ì½”ë”© ê²°ê³¼ (ì²˜ìŒ 10ê°œ):")
print(Y_class_onehot[:10])

# ì¸ì½”ë”© ë³µì›
Y_class_recovery = np.argmax(Y_class_onehot, axis=1).reshape(-1, 1)
print("\në³µì›ëœ ì¸ë±ìŠ¤ (ì²˜ìŒ 10ê°œ):")
print(Y_class_recovery[:10])
```

---

## í•µì‹¬ ìš”ì•½

### ë°ì´í„° ëˆ„ë½ì¹˜ ì²˜ë¦¬ ì ˆì°¨
1. **í™•ì¸**: `isnull()` í•¨ìˆ˜ë¡œ ëˆ„ë½ ë°ì´í„° í™•ì¸
2. **ê°œìˆ˜ íŒŒì•…**: `isnull().sum()`ìœ¼ë¡œ ëˆ„ë½ ë°ì´í„° ê°œìˆ˜ í™•ì¸
3. **ì²˜ë¦¬ ë°©ë²• ì„ íƒ**:
   - ëˆ„ë½ ê±´ìˆ˜ê°€ ì ì„ ê²½ìš°: `dropna()`ë¡œ ì‚­ì œ
   - ëˆ„ë½ ê±´ìˆ˜ê°€ ë§ì„ ê²½ìš°: `fillna()`ë¡œ ëŒ€ì²´ (í‰ê· ê°’, ì¤‘ê°„ê°’ ë“±)

### ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬
- `duplicated()`: ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
- `drop_duplicates()`: ì¤‘ë³µ í–‰ ì œê±°
- `subset` ë§¤ê°œë³€ìˆ˜ë¡œ íŠ¹ì • ì»¬ëŸ¼ ê¸°ì¤€ ì¤‘ë³µ ì œê±° ê°€ëŠ¥

### ë°ì´í„° ë³€í™˜ ê¸°ë²•
1. **ì •ê·œí™”**: ë°ì´í„° ìŠ¤ì¼€ì¼ í†µì¼
   - Min-Max ì •ê·œí™”: `(x - min) / (max - min)`
   - Z-score ì •ê·œí™”: `(x - mean) / std`

2. **íƒ€ì… ë³€í™˜**: `astype()` í•¨ìˆ˜ ì‚¬ìš©
   - ìˆ«ìí˜•: `'int'`, `'float'`
   - ë²”ì£¼í˜•: `'category'`

3. **êµ¬ê°„ ë‚˜ëˆ„ê¸°**: ì—°ì†í˜• â†’ ë²”ì£¼í˜• ë³€í™˜
   - `np.histogram()`: êµ¬ê°„ ê²½ê³„ê°’ ê³„ì‚°
   - `pd.cut()`: êµ¬ê°„ ë‚˜ëˆ„ê¸° ì‹¤í–‰

4. **ì›í•« ì¸ì½”ë”©**: ë²”ì£¼í˜• â†’ ìˆ˜ì¹˜í˜• ë³€í™˜
   - `OneHotEncoder`: sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
   - `reshape(-1, 1)`: 1ì°¨ì› â†’ 2ì°¨ì› ë³€í™˜ í•„ìš”

### ë°ì´í„° íƒ€ì… ë¶„ë¥˜
- **ì—°ì†í˜•**: í‰ê· ê°’ì´ ì¤‘ìš”, ìˆ˜ì¹˜ì  ì—°ì‚° ê°€ëŠ¥
- **ë²”ì£¼í˜•(ë¶ˆì—°ì†í˜•)**: ê°œìˆ˜ê°€ ì¤‘ìš”, ì›í•« ì¸ì½”ë”© í•„ìš”

### ì£¼ìš” ì£¼ì˜ì‚¬í•­
- ë¬¸ìì—´ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜ í›„ ì‚¬ìš©
- ì›í•« ì¸ì½”ë”© ì‹œ ë°ì´í„° ì°¨ì› í™•ì¸
- ì •ê·œí™”ëŠ” ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„±ì— ë”°ë¼ ì„ íƒì  ì ìš©
- ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë°ì´í„° ë¶„í¬ ê³ ë ¤

ì´ëŸ¬í•œ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì€ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ë‹¨ê³„ì…ë‹ˆë‹¤.


