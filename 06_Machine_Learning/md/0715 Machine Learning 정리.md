# π“ Machine Learning μ΄λ΅ 

##### π—“οΈ 2025.07.15
##### π“ Writer : Moon19ht

---

## π“ λ©μ°¨

1. [ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κ°λ΅ ](#1-ν•μ΄νΌνλΌλ―Έν„°-μµμ ν™”-κ°λ΅ )
2. [Optuna λ² μ΄μ§€μ• μµμ ν™”](#2-optuna-λ² μ΄μ§€μ•-μµμ ν™”)
3. [Scikit-learn Pipeline ν™μ©](#3-scikit-learn-pipeline-ν™μ©)
4. [GridSearchCV vs Optuna λΉ„κµ](#4-gridsearchcv-vs-optuna-λΉ„κµ)
5. [μ‹¤λ¬΄ μ μ© μ „λµ](#5-μ‹¤λ¬΄-μ μ©-μ „λµ)
6. [μΆ…ν•© μ •λ¦¬ λ° κ²°λ΅ ](#6-μΆ…ν•©-μ •λ¦¬-λ°-κ²°λ΅ )

---

## 1. ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κ°λ΅ 

### 1.1 ν•μ΄νΌνλΌλ―Έν„°λ€?

> **ν•μ΄νΌνλΌλ―Έν„°(Hyperparameter)**: λ¨λΈ ν•™μµ μ „μ— μ‚¬μ „μ— μ„¤μ •ν•΄μ•Ό ν•λ” λ§¤κ°λ³€μ

#### μ°Έκ³  λ¨λΈ νλΌλ―Έν„° vs ν•μ΄νΌνλΌλ―Έν„°

| κµ¬λ¶„ | λ¨λΈ νλΌλ―Έν„° | ν•μ΄νΌνλΌλ―Έν„° |
|------|---------------|----------------|
| **μ •μ** | λ°μ΄ν„°λ΅λ¶€ν„° ν•™μµλλ” κ°’ | μ‚¬μ „μ— μ„¤μ •ν•λ” κ°’ |
| **μμ‹** | μ„ ν•νκ·€μ κ°€μ¤‘μΉ, νΈν–¥ | ν•™μµλ¥ , νΈλ¦¬ κΉμ΄, μ •κ·ν™” κ°•λ„ |
| **κ²°μ • λ°©λ²•** | μ•κ³ λ¦¬μ¦μ΄ μλ™ ν•™μµ | μ‚¬λμ΄ μ§μ ‘ μ„¤μ • λλ” μµμ ν™” |
| **μν–¥** | λ¨λΈμ μμΈ΅ μ„±λ¥ | ν•™μµ κ³Όμ •κ³Ό μµμΆ… μ„±λ¥ |

### 1.2 ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”μ μ¤‘μ”μ„±

#### ν•µμ‹¬ μµμ ν™” λ©ν‘
- **μ„±λ¥ ν–¥μƒ**: λ¨λΈμ μμΈ΅ μ •ν™•λ„ κ°μ„ 
- **κ³Όμ ν•© λ°©μ§€**: μΌλ°ν™” μ„±λ¥ ν™•λ³΄
- **ν¨μ¨μ„± μ¦λ€**: ν•™μµ μ‹κ°„κ³Ό μμ› μµμ ν™”

#### μ£Όμ μµμ ν™” μ‹¤ν¨ μ‹ λ¬Έμ μ 
- κ³Όμ ν•©(Overfitting): ν›λ ¨ λ°μ΄ν„°μ—λ§ νΉν™”
- κ³Όμ†μ ν•©(Underfitting): μ¶©λ¶„ν ν•™μµλμ§€ μ•μ
- μμ› λ‚­λΉ„: λ¶ν•„μ”ν• κ³„μ‚° λ³µμ΅λ„

### 1.3 μ£Όμ” μµμ ν™” λ°©λ²•λ΅ 

#### Tip μ „ν†µμ  λ°©λ²•
1. **Manual Search**: μλ™ νƒμƒ‰
   - κ²½ν—κ³Ό μ§κ΄€μ— μμ΅΄
   - μ‹κ°„ μ†λ¨μ , λΉ„ν¨μ¨μ 

2. **Grid Search**: κ²©μ νƒμƒ‰
   - λ¨λ“  μ΅°ν•©μ„ μ²΄κ³„μ μΌλ΅ νƒμƒ‰
   - ν™•μ‹¤ν•μ§€λ§ μ‹κ°„ λ³µμ΅λ„ λ†’μ

3. **Random Search**: λ¬΄μ‘μ„ νƒμƒ‰
   - μ„μ μ΅°ν•© μ„ νƒ
   - Grid Searchλ³΄λ‹¤ ν¨μ¨μ 

#### ν„λ€μ  λ°©λ²•
1. **Bayesian Optimization**: λ² μ΄μ§€μ• μµμ ν™”
   - μ΄μ „ κ²°κ³Όλ¥Ό ν™μ©ν• μ§€λ¥μ  νƒμƒ‰
   - Optuna, Hyperopt λ“±

2. **Evolutionary Algorithms**: μ§„ν™” μ•κ³ λ¦¬μ¦
   - μ μ „ μ•κ³ λ¦¬μ¦ κΈ°λ°
   - DEAP, TPOT λ“±

---

## 2. Optuna λ² μ΄μ§€μ• μµμ ν™”

### 2.1 Optuna κ°μ”

#### ν•µμ‹¬ Optunaμ νΉμ§•
- **λ² μ΄μ§€μ• μµμ ν™”**: μ΄μ „ μ‹λ„ κ²°κ³Όλ¥Ό ν™μ©ν• μ§€λ¥μ  νƒμƒ‰
- **TPE μ•κ³ λ¦¬μ¦**: Tree-structured Parzen Estimator
- **Pruning μ§€μ›**: μ„±λ¥μ΄ λ‚®μ€ μ‹λ„ μ΅°κΈ° μΆ…λ£
- **λ‹¤μ–‘ν• λ¶„ν¬**: μ—°μ†ν•, λ²”μ£Όν•, μ •μν• νλΌλ―Έν„° μ§€μ›

#### κµ¬μ΅° ν•µμ‹¬ κµ¬μ„±μ”μ†

```python
# Study: μµμ ν™” μ‹¤ν— κ΄€λ¦¬
study = optuna.create_study(direction="maximize")

# Trial: κ°λ³„ μ‹λ„
def objective(trial):
    # ν•μ΄νΌνλΌλ―Έν„° μ μ•
    param = trial.suggest_int('param', 1, 100)
    # λ¨λΈ ν•™μµ λ° ν‰κ°€
    return score

# μµμ ν™” μ‹¤ν–‰
study.optimize(objective, n_trials=100)
```

### 2.2 μ‹¤μµ: μ λ°©μ•” λ°μ΄ν„° λ¶„λ¥ μµμ ν™”

#### μ°Έκ³  λ°μ΄ν„°μ…‹ νΉμ„±
- **λ©μ **: μ λ°©μ•” μ•…μ„±/μ–‘μ„± λ¶„λ¥
- **μƒν” μ**: 569κ°
- **νΉμ„± μ**: 30κ° (μ„Έν¬ν•µ μΈ΅μ •κ°’)
- **ν΄λμ¤ λ¶„ν¬**: μ–‘μ„± 62.7%, μ•…μ„± 37.3%

#### Tip νƒμƒ‰μ  λ°μ΄ν„° λ¶„μ„ (EDA)

```python
# ν΄λμ¤ λ¶κ· ν• ν™•μΈ
print(f"ν΄λμ¤ λ¶„ν¬:")
print(f"   μ–‘μ„±(0): {class_distribution[0]}κ° ({ratio:.1f}%)")
print(f"   μ•…μ„±(1): {class_distribution[1]}κ° ({ratio:.1f}%)")
print(f"ν΄λμ¤ λ¶κ· ν• λΉ„μ¨: {imbalance_ratio:.2f}:1")
```

**ν•µμ‹¬ λ°κ²¬μ‚¬ν•­:**
- μ•½κ°„μ ν΄λμ¤ λ¶κ· ν• μ΅΄μ¬ (1.68:1)
- Stratified Split ν•„μ”μ„± ν™•μΈ
- κ²°μΈ΅κ°’ μ—†μ, λ°μ΄ν„° ν’μ§ μ–‘νΈ

#### μ‹¤μµ RandomForest ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”

**μµμ ν™” λ€μƒ νλΌλ―Έν„°:**

| νλΌλ―Έν„° | μ„¤λ… | νƒμƒ‰ λ²”μ„ | μν–¥ |
|----------|------|-----------|------|
| `max_depth` | νΈλ¦¬μ μµλ€ κΉμ΄ | 5~20 | κ³Όμ ν•© μ μ–΄ |
| `min_samples_leaf` | λ¦¬ν”„λ…Έλ“ μµμ† μƒν” μ | 1~10 | κ³Όμ ν•© λ°©μ§€ |
| `min_samples_split` | λ¶„ν•  μµμ† μƒν” μ | 2~10 | λ¶„ν•  κΈ°μ¤€ |
| `n_estimators` | νΈλ¦¬μ κ°μ | 50~100 | λ¨λΈ λ³µμ΅λ„ |

#### κ²°κ³Ό μµμ ν™” κ²°κ³Ό λ¶„μ„

**μ„±λ¥ κ°μ„ :**
- **κΈ°λ³Έ λ¨λΈ**: 0.9451 μ •ν™•λ„
- **μµμ ν™” λ¨λΈ**: 0.9582 μ •ν™•λ„
- **ν–¥μƒλ¥ **: +1.40% (ν†µκ³„μ μΌλ΅ μ μλ―Έ)

**μµμ  ν•μ΄νΌνλΌλ―Έν„°:**
```python
{
    'max_depth': 13,
    'min_samples_leaf': 1,
    'min_samples_split': 3,
    'n_estimators': 55
}
```

**ν•μ΄νΌνλΌλ―Έν„° μ¤‘μ”λ„:**
1. `min_samples_leaf`: 0.635 (κ°€μ¥ μ¤‘μ”)
2. `min_samples_split`: 0.465
3. `n_estimators`: 0.380
4. `max_depth`: 0.084

#### κ²°κ³Ό μµμΆ… ν…μ¤νΈ μ„±λ¥

```
β… ν…μ¤νΈ μ •ν™•λ„: 0.9649
β… ROC AUC μ μ: 0.9878

π“ λ¶„λ¥ λ¦¬ν¬νΈ:
                precision    recall  f1-score   support
   μ–‘μ„± (Benign)       0.99      0.96      0.97        72
μ•…μ„± (Malignant)       0.93      0.98      0.95        42
```

**μ„μƒ ν™μ© κ°€λ¥μ„±:**
- 96.49% μ •ν™•λ„: μ„μƒμ—μ„ ν™μ© κ°€λ¥ν• μμ¤€
- λ†’μ€ λ―Όκ°λ„(μ¬ν„μ¨): μ•…μ„± μΆ…μ–‘ λ†“μΉ  μ„ν— μµμ†ν™”
- μ°μν• νΉμ΄λ„(μ •λ°€λ„): μ¤μ§„ μ„ν— κ°μ†

---

## 3. Scikit-learn Pipeline ν™μ©

### 3.1 Pipeline κΈ°μ΄ κ°λ…

#### κµ¬μ΅° Pipelineμ΄λ€?
> **Pipeline**: μ—¬λ¬ λ‹¨κ³„μ λ°μ΄ν„° μ²λ¦¬ κ³Όμ •μ„ ν•λ‚μ κ°μ²΄λ΅ μ—°κ²°ν•λ” λ„κµ¬

#### ν•µμ‹¬ Pipelineμ ν•µμ‹¬ κ°€μΉ

**1. μΌκ΄€μ„± (Consistency)**
```python
# ν›λ ¨κ³Ό μμΈ΅μ—μ„ λ™μΌν• μ „μ²λ¦¬ μμ„ λ³΄μ¥
pipeline.fit(X_train, y_train)      # fit: scaler.fit β†’ model.fit
y_pred = pipeline.predict(X_test)   # predict: scaler.transform β†’ model.predict
```

**2. μ•μ „μ„± (Safety)**
- λ°μ΄ν„° λ„μ¶(Data Leakage) λ°©μ§€
- ν…μ¤νΈ λ°μ΄ν„°κ°€ ν›λ ¨ κ³Όμ •μ— μν–¥μ„ μ£Όμ§€ μ•μ

**3. μ¬μ‚¬μ©μ„± (Reusability)**
- ν• λ² μ •μν• νμ΄ν”„λΌμΈμ„ λ‹¤λ¥Έ λ°μ΄ν„°μ— μ μ©
- μ½”λ“ λ¨λ“ν™”μ™€ μ μ§€λ³΄μμ„± ν–¥μƒ

#### κµ¬μ΅° Pipeline κµ¬μ΅°

```python
from sklearn.pipeline import Pipeline

# κΈ°λ³Έ κµ¬μ΅°
pipeline = Pipeline([
    ('step_name1', transformer),  # μ „μ²λ¦¬ λ‹¨κ³„
    ('step_name2', estimator)     # λ¨λΈ λ‹¨κ³„
])

# μ‹¤μ  μμ‹
pipeline = Pipeline([
    ('scaler', StandardScaler()),           # 1λ‹¨κ³„: ν‘μ¤€ν™”
    ('classifier', RandomForestClassifier()) # 2λ‹¨κ³„: λ¶„λ¥
])
```

### 3.2 μ‹¤μµ: Iris λ‹¤μ¤‘λ¶„λ¥ Pipeline

#### λΉ„κµ νμ΄ν”„λΌμΈ vs μλ™ μ²λ¦¬ λΉ„κµ

**μλ™ μ²λ¦¬ λ°©μ‹:**
```python
# 1. μ¤μΌ€μΌλ§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 2. λ¨λΈ ν•™μµ
model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X_train_scaled, y_train)

# 3. μμΈ΅
y_pred = model.predict(X_test_scaled)
```

**Pipeline λ°©μ‹:**
```python
# ν• λ²μ— μ •μ
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svc', SVC(kernel='rbf', C=1.0, gamma='scale'))
])

# ν• λ²μ— ν•™μµ
pipeline.fit(X_train, y_train)

# ν• λ²μ— μμΈ΅
y_pred = pipeline.predict(X_test)
```

**κ²°κ³Ό λΉ„κµ:**
- μλ™ μ²λ¦¬: 0.9667 μ •ν™•λ„
- Pipeline: 0.9667 μ •ν™•λ„
- **μ°¨μ΄: 0.000000 (λ™μΌν•¨ ν™•μΈ)**

### 3.3 Pipeline + GridSearchCV ν†µν•©

#### Tip ν•μ΄νΌνλΌλ―Έν„° λ…λ… κ·μΉ™

```python
# νμ΄ν”„λΌμΈ λ‹¨κ³„λ…__νλΌλ―Έν„°λ…
param_grid = {
    'scaler': [StandardScaler(), MinMaxScaler()],
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__solver': ['liblinear', 'lbfgs']
}
```

#### κ²°κ³Ό μ λ°©μ•” λ°μ΄ν„° GridSearchCV κ²°κ³Ό

**νƒμƒ‰ μ„¤μ •:**
- μ΄ μ΅°ν•© μ: 20κ° (2Γ—5Γ—2)
- κµμ°¨κ²€μ¦: 5-fold Stratified
- ν‰κ°€μ§€ν‘: ROC AUC (λ¶κ· ν• λ°μ΄ν„°μ— μ ν•©)

**μµμ ν™” κ²°κ³Ό:**
```
π† μµκ³  ROC AUC μ μ: 0.9960
π“ μµμ  ν•μ΄νΌνλΌλ―Έν„°:
   classifier__C: 1
   classifier__solver: liblinear
   scaler: StandardScaler

π― μµμΆ… ν…μ¤νΈ μ„±λ¥:
   μ •ν™•λ„: 0.9825
   ROC AUC: 0.9957
```

### 3.4 Pipeline + Optuna ν†µν•©

#### μ‹¤μµ RandomForest + Pipeline μµμ ν™”

```python
def objective_pipeline(trial):
    # ν•μ΄νΌνλΌλ―Έν„° μ μ•
    max_depth = trial.suggest_int('max_depth', 5, 20)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
    n_estimators = trial.suggest_int('n_estimators', 50, 100)
    
    # μ¤μΌ€μΌλ¬ μ„ νƒ
    scaler_name = trial.suggest_categorical('scaler', ['standard', 'minmax'])
    scaler = StandardScaler() if scaler_name == 'standard' else MinMaxScaler()
    
    # Pipeline κµ¬μ„±
    pipeline = Pipeline([
        ('scaler', scaler),
        ('classifier', RandomForestClassifier(...))
    ])
    
    # κµμ°¨κ²€μ¦ ν‰κ°€
    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)
    return cv_scores.mean()
```

**μµμ ν™” κ²°κ³Ό:**
- **κµμ°¨κ²€μ¦ μ •ν™•λ„**: 0.9692
- **ν…μ¤νΈ μ •ν™•λ„**: 0.9737
- **ROC AUC**: 0.9922
- **μ„±λ¥ ν–¥μƒ**: +0.68%

---

## 4. GridSearchCV vs Optuna λΉ„κµ

### 4.1 μΆ…ν•© μ„±λ¥ λΉ„κµ

#### λΉ„κµ μ‹¤ν— μ„¤μ • λΉ„κµ

| λ°©λ²•λ΅  | λ°μ΄ν„°μ…‹ | λ¨λΈ | νƒμƒ‰ κ³µκ°„ | μ‹λ„ νμ |
|--------|----------|------|-----------|-----------|
| GridSearchCV | Breast Cancer | Logistic Regression | 20κ° μ΅°ν•© | 20ν |
| Optuna | Breast Cancer | Random Forest | μ—°μ† κ³µκ°„ | 50ν |

#### κ²°κ³Ό μ„±λ¥ κ²°κ³Ό λΉ„κµ

| μ§€ν‘ | GridSearchCV | Optuna | μΉμ |
|------|-------------|--------|------|
| **μ •ν™•λ„** | 0.9825 | 0.9737 | GridSearchCV (+0.0088) |
| **ROC AUC** | 0.9957 | 0.9922 | GridSearchCV (+0.0035) |
| **μµμ ν™” μ‹κ°„** | 6.6μ΄ | 12.8μ΄ | GridSearchCV |
| **ν¨μ¨μ„±** | 14.89 | 7.62 | GridSearchCV |

### 4.2 λ°©λ²•λ΅ λ³„ μ¥λ‹¨μ  λ¶„μ„

#### λΉ„κµ GridSearchCV

**μ¥μ :**
- β… **μ „μ νƒμƒ‰**: λ¨λ“  μ΅°ν•©μ„ ν™•μ‹¤ν κ²€ν† 
- β… **μ•μ •μ„±**: μμΈ΅ κ°€λ¥ν• κ²°κ³Ό
- β… **μ‘μ€ κ³µκ°„μ—μ„ ν¨μ¨μ **: μ΅°ν•© μκ°€ μ μ„ λ• μµμ 
- β… **ν•΄μ„ μ©μ΄**: κ²°κ³Ό λ¶„μ„μ΄ μ§κ΄€μ 

**λ‹¨μ :**
- β **μ΅°ν•© ν­λ°**: νλΌλ―Έν„° μ¦κ°€ μ‹ κΈ°ν•κΈ‰μμ  μ¦κ°€
- β **μ—°μ† κ³µκ°„ ν•κ³„**: μ΄μ‚°κ°’λ§ νƒμƒ‰ κ°€λ¥
- β **μ‹κ°„ λ³µμ΅λ„**: O(n^k)λ΅ ν™•μ¥μ„± λ¶€μ΅±

#### λΉ„κµ Optuna

**μ¥μ :**
- β… **μ§€λ¥μ  νƒμƒ‰**: λ² μ΄μ§€μ• μµμ ν™”λ΅ ν¨μ¨μ 
- β… **μ—°μ† κ³µκ°„**: μ‹¤μκ°’ νλΌλ―Έν„° μµμ ν™”
- β… **μ΅°κΈ° μΆ…λ£**: PruningμΌλ΅ μ‹κ°„ μ μ•½
- β… **ν™•μ¥μ„±**: ν° νƒμƒ‰ κ³µκ°„μ—μ„ μ°μ

**λ‹¨μ :**
- β **ν™•λ¥ μ  κ²°κ³Ό**: μ‹¤ν–‰λ§λ‹¤ κ²°κ³Ό λ³€λ™
- β **λ³µμ΅μ„±**: λ² μ΄μ§€μ• μµμ ν™” μ΄ν•΄ ν•„μ”
- β **μ΄κΈ° μ¤λ²„ν—¤λ“**: μ‘μ€ λ¬Έμ μ—μ„ λΉ„ν¨μ¨μ 

### 4.3 μ‹¤λ¬΄ μ μ© κ°€μ΄λ“

#### λ‹¨κ³„ μƒν™©λ³„ μ„ νƒ κΈ°μ¤€

```python
def choose_optimization_method(search_space_size, time_constraint, reproducibility):
    if search_space_size < 100:
        if reproducibility == "high":
            return "GridSearchCV"
        else:
            return "GridSearchCV λλ” Optuna"
    
    elif search_space_size < 1000:
        if time_constraint == "strict":
            return "Optuna + Pruning"
        else:
            return "Optuna"
    
    else:  # ν° νƒμƒ‰ κ³µκ°„
        return "Optuna + Multi-objective"
```

#### λΉ„κµ μμ‚¬κ²°μ • λ§¤νΈλ¦­μ¤

| μ΅°κ±΄ | GridSearchCV | Optuna |
|------|-------------|--------|
| νƒμƒ‰ κ³µκ°„ < 100κ° | β­β­β­ | β­β­ |
| νƒμƒ‰ κ³µκ°„ > 100κ° | β­ | β­β­β­ |
| μ‹κ°„ μ μ•½ μμ | β­ | β­β­β­ |
| μ¬ν„μ„± μ¤‘μ” | β­β­β­ | β­β­ |
| μ—°μ† νλΌλ―Έν„° | β­ | β­β­β­ |
| μ΄λ³΄μ μΉν™”μ„± | β­β­β­ | β­β­ |

---

## 5. μ‹¤λ¬΄ μ μ© μ „λµ

### 5.1 ν”„λ΅μ νΈ λ‹¨κ³„λ³„ κ°€μ΄λ“

#### λ‹¨κ³„ Phase 1: λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘
```python
# κΈ°λ³Έ PipelineμΌλ΅ λ² μ΄μ¤λΌμΈ κµ¬μ¶•
baseline_pipeline = Pipeline([
    ('preprocessor', StandardScaler()),
    ('model', RandomForestClassifier(random_state=42))
])

# λΉ λ¥Έ μ„±λ¥ ν™•μΈ
baseline_score = cross_val_score(baseline_pipeline, X, y, cv=5).mean()
```

#### λ‹¨κ³„ Phase 2: μ²΄κ³„μ  μµμ ν™”
```python
# μ‘μ€ νƒμƒ‰ κ³µκ°„: GridSearchCV
if len(param_combinations) < 100:
    grid_search = GridSearchCV(
        pipeline, param_grid, 
        cv=5, scoring='accuracy', n_jobs=-1
    )
    
# ν° νƒμƒ‰ κ³µκ°„: Optuna
else:
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=100)
```

#### λ‹¨κ³„ Phase 3: μ„±λ¥ κ²€μ¦
```python
# μµμΆ… λ¨λΈ ν‰κ°€
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

# μΆ…ν•© ν‰κ°€
print(classification_report(y_test, y_pred))
print(f"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
```

### 5.2 ν•μ΄νΌνλΌλ―Έν„° μ„ νƒ μ „λµ

#### ν•µμ‹¬ λ¨λΈλ³„ ν•µμ‹¬ νλΌλ―Έν„°

**RandomForest:**
```python
# ν•µμ‹¬ νλΌλ―Έν„° μ°μ„ μμ„
priority_params = {
    'n_estimators': [50, 100, 200],      # λ†’μ€ μν–¥
    'max_depth': [10, 20, None],         # κ³Όμ ν•© μ μ–΄
    'min_samples_split': [2, 5, 10],     # λ¶„ν•  κΈ°μ¤€
    'min_samples_leaf': [1, 2, 4]        # λ¦¬ν”„ λ…Έλ“ ν¬κΈ°
}
```

**LogisticRegression:**
```python
# L1/L2 μ •κ·ν™”μ™€ κ°•λ„
priority_params = {
    'C': [0.01, 0.1, 1, 10, 100],       # μ •κ·ν™” κ°•λ„
    'penalty': ['l1', 'l2', 'elasticnet'], # μ •κ·ν™” λ°©λ²•
    'solver': ['liblinear', 'lbfgs']      # μµμ ν™” μ•κ³ λ¦¬μ¦
}
```

**SVM:**
```python
# μ»¤λ„κ³Ό ν•μ΄νΌνλΌλ―Έν„°
priority_params = {
    'C': [0.1, 1, 10, 100],             # λ§μ§„ μ΅°μ 
    'gamma': ['scale', 'auto', 0.1, 1],  # RBF μ»¤λ„ ν­
    'kernel': ['rbf', 'linear', 'poly']   # μ»¤λ„ ν•¨μ
}
```

### 5.3 μ„±λ¥ ν‰κ°€ λ° ν•΄μ„

#### ν•µμ‹¬ ν‰κ°€μ§€ν‘ μ„ νƒ κ°€μ΄λ“

**μ΄μ§„ λ¶„λ¥:**
```python
# κ· ν• λ°μ΄ν„°
primary_metric = "accuracy"
secondary_metrics = ["precision", "recall", "f1_score"]

# λ¶κ· ν• λ°μ΄ν„°
primary_metric = "roc_auc"
secondary_metrics = ["precision_recall_auc", "f1_score"]

# μλ£/μ•μ „ λ¶„μ•Ό
primary_metric = "recall"  # λ―Όκ°λ„ μ¤‘μ‹
secondary_metrics = ["precision", "specificity"]
```

**λ‹¤μ¤‘ λ¶„λ¥:**
```python
# κΈ°λ³Έ ν‰κ°€
primary_metric = "accuracy"
secondary_metrics = ["macro_f1", "weighted_f1"]

# ν΄λμ¤λ³„ λ¶„μ„
detailed_metrics = ["precision_macro", "recall_macro"]
```

#### ν•΄μ„ κ²°κ³Ό ν•΄μ„ κ°€μ΄λ“

**ν†µκ³„μ  μ μμ„± κ²€μ¦:**
```python
def validate_improvement(baseline_scores, optimized_scores, alpha=0.05):
    from scipy.stats import ttest_rel
    
    t_stat, p_value = ttest_rel(optimized_scores, baseline_scores)
    
    if p_value < alpha:
        return "ν†µκ³„μ μΌλ΅ μ μν• κ°μ„ "
    else:
        return "ν†µκ³„μ μΌλ΅ μ μν•μ§€ μ•μ"
```

**μ‹¤μ©μ  μ μμ„± ν‰κ°€:**
```python
def assess_practical_significance(improvement, threshold=0.01):
    if improvement > threshold:
        return "μ‹¤μ©μ μΌλ΅ μλ―Έμλ” κ°μ„ "
    else:
        return "λ―Έλ―Έν• κ°μ„  (κ³Όμ ν•© μ„ν— κ³ λ ¤)"
```

---

## 6. μΆ…ν•© μ •λ¦¬ λ° κ²°λ΅ 

### 6.1 ν•µμ‹¬ ν•™μµ λ‚΄μ© μ”μ•½

#### ν•µμ‹¬ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” ν•µμ‹¬ μ›λ¦¬

1. **μ²΄κ³„μ  μ ‘κ·Ό**: λλ¤ν• μ‹λ„λ³΄λ‹¤ κ³„νμ  νƒμƒ‰
2. **κµμ°¨κ²€μ¦**: μΌλ°ν™” μ„±λ¥ ν™•λ³΄
3. **μ΅°κΈ° μΆ…λ£**: ν¨μ¨μ  μμ› ν™μ©
4. **ν†µκ³„μ  κ²€μ¦**: κ°μ„ μ μ μμ„± ν™•μΈ

#### ν•µμ‹¬ Pipeline ν™μ©μ ν•µμ‹¬ κ°€μΉ

1. **μΌκ΄€μ„±**: ν›λ ¨-μμΈ΅ κ³Όμ • ν†µμΌ
2. **μ•μ „μ„±**: λ°μ΄ν„° λ„μ¶ λ°©μ§€
3. **μ¬μ‚¬μ©μ„±**: μ½”λ“ λ¨λ“ν™”
4. **μ μ§€λ³΄μμ„±**: μ²΄κ³„μ  κ΄€λ¦¬

#### λΉ„κµ λ°©λ²•λ΅  μ„ νƒ κΈ°μ¤€

```python
# μμ‚¬κ²°μ • νΈλ¦¬
if problem_complexity == "simple":
    use_basic_pipeline()
elif search_space < 100:
    use_gridsearch()
elif time_constraint == "strict":
    use_optuna_with_pruning()
else:
    use_optuna_full()
```

### 6.2 μ‹¤λ¬΄ μ μ© Best Practices

#### Best ν”„λ΅μ νΈ μ›ν¬ν”λ΅μ°

```mermaid
graph TD
    A[λ°μ΄ν„° νƒμƒ‰] --> B[κΈ°λ³Έ Pipeline κµ¬μ¶•]
    B --> C[λ² μ΄μ¤λΌμΈ μ„±λ¥ μΈ΅μ •]
    C --> D[μµμ ν™” λ°©λ²• μ„ νƒ]
    D --> E[ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”]
    E --> F[μ„±λ¥ κ²€μ¦]
    F --> G[μµμΆ… λ¨λΈ λ°°ν¬]
```

#### μ²΄ν¬λ¦¬μ¤νΈ

**λ°μ΄ν„° μ¤€λΉ„:**
- [ ] κ²°μΈ΅κ°’ μ²λ¦¬ μ™„λ£
- [ ] μ΄μƒμΉ κ²€ν†  μ™„λ£
- [ ] ν΄λμ¤ λ¶κ· ν• ν™•μΈ
- [ ] ν›λ ¨/κ²€μ¦/ν…μ¤νΈ λ¶„ν• 

**λ¨λΈλ§:**
- [ ] λ² μ΄μ¤λΌμΈ λ¨λΈ κµ¬μ¶•
- [ ] μ μ ν• ν‰κ°€μ§€ν‘ μ„ νƒ
- [ ] κµμ°¨κ²€μ¦ μ„¤μ •
- [ ] ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰ κ³µκ°„ μ •μ

**μµμ ν™”:**
- [ ] μµμ ν™” λ°©λ²• μ„ νƒ (Grid vs Optuna)
- [ ] μ‹κ°„/μμ› μ μ•½ κ³ λ ¤
- [ ] μ§„ν–‰μƒν™© λ¨λ‹ν„°λ§
- [ ] κ²°κ³Ό ν•΄μ„ λ° κ²€μ¦

**λ°°ν¬:**
- [ ] μµμΆ… μ„±λ¥ ν™•μΈ
- [ ] λ¨λΈ μ €μ¥ λ° λ²„μ „ κ΄€λ¦¬
- [ ] μ¶”λ΅  νμ΄ν”„λΌμΈ κµ¬μ¶•
- [ ] μ„±λ¥ λ¨λ‹ν„°λ§ μ„¤μ •

### 6.3 ν–¥ν›„ ν•™μµ λ°©ν–¥

#### μ¶”μ² κ³ κΈ‰ μ£Όμ 

1. **λ©€ν‹° μ¤λΈμ ν‹°λΈ μµμ ν™”**
   - μ •ν™•λ„μ™€ μ†λ„ λ™μ‹ μµμ ν™”
   - Pareto μµμ ν™” μ μ©

2. **AutoML ν†µν•©**
   - TPOT, Auto-sklearn ν™μ©
   - μλ™ νΉμ„± μ—”μ§€λ‹μ–΄λ§

3. **λ¶„μ‚° μµμ ν™”**
   - Optuna λ¶„μ‚° μ‹¤ν–‰
   - ν΄λ¬μ¤ν„° ν™κ²½ ν™μ©

4. **κ³ κΈ‰ Pipeline κµ¬μ„±**
   - ColumnTransformer ν™μ©
   - μ»¤μ¤ν…€ Transformer κ°λ°

#### μ¶”μ² μλ£

**κ³µμ‹ λ¬Έμ„:**
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Scikit-learn Pipeline Guide](https://scikit-learn.org/stable/modules/pipeline.html)

**μ‹¬ν™” ν•™μµ:**
- Bayesian Optimization μ΄λ΅ 
- Hyperparameter Tuning μ „λ¬Έμ„
- MLOps νμ΄ν”„λΌμΈ κµ¬μ¶•

### 6.4 λ§λ¬΄λ¦¬ λ©”μ‹μ§€

> **"μ™„λ²½ν• ν•μ΄νΌνλΌλ―Έν„°λ” μ΅΄μ¬ν•μ§€ μ•λ”λ‹¤. λ°μ΄ν„°μ™€ λ¬Έμ μ— λ§λ” μµμ μ μ΅°ν•©μ„ μ°Ύλ” κ²ƒμ΄ λ©ν‘λ‹¤."**

#### ν•µμ‹¬ κΈ°μ–µν•  ν•µμ‹¬ μ›μΉ™

1. **μΈ΅μ •ν•λΌ**: κ°μ„ μ„ μμΉλ΅ μ…μ¦
2. **λ‹¨μν•κ² μ‹μ‘ν•λΌ**: λ³µμ΅ν• μµμ ν™”λ” λ‚μ¤‘μ—
3. **κ²€μ¦ν•λΌ**: κ³Όμ ν•©μ„ ν•­μƒ μμ‹¬
4. **λ¬Έμ„ν™”ν•λΌ**: μ‹¤ν— κ³Όμ •κ³Ό κ²°κ³Όλ¥Ό κΈ°λ΅
5. **λ°λ³µν•λΌ**: μ§€μ†μ μΈ κ°μ„ μ΄ ν•µμ‹¬
