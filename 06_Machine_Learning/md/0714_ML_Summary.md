# ğŸ“Š Machine Learning ì´ë¡ 

##### ğŸ—“ï¸ 2025.07.14
##### ğŸ“ Writer : Moon19ht

---

## ğŸ“š ëª©ì°¨

1. [ë°ì´í„° íŠ¹ì„± ì²˜ë¦¬ (ë²”ì£¼í˜• ë°ì´í„°)](#1-ë°ì´í„°-íŠ¹ì„±-ì²˜ë¦¬-ë²”ì£¼í˜•-ë°ì´í„°)
2. [í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹](#2-í•˜ì´í¼íŒŒë¼ë¯¸í„°-íŠœë‹)
3. [ëª¨ë¸ í‰ê°€ ë° ë¹„êµ](#3-ëª¨ë¸-í‰ê°€-ë°-ë¹„êµ)
4. [ê³ ê¸‰ ìµœì í™” ì „ëµ](#4-ê³ ê¸‰-ìµœì í™”-ì „ëµ)
5. [ê³ ê¸‰ ê¸°ë²• ë° ë‹¤ìŒ ë‹¨ê³„](#5-ê³ ê¸‰-ê¸°ë²•-ë°-ë‹¤ìŒ-ë‹¨ê³„)
6. [ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](#6-ì²´í¬ë¦¬ìŠ¤íŠ¸-ë°-ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤)
7. [í•™ìŠµ ì •ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„](#7-í•™ìŠµ-ì •ë¦¬-ë°-ë‹¤ìŒ-ë‹¨ê³„)

---

## 1. ë°ì´í„° íŠ¹ì„± ì²˜ë¦¬ (ë²”ì£¼í˜• ë°ì´í„°)

### 1.1 ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ì˜ í•„ìš”ì„±

#### ë¬¸ì œì 
- ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ëŒ€ë¶€ë¶„ ìˆ«ìí˜• ë°ì´í„°ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
- ë¬¸ìì—´ í˜•íƒœì˜ ë²”ì£¼í˜• ë°ì´í„°ëŠ” ì§ì ‘ ì‚¬ìš© ë¶ˆê°€
- ìˆ«ì í˜•íƒœì˜ ë²”ì£¼í˜• ë°ì´í„°ë„ ì˜ëª» í•´ì„ë  ìœ„í—˜

#### ì˜ˆì‹œ
```python
# ì§ì—…ë¶„ë¥˜ê°€ 1~16ì˜ ìˆ«ìë¡œ ì½”ë”©ëœ ê²½ìš°
# 1ë³´ë‹¤ 16ì´ ë” í° ê°’ìœ¼ë¡œ ì¸ì‹ë˜ì–´ ëª¨ë¸ì´ ì˜ëª»ëœ ì¤‘ìš”ë„ë¥¼ í•™ìŠµ
```

### 1.2 ë°©ë²• 1: `pd.get_dummies()`

#### íŠ¹ì§•
- **ì¥ì **: ê°„ë‹¨í•˜ê³  ì§ê´€ì , pandasì™€ ì˜ í†µí•©ë¨
- **ë‹¨ì **: ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„° ì¸ì‹ ëª»í•¨, ì»¬ëŸ¼ë³„ ë‹¤ë¥¸ ì²˜ë¦¬ ì–´ë ¤ì›€

#### ì‚¬ìš©ë²•
```python
# ê¸°ë³¸ ì‚¬ìš©ë²•
data_encoded = pd.get_dummies(data)

# íƒ€ê²Ÿ ë³€ìˆ˜ì™€ íŠ¹ì„± ë³€ìˆ˜ ë¶„ë¦¬
income_columns = [col for col in data_encoded.columns if col.startswith('income_')]
feature_columns = [col for col in data_encoded.columns if not col.startswith('income_')]

X = data_encoded[feature_columns]
y = data_encoded['income_ >50K']  # íƒ€ê²Ÿ ë³€ìˆ˜
```

#### ì ìš© ì˜ˆì‹œ
```python
# Adult ë°ì´í„°ì…‹ ì ìš© ê²°ê³¼
ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ ìˆ˜: 7
ì¸ì½”ë”© í›„ ì»¬ëŸ¼ ìˆ˜: 46
```

### 1.3 ë°©ë²• 2: `OneHotEncoder`

#### íŠ¹ì§•
- **ì¥ì **: ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥, scikit-learn íŒŒì´í”„ë¼ì¸ê³¼ í˜¸í™˜
- **ë‹¨ì **: ë³„ë„ í´ë˜ìŠ¤ ì‚¬ìš© í•„ìš”, pandas DataFrameê³¼ ë¶„ë¦¬ë¨

#### ì‚¬ìš©ë²•
```python
from sklearn.preprocessing import OneHotEncoder

# OneHotEncoder ìƒì„±
ohe = OneHotEncoder(sparse_output=False)

# ë³€í™˜ ìˆ˜í–‰
encoded_array = ohe.fit_transform(demo_df)

# íŠ¹ì„±ëª… í™•ì¸
feature_names = ohe.get_feature_names_out()
```

#### ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„° ì²˜ë¦¬
```python
# ë¬¸ì œ: get_dummiesëŠ” ìˆ«ìí˜• ë²”ì£¼ë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ì²˜ë¦¬
demo_df = pd.DataFrame({
    'ìˆ«ìíŠ¹ì„±': [0, 1, 2, 1],  # ë²”ì£¼í˜•ì´ì§€ë§Œ ìˆ«ìë¡œ í‘œí˜„
    'ë²”ì£¼í˜•íŠ¹ì„±': ['ì–‘ë§', 'ì—¬ìš°', 'ì–‘ë§', 'ìƒì']    
})

# í•´ê²°ë°©ë²• 1: ë¬¸ìì—´ ë³€í™˜ í›„ get_dummies
demo_df['ìˆ«ìíŠ¹ì„±'] = demo_df['ìˆ«ìíŠ¹ì„±'].astype(str)

# í•´ê²°ë°©ë²• 2: OneHotEncoder ì‚¬ìš© (ìë™ìœ¼ë¡œ ë²”ì£¼ë¡œ ì¸ì‹)
```

### 1.4 ë°©ë²• 3: `ColumnTransformer`

#### íŠ¹ì§•
- **ì¥ì **: ì»¬ëŸ¼ë³„ ë‹¤ë¥¸ ì „ì²˜ë¦¬ ê°€ëŠ¥, ë³µì¡í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•, ì¬ì‚¬ìš©ì„± ë†’ìŒ
- **ë‹¨ì **: ì„¤ì •ì´ ë³µì¡, ì´ˆë³´ìì—ê²Œ ì–´ë ¤ì›€

#### ì‚¬ìš©ë²•
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# ì»¬ëŸ¼ êµ¬ë¶„
numeric_columns = ['age', 'hours-per-week']
categorical_columns = ['workclass', 'education', 'gender', 'occupation']

# ColumnTransformer ìƒì„±
ct = ColumnTransformer([
    ("scaling", StandardScaler(), numeric_columns),
    ("onehot", OneHotEncoder(sparse_output=False), categorical_columns)
])

# ë³€í™˜ ìˆ˜í–‰
transformed_data = ct.fit_transform(ct_data)
```

#### ì¥ì 
```python
# í•œ ë²ˆì— ì—¬ëŸ¬ ì „ì²˜ë¦¬ ì‘ì—… ìˆ˜í–‰
ì›ë³¸ ë°ì´í„° í˜•íƒœ: (32561, 6)
ë³€í™˜ëœ ë°ì´í„° í˜•íƒœ: (32561, 44)

# ìˆ«ìí˜•: StandardScaler ì ìš©
# ë²”ì£¼í˜•: OneHotEncoder ì ìš©
```

### 1.5 ë°©ë²•ë³„ ë¹„êµ ë° ê¶Œì¥ì‚¬í•­

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œì  |
|------|------|------| --------|
| **pd.get_dummies()** | ê°„ë‹¨í•˜ê³  ì§ê´€ì <br>pandasì™€ ì˜ í†µí•©ë¨ | ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„° ì¸ì‹ ëª»í•¨<br>ì»¬ëŸ¼ë³„ ë‹¤ë¥¸ ì²˜ë¦¬ ì–´ë ¤ì›€ | ëª¨ë“  ì»¬ëŸ¼ì´ ë¬¸ìì—´ ë²”ì£¼í˜•<br>ê°„ë‹¨í•œ ì „ì²˜ë¦¬ |
| **OneHotEncoder** | ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥<br>scikit-learn íŒŒì´í”„ë¼ì¸ê³¼ í˜¸í™˜ | ë³„ë„ í´ë˜ìŠ¤ ì‚¬ìš© í•„ìš”<br>pandas DataFrameê³¼ ë¶„ë¦¬ë¨ | ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„° í¬í•¨<br>ML íŒŒì´í”„ë¼ì¸ êµ¬ì¶• |
| **ColumnTransformer** | ì»¬ëŸ¼ë³„ ë‹¤ë¥¸ ì „ì²˜ë¦¬ ê°€ëŠ¥<br>ë³µì¡í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•<br>ì¬ì‚¬ìš©ì„± ë†’ìŒ | ì„¤ì •ì´ ë³µì¡<br>ì´ˆë³´ìì—ê²Œ ì–´ë ¤ì›€ | í˜¼í•©ëœ ë°ì´í„° íƒ€ì…<br>ë³µì¡í•œ ì „ì²˜ë¦¬ í•„ìš” |

#### ê¶Œì¥ ì‚¬ìš©ë²•
1. **ê°„ë‹¨í•œ ë²”ì£¼í˜• ë°ì´í„°**: `pd.get_dummies()`
2. **ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„°**: `OneHotEncoder`
3. **ë³µì¡í•œ í˜¼í•© ë°ì´í„°**: `ColumnTransformer`

---

## 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### 2.1 í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°œë…

#### ì •ì˜
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ëª¨ë¸ í•™ìŠµ ì „ì— ë¯¸ë¦¬ ì„¤ì •í•´ì•¼ í•˜ëŠ” íŒŒë¼ë¯¸í„°
- **íŠ¹ì§•**: ë°ì´í„°ë¡œë¶€í„° í•™ìŠµë˜ì§€ ì•Šê³  ì‚¬ìš©ìê°€ ì§ì ‘ ì„¤ì •
- **ì¤‘ìš”ì„±**: ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ê³¼ì í•©/ê³¼ì†Œì í•©ì— ì§ì ‘ì  ì˜í–¥

#### ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜ˆì‹œ
```python
# SVM
C = [0.1, 1, 10, 100]          # ê·œì œ íŒŒë¼ë¯¸í„°
gamma = [1, 0.1, 0.01, 0.001]  # ì»¤ë„ íŒŒë¼ë¯¸í„°  
kernel = ['rbf', 'linear']      # ì»¤ë„ í•¨ìˆ˜

# RandomForest
n_estimators = [50, 100, 200]     # íŠ¸ë¦¬ ê°œìˆ˜
max_depth = [None, 3, 10, 20]     # ìµœëŒ€ ê¹Šì´
min_samples_split = [2, 5, 10]    # ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜

# GradientBoosting
n_estimators = [50, 100, 200]     # ë¶€ìŠ¤íŒ… ìŠ¤í…Œì´ì§€ ìˆ˜
max_depth = [3, 5, 10]            # ìµœëŒ€ ê¹Šì´
learning_rate = [0.01, 0.1, 0.2]  # í•™ìŠµë¥ 
```

### 2.2 GridSearchCV í™œìš©

#### ê¸°ë³¸ ê°œë…
```python
from sklearn.model_selection import GridSearchCV

# íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'linear']
}

# GridSearchCV ì„¤ì •
grid_search = GridSearchCV(
    estimator=SVC(random_state=42), 
    param_grid=param_grid, 
    cv=5,                    # 5-fold êµì°¨ê²€ì¦
    scoring='accuracy',      # í‰ê°€ ì§€í‘œ
    n_jobs=-1,              # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    verbose=1               # ì§„í–‰ìƒí™© ì¶œë ¥
)

# í•™ìŠµ ì‹¤í–‰
grid_search.fit(X_train, y_train)
```

#### ê²°ê³¼ ë¶„ì„
```python
# ìµœì  ê²°ê³¼ í™•ì¸
print(f"ìµœì ì˜ íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœê³  êµì°¨ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.4f}")
print(f"ìµœì  ëª¨ë¸: {grid_search.best_estimator_}")
```

### 2.3 ë‹¨ì¼ ëª¨ë¸ íŠœë‹ (SVM ì˜ˆì‹œ)

#### SVM ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

##### 1. C (ê·œì œ íŒŒë¼ë¯¸í„°)
- **ì—­í• **: ì˜¤ì°¨ í—ˆìš© ë²”ìœ„ ì¡°ì ˆ
- **ë†’ì€ ê°’**: ê³¼ëŒ€ì í•© ìœ„í—˜ ì¦ê°€, í›ˆë ¨ ë°ì´í„°ì— ë” ì •í™•
- **ë‚®ì€ ê°’**: ê³¼ì†Œì í•© ìœ„í—˜ ì¦ê°€, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

##### 2. gamma (ì»¤ë„ íŒŒë¼ë¯¸í„°)
- **ì—­í• **: RBF ì»¤ë„ì˜ ì˜í–¥ ë²”ìœ„ ì¡°ì ˆ
- **ë†’ì€ ê°’**: ê³¼ëŒ€ì í•© ìœ„í—˜, ê²°ì • ê²½ê³„ê°€ ë³µì¡
- **ë‚®ì€ ê°’**: ê³¼ì†Œì í•© ìœ„í—˜, ê²°ì • ê²½ê³„ê°€ ë‹¨ìˆœ

##### 3. kernel (ì»¤ë„ í•¨ìˆ˜)
- **linear**: ì„ í˜• ë°ì´í„°ì— ì í•©
- **rbf**: ë¹„ì„ í˜• ë°ì´í„°ì— ì í•© (ê°€ì¥ ì¼ë°˜ì )
- **poly**: ë‹¤í•­ì‹ ì»¤ë„
- **sigmoid**: ì‹œê·¸ëª¨ì´ë“œ ì»¤ë„

#### ìˆ˜ë ´ ë¬¸ì œ í•´ê²°
```python
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ìˆ˜ë ´ ë¬¸ì œ í•´ê²°
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì •
model = LogisticRegression(
    max_iter=5000,        # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€
    solver='liblinear',   # ì´ì§„ ë¶„ë¥˜ì— íš¨ê³¼ì ì¸ ì†”ë²„
    C=1.0,               # ì •ê·œí™” ê°•ë„
    random_state=42      # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
)
```

### 2.4 ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ

#### ì‹¤ìŠµ ê²°ê³¼ ì˜ˆì‹œ
```python
# ìœ ë°©ì•” ë°ì´í„°ì…‹ ê²°ê³¼ (ì •í™•ë„ ê¸°ì¤€)
êµì°¨ê²€ì¦ ì„±ëŠ¥ ìˆœìœ„:
1. RANDOM_FOREST: 0.9554 (95.54%)
2. SVM: 0.9530 (95.30%)  
3. GRADIENT_BOOSTING: 0.9530 (95.30%)

í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥:
- SVM: 0.9441 (94.41%)
- RANDOM_FOREST: 0.9720 (97.20%)  
- GRADIENT_BOOSTING: 0.9720 (97.20%)
```

#### ê° ëª¨ë¸ì˜ íŠ¹ì§•

##### SVM (Support Vector Machine)
- **ì¥ì **: ê³ ì°¨ì› ë°ì´í„°ì— íš¨ê³¼ì , ì»¤ë„ íŠ¸ë¦­ìœ¼ë¡œ ë¹„ì„ í˜• ë¶„ë¥˜ ê°€ëŠ¥
- **ë‹¨ì **: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ëŠë¦¼, í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë¯¼ê°
- **ì ìš©**: í…ìŠ¤íŠ¸ ë¶„ë¥˜, ì´ë¯¸ì§€ ë¶„ë¥˜

##### RandomForest
- **ì¥ì **: ê³¼ì í•© ë°©ì§€ íš¨ê³¼, íŠ¹ì„± ì¤‘ìš”ë„ ì œê³µ, ì•ˆì •ì 
- **ë‹¨ì **: í•´ì„ì´ ì–´ë ¤ì›€, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ
- **ì ìš©**: ì¼ë°˜ì ì¸ ë¶„ë¥˜/íšŒê·€ ë¬¸ì œ

##### GradientBoosting
- **ì¥ì **: ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥, ìˆœì°¨ì  í•™ìŠµìœ¼ë¡œ ì˜¤ë¥˜ ê°œì„ 
- **ë‹¨ì **: í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë¯¼ê°, ê³¼ì í•© ìœ„í—˜
- **ì ìš©**: ê²½ì§„ëŒ€íšŒ, ê³ ì„±ëŠ¥ì´ í•„ìš”í•œ ë¬¸ì œ

### 2.5 ê³¼ì í•© ê°ì§€ ë° ë°©ì§€

#### ê³¼ì í•© ê°ì§€ ë°©ë²•
```python
# í›ˆë ¨ vs í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ
train_accuracy = best_model.score(X_train, y_train)
test_accuracy = best_model.score(X_test, y_test)

overfitting_gap = train_accuracy - test_accuracy

if overfitting_gap > 0.05:
    print("ê³¼ì í•© ì˜ì‹¬ (í›ˆë ¨-í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì°¨ì´ > 5%)")
elif overfitting_gap < -0.02:
    print("ê³¼ì†Œì í•© ì˜ì‹¬")
else:
    print("ì ì ˆí•œ ëª¨ë¸")
```

#### ê³¼ì í•© ë°©ì§€ ë°©ë²•
1. **êµì°¨ê²€ì¦ ì‚¬ìš©**: 5-fold, 10-fold CV
2. **ì •ê·œí™” ì ìš©**: L1, L2 ì •ê·œí™”
3. **ì¡°ê¸° ì¢…ë£Œ**: Early Stopping
4. **ë°ì´í„° ì¦ê°•**: Data Augmentation
5. **ì•™ìƒë¸” ê¸°ë²•**: Voting, Bagging

---

## ğŸ“Š 3. ëª¨ë¸ í‰ê°€ ë° ë¹„êµ

### 3.1 í‰ê°€ ì§€í‘œ

#### ë¶„ë¥˜ ë¬¸ì œ ì£¼ìš” ì§€í‘œ
```python
from sklearn.metrics import classification_report, confusion_matrix

# ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ
print(classification_report(y_test, y_pred, target_names=['ì•…ì„±', 'ì–‘ì„±']))

# í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(y_test, y_pred)
```

#### ì£¼ìš” ì§€í‘œ ì„¤ëª…
- **ì •í™•ë„ (Accuracy)**: ì „ì²´ ì˜ˆì¸¡ ì¤‘ ë§ì¶˜ ë¹„ìœ¨
- **ì •ë°€ë„ (Precision)**: ì–‘ì„± ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨
- **ì¬í˜„ìœ¨ (Recall)**: ì‹¤ì œ ì–‘ì„± ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨  
- **F1-Score**: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· 
- **íŠ¹ì´ë„ (Specificity)**: ì‹¤ì œ ìŒì„± ì¤‘ ìŒì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨

### 3.2 êµì°¨ê²€ì¦ ì „ëµ

#### K-Fold êµì°¨ê²€ì¦
```python
from sklearn.model_selection import cross_val_score

# 5-fold êµì°¨ê²€ì¦
cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"êµì°¨ê²€ì¦ í‰ê·  ì ìˆ˜: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
```

#### Stratified K-Fold
```python
from sklearn.model_selection import StratifiedKFold

# ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
```

### 3.3 ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”

#### ë§‰ëŒ€ ê·¸ë˜í”„ ë¹„êµ
```python
import matplotlib.pyplot as plt
import seaborn as sns

# ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
models = ['SVM', 'RandomForest', 'GradientBoosting']
cv_scores = [0.9530, 0.9554, 0.9530]
test_scores = [0.9441, 0.9720, 0.9720]

x_pos = np.arange(len(models))
width = 0.35

plt.figure(figsize=(10, 6))
plt.bar(x_pos - width/2, cv_scores, width, label='êµì°¨ê²€ì¦', alpha=0.8)
plt.bar(x_pos + width/2, test_scores, width, label='í…ŒìŠ¤íŠ¸', alpha=0.8)

plt.xlabel('ëª¨ë¸')
plt.ylabel('ì •í™•ë„')
plt.title('êµì°¨ê²€ì¦ vs í…ŒìŠ¤íŠ¸ ì„±ëŠ¥')
plt.xticks(x_pos, models)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

#### í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ
```python
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['ì•…ì„±', 'ì–‘ì„±'],
            yticklabels=['ì•…ì„±', 'ì–‘ì„±'])
plt.title('í˜¼ë™ í–‰ë ¬')
plt.ylabel('ì‹¤ì œ')
plt.xlabel('ì˜ˆì¸¡')
plt.show()
```

---

## ğŸ¯ 4. ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

### 4.1 íš¨ìœ¨ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì „ëµ

#### 1ë‹¨ê³„: ë„“ì€ ë²”ìœ„ íƒìƒ‰
```python
# ëŒ€ëµì ì¸ ë²”ìœ„ì—ì„œ ì‹œì‘
param_grid_coarse = {
    'C': [0.001, 0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10]
}
```

#### 2ë‹¨ê³„: ì„¸ë°€í•œ íƒìƒ‰
```python
# ìœ ë§í•œ ì˜ì—­ì—ì„œ ì„¸ë°€í•˜ê²Œ
param_grid_fine = {
    'C': [0.5, 1, 2, 5],
    'gamma': [0.01, 0.05, 0.1, 0.2]
}
```

#### 3ë‹¨ê³„: RandomizedSearchCV í™œìš©
```python
from sklearn.model_selection import RandomizedSearchCV

# ë” ë„“ì€ íƒìƒ‰ ê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=100,  # ì‹œë„ íšŸìˆ˜
    cv=5,
    random_state=42,
    n_jobs=-1
)
```

### 4.2 ëª¨ë¸ ì„ íƒ ê¸°ì¤€

#### ê³ ë ¤ì‚¬í•­
1. **ì˜ˆì¸¡ ì„±ëŠ¥**: ì •í™•ë„, F1-score ë“±
2. **í•´ì„ ê°€ëŠ¥ì„±**: íŠ¹ì„± ì¤‘ìš”ë„, ê²°ì • ê²½ê³„
3. **ê³„ì‚° íš¨ìœ¨ì„±**: í•™ìŠµ ì‹œê°„, ì˜ˆì¸¡ ì‹œê°„
4. **ì¼ë°˜í™” ëŠ¥ë ¥**: ê³¼ì í•© ìœ„í—˜ë„
5. **ë°ì´í„° íŠ¹ì„±**: í¬ê¸°, ì°¨ì›, ë…¸ì´ì¦ˆ ìˆ˜ì¤€

#### ì˜ì‚¬ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤
| ëª¨ë¸ | ì„±ëŠ¥ | í•´ì„ì„± | ì†ë„ | ì¼ë°˜í™” | ì¢…í•© ì ìˆ˜ |
|------|------|--------|------|--------|----------|
| SVM | ë†’ìŒ | ë‚®ìŒ | ë³´í†µ | ë†’ìŒ | 5/4 |
| RandomForest | ë†’ìŒ | ë³´í†µ | ë¹ ë¦„ | ë†’ìŒ | 5/5 |
| GradientBoosting | ë§¤ìš°ë†’ìŒ | ë‚®ìŒ | ëŠë¦¼ | ë³´í†µ | 5/4 |

### 4.3 íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

#### ì „ì²˜ë¦¬ + ëª¨ë¸ í†µí•©
```python
from sklearn.pipeline import Pipeline

# ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
pipeline = Pipeline([
    ('preprocessor', ColumnTransformer([
        ('scaler', StandardScaler(), numeric_features),
        ('encoder', OneHotEncoder(), categorical_features)
    ])),
    ('classifier', RandomForestClassifier(random_state=42))
])

# íŒŒì´í”„ë¼ì¸ì— GridSearch ì ìš©
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 10, 20],
    'preprocessor__scaler__with_mean': [True, False]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

### 4.4 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### ì§€ì†ì ì¸ ëª¨ë¸ í‰ê°€
```python
# ì„±ëŠ¥ ì¶”ì  í•¨ìˆ˜
def evaluate_model(model, X_test, y_test):
    """ëª¨ë¸ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€"""
    y_pred = model.predict(X_test)
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, average='weighted'),
        'recall': recall_score(y_test, y_pred, average='weighted'),
        'f1': f1_score(y_test, y_pred, average='weighted')
    }
    
    return metrics

# ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
results = {}
for name, model in models.items():
    results[name] = evaluate_model(model, X_test, y_test)
    
# ê²°ê³¼ DataFrameìœ¼ë¡œ ì •ë¦¬
import pandas as pd
results_df = pd.DataFrame(results).T
print(results_df.round(4))
```

---

## 5. ê³ ê¸‰ ê¸°ë²• ë° ë‹¤ìŒ ë‹¨ê³„

### 5.1 ê³ ê¸‰ ìµœì í™” ê¸°ë²•

#### Bayesian Optimization
```python
# Optuna ì‚¬ìš© ì˜ˆì‹œ
import optuna

def objective(trial):
    C = trial.suggest_float('C', 0.01, 100, log=True)
    gamma = trial.suggest_float('gamma', 0.001, 1, log=True)
    
    model = SVC(C=C, gamma=gamma, random_state=42)
    score = cross_val_score(model, X_train, y_train, cv=5).mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

#### AutoML ë„êµ¬
- **Auto-sklearn**: ìë™ ëª¨ë¸ ì„ íƒ ë° íŠœë‹
- **TPOT**: ìœ ì „ì ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ìµœì í™”
- **H2O AutoML**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AutoML

### 5.2 ì•™ìƒë¸” ê¸°ë²•

#### Voting Classifier
```python
from sklearn.ensemble import VotingClassifier

# ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©
voting_clf = VotingClassifier(
    estimators=[
        ('svm', best_svm),
        ('rf', best_rf), 
        ('gb', best_gb)
    ],
    voting='soft'  # í™•ë¥  í‰ê· 
)

voting_clf.fit(X_train, y_train)
```

#### Stacking
```python
from sklearn.ensemble import StackingClassifier

# ë©”íƒ€ í•™ìŠµê¸° ì‚¬ìš©
stacking_clf = StackingClassifier(
    estimators=[
        ('svm', best_svm),
        ('rf', best_rf)
    ],
    final_estimator=LogisticRegression(),
    cv=5
)
```

### 5.3 ëª¨ë¸ í•´ì„

#### íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
```python
# RandomForest íŠ¹ì„± ì¤‘ìš”ë„
feature_importance = best_rf.feature_importances_
feature_names = X.columns

# ì‹œê°í™”
plt.figure(figsize=(10, 8))
indices = np.argsort(feature_importance)[::-1][:10]
plt.barh(range(10), feature_importance[indices])
plt.yticks(range(10), [feature_names[i] for i in indices])
plt.title('íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)')
plt.show()
```

#### SHAP ê°’ í™œìš©
```python
import shap

# SHAP ì„¤ëª…ì ìƒì„±
explainer = shap.TreeExplainer(best_rf)
shap_values = explainer.shap_values(X_test[:100])

# ìš”ì•½ í”Œë¡¯
shap.summary_plot(shap_values[1], X_test[:100])
```

---

## 6. ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 6.1 ë°ì´í„° ì „ì²˜ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ê²°ì¸¡ê°’ ì²˜ë¦¬**: ì ì ˆí•œ ëŒ€ì¹˜ ë°©ë²• ì„ íƒ
- [ ] **ì´ìƒê°’ íƒì§€**: IQR, Z-score ë“± í™œìš©
- [ ] **ë²”ì£¼í˜• ì¸ì½”ë”©**: ìƒí™©ì— ë§ëŠ” ë°©ë²• ì„ íƒ
- [ ] **ìˆ˜ì¹˜í˜• ìŠ¤ì¼€ì¼ë§**: StandardScaler, MinMaxScaler ë“±
- [ ] **íŠ¹ì„± ì„ íƒ**: ìƒê´€ê´€ê³„, ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ
- [ ] **ë°ì´í„° ë¶„í• **: train/validation/test ì ì ˆí•œ ë¹„ìœ¨

### 6.2 ëª¨ë¸ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ê¸°ì¤€ ëª¨ë¸**: ë‹¨ìˆœí•œ ëª¨ë¸ë¡œ ê¸°ì¤€ ì„¤ì •
- [ ] **êµì°¨ê²€ì¦**: ì ì ˆí•œ CV ì „ëµ ì„ íƒ
- [ ] **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ì²´ê³„ì ì¸ íƒìƒ‰ ì „ëµ
- [ ] **ê³¼ì í•© í™•ì¸**: í›ˆë ¨/ê²€ì¦ ì„±ëŠ¥ ì°¨ì´ ëª¨ë‹ˆí„°ë§
- [ ] **ë‹¤ì¤‘ ëª¨ë¸**: ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
- [ ] **ì•™ìƒë¸”**: ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ëª¨ë¸ ì¡°í•©

### 6.3 í‰ê°€ ë° ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ì ì ˆí•œ ì§€í‘œ**: ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì— ë§ëŠ” ì§€í‘œ ì„ íƒ
- [ ] **í†µê³„ì  ìœ ì˜ì„±**: ì„±ëŠ¥ ì°¨ì´ì˜ ì‹ ë¢°ì„± ê²€ì¦
- [ ] **ì‹¤ì œ ë°ì´í„°**: ìµœì‹  ë°ì´í„°ë¡œ ì„±ëŠ¥ ì¬ê²€ì¦
- [ ] **ì—ëŸ¬ ë¶„ì„**: ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„
- [ ] **í•´ì„ ê°€ëŠ¥ì„±**: ëª¨ë¸ ê²°ì • ê³¼ì • ì´í•´
- [ ] **ë°°í¬ ì¤€ë¹„**: ì‹¤ì œ í™˜ê²½ ê³ ë ¤ì‚¬í•­ ì ê²€

---

## ğŸ“ 7. í•™ìŠµ ì •ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„

### 7.1 í•µì‹¬ í•™ìŠµ ë‚´ìš©

#### ë°ì´í„° ì „ì²˜ë¦¬
- ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ 3ê°€ì§€ ë°©ë²• (`get_dummies`, `OneHotEncoder`, `ColumnTransformer`)
- ê° ë°©ë²•ì˜ ì¥ë‹¨ì  ë° ì ìš© ì‹œë‚˜ë¦¬ì˜¤
- ìˆ«ìí˜• ë²”ì£¼ ë°ì´í„°ì˜ ì˜¬ë°”ë¥¸ ì²˜ë¦¬ë²•

#### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- GridSearchCVë¥¼ í†µí•œ ì²´ê³„ì  ìµœì í™”
- êµì°¨ê²€ì¦ê³¼ ê³¼ì í•© ê°ì§€
- ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ ë° ì„±ëŠ¥ í‰ê°€
- SVM, RandomForest, GradientBoosting íŠ¹ì„± ì´í•´

#### ëª¨ë¸ í‰ê°€
- ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ í™œìš©
- í˜¼ë™ í–‰ë ¬ ë° ë¶„ë¥˜ ë³´ê³ ì„œ í•´ì„
- ì‹œê°í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ë¹„êµ

### 7.2 ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸

1. **ë‹¨ê³„ì  ì ‘ê·¼**: ê°„ë‹¨í•œ ëª¨ë¸ â†’ ë³µì¡í•œ ëª¨ë¸
2. **ì²´ê³„ì  íŠœë‹**: ë„“ì€ íƒìƒ‰ â†’ ì„¸ë°€í•œ ì¡°ì •  
3. **ì¢…í•©ì  í‰ê°€**: ì„±ëŠ¥ + í•´ì„ì„± + íš¨ìœ¨ì„±
4. **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**: ëª¨ë¸ ì„±ëŠ¥ ì¶”ì  ë° ì—…ë°ì´íŠ¸

### 7.3 ë‹¤ìŒ í•™ìŠµ ê¶Œì¥ì‚¬í•­

#### ë‹¨ê¸° ëª©í‘œ (1-2ì£¼)
- [ ] **RandomizedSearchCV** ë° **Bayesian Optimization** í•™ìŠµ
- [ ] **ì•™ìƒë¸” ê¸°ë²•** (Voting, Bagging, Stacking) ì‹¤ìŠµ
- [ ] **íŠ¹ì„± ì„ íƒ** ë° **ì°¨ì› ì¶•ì†Œ** ê¸°ë²• í•™ìŠµ

#### ì¤‘ê¸° ëª©í‘œ (1-2ê°œì›”)  
- [ ] **AutoML ë„êµ¬** (Auto-sklearn, TPOT) í™œìš©
- [ ] **ëª¨ë¸ í•´ì„** ê¸°ë²• (SHAP, LIME) í•™ìŠµ
- [ ] **ì‹¤ì œ í”„ë¡œì íŠ¸** ë°ì´í„°ë¡œ end-to-end íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

#### ì¥ê¸° ëª©í‘œ (3-6ê°œì›”)
- [ ] **ë”¥ëŸ¬ë‹** í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] **ë¶„ì‚° ì»´í“¨íŒ…** í™˜ê²½ì—ì„œì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- [ ] **MLOps** íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ë°°í¬

### 7.4 ì¶”ê°€ í•™ìŠµ ìë£Œ

#### ë„ì„œ
- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman
- "Pattern Recognition and Machine Learning" - Christopher Bishop

#### ì˜¨ë¼ì¸ ìë£Œ
- Scikit-learn ê³µì‹ ë¬¸ì„œ: https://scikit-learn.org/
- Kaggle Learn: https://www.kaggle.com/learn
- Fast.ai: https://www.fast.ai/

#### ì‹¤ìŠµ í”Œë«í¼
- **Kaggle**: ê²½ì§„ëŒ€íšŒ ë° ë°ì´í„°ì…‹
- **Google Colab**: ë¬´ë£Œ GPU í™˜ê²½
- **Papers with Code**: ìµœì‹  ì—°êµ¬ ë™í–¥

---

## ë§ˆë¬´ë¦¬

### ì£¼ìš” ì„±ê³¼
- **ë°ì´í„° ì „ì²˜ë¦¬**: ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ ë§ˆìŠ¤í„°
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: ì²´ê³„ì  ëª¨ë¸ ìµœì í™” ë°©ë²• ìŠµë“
- **ëª¨ë¸ í‰ê°€**: ì¢…í•©ì  ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ ëŠ¥ë ¥ í™•ë³´
- **ì‹¤ë¬´ ì—­ëŸ‰**: ì‹¤ì œ í”„ë¡œì íŠ¸ ì ìš© ê°€ëŠ¥í•œ ê¸°ìˆ  ìŠ¤íƒ êµ¬ì¶•

### ë‹¤ìŒ ì—¬ì •
ë¨¸ì‹ ëŸ¬ë‹ì˜ í•µì‹¬ ê¸°ë²•ë“¤ì„ ìµí˜”ìœ¼ë‹ˆ, ì´ì œ ë” ë³µì¡í•˜ê³  ì‹¤ì „ì ì¸ ë¬¸ì œë“¤ì— ë„ì „í•´ë³¼ ì°¨ë¡€ì…ë‹ˆë‹¤. 
