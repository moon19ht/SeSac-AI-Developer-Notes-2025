# ğŸ§  Deep Learning ì™„ì „ ì •ë³µ ê°€ì´ë“œ

##### ğŸ“ Writer : Moon19ht

---

## ëª©ì°¨

1. [íŒŒì´ì¬ ê¸°ì´ˆ ë° ê°œë°œí™˜ê²½ ì„¤ì •](#1-íŒŒì´ì¬-ê¸°ì´ˆ-ë°-ê°œë°œí™˜ê²½-ì„¤ì •)
2. [ì›¹ í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆ](#2-ì›¹-í”„ë¡œê·¸ë˜ë°-ê¸°ì´ˆ)
3. [ì›¹ í¬ë¡¤ë§](#3-ì›¹-í¬ë¡¤ë§)
4. [í†µê³„í•™ ê¸°ì´ˆ](#4-í†µê³„í•™-ê¸°ì´ˆ)
5. [ìˆ˜í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©](#5-ìˆ˜í•™-ë¼ì´ë¸ŒëŸ¬ë¦¬-í™œìš©)
6. [ì´ë¯¸ì§€ ì²˜ë¦¬](#6-ì´ë¯¸ì§€-ì²˜ë¦¬)
7. [ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ](#7-ë¨¸ì‹ ëŸ¬ë‹-ê¸°ì´ˆ)
8. [ë”¥ëŸ¬ë‹ í•µì‹¬ ê°œë…](#8-ë”¥ëŸ¬ë‹-í•µì‹¬-ê°œë…)
9. [ì‹ ê²½ë§ êµ¬ì¡°](#9-ì‹ ê²½ë§-êµ¬ì¡°)
10. [CNN (í•©ì„±ê³± ì‹ ê²½ë§)](#10-cnn-í•©ì„±ê³±-ì‹ ê²½ë§)
11. [RNN (ìˆœí™˜ ì‹ ê²½ë§)](#11-rnn-ìˆœí™˜-ì‹ ê²½ë§)
12. [ê³ ê¸‰ ë”¥ëŸ¬ë‹ ê¸°ë²•](#12-ê³ ê¸‰-ë”¥ëŸ¬ë‹-ê¸°ë²•)
13. [ëª¨ë¸ í‰ê°€ ë° ìµœì í™”](#13-ëª¨ë¸-í‰ê°€-ë°-ìµœì í™”)
14. [ì‹¤ì „ í”„ë¡œì íŠ¸](#14-ì‹¤ì „-í”„ë¡œì íŠ¸)

---

## 1. íŒŒì´ì¬ ê¸°ì´ˆ ë° ê°œë°œí™˜ê²½ ì„¤ì •

### 1.1 íŒŒì´ì¬ ì†Œê°œ

**íŒŒì´ì¬(Python)**ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì¸(Guido van Rossum)ì´ ë°œí‘œí•œ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì…ë‹ˆë‹¤.

#### íŒŒì´ì¬ì˜ íŠ¹ì§•
- **ê°€ë…ì„±**: ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ì½”ë“œ
- **ë“¤ì—¬ì“°ê¸°**: ì½”ë“œ ë¸”ë¡ì„ ë“¤ì—¬ì“°ê¸°ë¡œ êµ¬ë¶„
- **í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬**: ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê³µ
- **ë™ì  íƒ€ì´í•‘**: ëŸ°íƒ€ì„ì— íƒ€ì… ê²°ì •
- **ë¬´ë£Œ**: ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ì„¼ìŠ¤
- **ì ‘ì°©ì„±**: Cì–¸ì–´ì™€ì˜ ê²°í•©ì´ ìš©ì´
- **í¬ë¡œìŠ¤ í”Œë«í¼**: Windows, macOS, Linux ì§€ì›

#### íŒŒì´ì¬ì˜ í™œìš© ë¶„ì•¼
- ì‹œìŠ¤í…œ ìœ í‹¸ë¦¬í‹° ì œì‘
- GUI í”„ë¡œê·¸ë˜ë°
- ì›¹ í”„ë¡œê·¸ë˜ë° (Django, Flask, FastAPI)
- ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”
- ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹
- í¬ë¡¤ë§ ë° ìë™í™”
- IoT ë° ì„ë² ë””ë“œ ì‹œìŠ¤í…œ
- ê²Œì„ ê°œë°œ
- ë¸”ë¡ì²´ì¸ ê°œë°œ

> **ì°¸ê³ **: íŒŒì´ì¬ì€ í•™ìŠµí•˜ê¸° ì‰½ê³  ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ë°ì´í„° ê³¼í•™ ë¶„ì•¼ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.

### 1.2 ê°œë°œí™˜ê²½ ì„¤ì •

#### Anaconda ì„¤ì¹˜ ë° ì„¤ì •
1. [Anaconda ê³µì‹ ì‚¬ì´íŠ¸](https://www.anaconda.com/download)ì—ì„œ ìš´ì˜ì²´ì œì— ë§ëŠ” ë²„ì „ ë‹¤ìš´ë¡œë“œ
2. ì„¤ì¹˜ ì‹œ "Add Anaconda to PATH" ì˜µì…˜ ì²´í¬ (ê¶Œì¥)
3. ì„¤ì¹˜ ì™„ë£Œ í›„ Anaconda Prompt ë˜ëŠ” í„°ë¯¸ë„ ì‹¤í–‰

#### ê°€ìƒí™˜ê²½ ìƒì„± ë° ê´€ë¦¬
```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (Python 3.9 ê¶Œì¥)
conda create --name deeplearning python=3.9

# ê°€ìƒí™˜ê²½ í™œì„±í™”
conda activate deeplearning

# ê°€ìƒí™˜ê²½ ë¹„í™œì„±í™”
conda deactivate

# ê°€ìƒí™˜ê²½ ëª©ë¡ í™•ì¸
conda env list

# ê°€ìƒí™˜ê²½ ì‚­ì œ
conda remove --name deeplearning --all
```

#### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ ë°ì´í„° ê³¼í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬
conda install numpy pandas matplotlib seaborn scikit-learn

# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
pip install tensorflow
pip install torch torchvision torchaudio

# ì¶”ê°€ ìœ ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install jupyter jupyterlab
pip install opencv-python
pip install pillow
pip install requests beautifulsoup4
pip install plotly
pip install streamlit
```

#### GPU ì§€ì› ì„¤ì • (ì„ íƒì‚¬í•­)
```bash
# NVIDIA GPU ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
nvcc --version

# PyTorch GPU ë²„ì „ ì„¤ì¹˜ (CUDA 11.8 ê¸°ì¤€)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# TensorFlow GPU ë²„ì „ (TensorFlow 2.10+ ìë™ GPU ì§€ì›)
pip install tensorflow[and-cuda]
```

#### ì„¤ì¹˜ í™•ì¸
```python
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import torch

print(f"Python ë²„ì „: {sys.version}")
print(f"NumPy ë²„ì „: {np.__version__}")
print(f"Pandas ë²„ì „: {pd.__version__}")
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"PyTorch ë²„ì „: {torch.__version__}")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
print(f"TensorFlow GPU ì‚¬ìš© ê°€ëŠ¥: {tf.config.list_physical_devices('GPU')}")
print(f"PyTorch GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU ì¥ì¹˜ëª…: {torch.cuda.get_device_name(0)}")
```

### 1.3 ê°œë°œ ë„êµ¬ ì„¤ì •

#### Visual Studio Code ì„¤ì •
1. **Python í™•ì¥íŒ© ì„¤ì¹˜**
   - Python (Microsoft)
   - Pylance
   - Python Docstring Generator
   - autoDocstring

2. **ì‘ì—… í™˜ê²½ ì„¤ì •**
   ```json
   // settings.json
   {
       "python.defaultInterpreterPath": "~/anaconda3/envs/deeplearning/bin/python",
       "python.linting.enabled": true,
       "python.linting.pylintEnabled": true,
       "python.formatting.provider": "black",
       "python.terminal.activateEnvironment": true
   }
   ```

3. **ìœ ìš©í•œ ë‹¨ì¶•í‚¤**
   - `Ctrl + Shift + P`: ëª…ë ¹ íŒ”ë ˆíŠ¸
   - `Ctrl + /`: ì£¼ì„ í† ê¸€
   - `Shift + Alt + F`: ì½”ë“œ í¬ë§·íŒ…
   - `F5`: ë””ë²„ê¹… ì‹œì‘

#### Jupyter Notebook/Lab ì„¤ì •
```bash
# Jupyter í™•ì¥ ì„¤ì¹˜
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user

# JupyterLab í™•ì¥
pip install jupyterlab-git
pip install jupyterlab-lsp
pip install python-lsp-server[all]

# Jupyter ì‹œì‘
jupyter notebook  # ê¸°ë³¸ ë…¸íŠ¸ë¶
jupyter lab      # JupyterLab (ê¶Œì¥)
```

**ì£¼ìš” ë‹¨ì¶•í‚¤**:
- `Shift + Enter`: ì…€ ì‹¤í–‰ í›„ ë‹¤ìŒ ì…€ë¡œ ì´ë™
- `Ctrl + Enter`: ì…€ ì‹¤í–‰
- `Alt + Enter`: ì…€ ì‹¤í–‰ í›„ ìƒˆ ì…€ ì¶”ê°€
- `A`: ìœ„ì— ìƒˆ ì…€ ì¶”ê°€
- `B`: ì•„ë˜ì— ìƒˆ ì…€ ì¶”ê°€
- `DD`: ì…€ ì‚­ì œ
- `M`: ë§ˆí¬ë‹¤ìš´ ì…€ë¡œ ë³€ê²½
- `Y`: ì½”ë“œ ì…€ë¡œ ë³€ê²½

#### í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •
```
deeplearning_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed/     # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ external/      # ì™¸ë¶€ ë°ì´í„°
â”œâ”€â”€ notebooks/         # Jupyter ë…¸íŠ¸ë¶
â”œâ”€â”€ src/              # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ data/         # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ features/     # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
â”‚   â”œâ”€â”€ models/       # ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ visualization/ # ì‹œê°í™”
â”œâ”€â”€ models/           # ì €ì¥ëœ ëª¨ë¸
â”œâ”€â”€ reports/          # ë³´ê³ ì„œ
â”œâ”€â”€ requirements.txt  # ì˜ì¡´ì„± ëª©ë¡
â””â”€â”€ README.md        # í”„ë¡œì íŠ¸ ì„¤ëª…
```

---

## 2. ì›¹ í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆ

### 2.1 Flask ê¸°ì´ˆ

FlaskëŠ” Pythonìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ë§ˆì´í¬ë¡œ ì›¹ í”„ë ˆì„ì›Œí¬ë¡œ, ê°„ë‹¨í•˜ê³  ìœ ì—°í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì— ì í•©í•©ë‹ˆë‹¤.

#### ì„¤ì¹˜ ë° ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
pip install flask flask-cors flask-restful
```

```python
# app.py
from flask import Flask, jsonify, request
from flask_cors import CORS

app = Flask(__name__)
CORS(app)  # CORS ì„¤ì •ìœ¼ë¡œ ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥

@app.route("/")
def home():
    return jsonify({
        "message": "ë”¥ëŸ¬ë‹ API ì„œë²„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!",
        "status": "success"
    })

@app.route("/health")
def health_check():
    return jsonify({"status": "healthy"})

if __name__ == "__main__":
    app.run(debug=True, host='0.0.0.0', port=5000)
```

#### ë¼ìš°íŒ…ê³¼ HTTP ë©”ì„œë“œ
```python
from flask import Flask, request, jsonify

app = Flask(__name__)

# GET ìš”ì²­
@app.route('/users', methods=['GET'])
def get_users():
    return jsonify({"users": ["Alice", "Bob", "Charlie"]})

# POST ìš”ì²­
@app.route('/users', methods=['POST'])
def create_user():
    data = request.get_json()
    if not data or 'name' not in data:
        return jsonify({"error": "ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤"}), 400
    
    return jsonify({
        "message": f"ì‚¬ìš©ì {data['name']}ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
        "user": data
    }), 201

# ê²½ë¡œ ë§¤ê°œë³€ìˆ˜
@app.route('/users/<int:user_id>')
def get_user(user_id):
    return jsonify({"user_id": user_id, "name": f"User{user_id}"})

# ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜
@app.route('/search')
def search():
    query = request.args.get('q', '')
    limit = request.args.get('limit', 10, type=int)
    
    return jsonify({
        "query": query,
        "limit": limit,
        "results": []
    })
```

> **ì°¸ê³ **: `debug=True` ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ì½”ë“œ ë³€ê²½ ì‹œ ìë™ìœ¼ë¡œ ì„œë²„ê°€ ì¬ì‹œì‘ë˜ì–´ ê°œë°œì´ í¸ë¦¬í•©ë‹ˆë‹¤.

### 2.2 RESTful API ì„¤ê³„

```python
from flask import Flask, request, jsonify
from flask_restful import Resource, Api
import uuid
from datetime import datetime

app = Flask(__name__)
api = Api(app)

# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©)
models = {}

class ModelResource(Resource):
    def get(self, model_id=None):
        if model_id:
            if model_id in models:
                return models[model_id]
            return {"error": "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}, 404
        return {"models": list(models.values())}
    
    def post(self):
        data = request.get_json()
        
        # ì…ë ¥ ê²€ì¦
        required_fields = ['name', 'type', 'accuracy']
        for field in required_fields:
            if field not in data:
                return {"error": f"{field}ëŠ” í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤"}, 400
        
        # ëª¨ë¸ ìƒì„±
        model_id = str(uuid.uuid4())
        model = {
            "id": model_id,
            "name": data['name'],
            "type": data['type'],
            "accuracy": data['accuracy'],
            "created_at": datetime.now().isoformat(),
            "status": "active"
        }
        
        models[model_id] = model
        return model, 201
    
    def put(self, model_id):
        if model_id not in models:
            return {"error": "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}, 404
        
        data = request.get_json()
        models[model_id].update(data)
        models[model_id]["updated_at"] = datetime.now().isoformat()
        
        return models[model_id]
    
    def delete(self, model_id):
        if model_id not in models:
            return {"error": "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}, 404
        
        del models[model_id]
        return {"message": "ëª¨ë¸ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}

# ë¼ìš°íŠ¸ ë“±ë¡
api.add_resource(ModelResource, '/api/models', '/api/models/<string:model_id>')

if __name__ == '__main__':
    app.run(debug=True)
```

---

## 3. ì›¹ í¬ë¡¤ë§

### 3.1 í¬ë¡¤ë§ ê¸°ì´ˆ ê°œë…

ì›¹ í¬ë¡¤ë§ì€ ì›¹ìƒì˜ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ê¸°ìˆ ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.

#### í¬ë¡¤ë§ vs ìŠ¤í¬ë˜í•‘
- **í¬ë¡¤ë§**: ì›¹ì‚¬ì´íŠ¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ë§í¬ë¥¼ ë”°ë¼ê°€ë©° ë°ì´í„° ìˆ˜ì§‘
- **ìŠ¤í¬ë˜í•‘**: íŠ¹ì • ì›¹í˜ì´ì§€ì—ì„œ ì›í•˜ëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œ

#### ë²•ì /ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­
- robots.txt íŒŒì¼ í™•ì¸ ë° ì¤€ìˆ˜
- ì›¹ì‚¬ì´íŠ¸ ì´ìš©ì•½ê´€ ê²€í† 
- ì ì ˆí•œ ìš”ì²­ ê°„ê²© ìœ ì§€ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
- ì €ì‘ê¶Œ ë° ê°œì¸ì •ë³´ ë³´í˜¸ ì¤€ìˆ˜

#### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install requests beautifulsoup4 selenium lxml
pip install pandas numpy matplotlib
```

### 3.2 Requestsì™€ BeautifulSoup

#### ê¸°ë³¸ HTTP ìš”ì²­
```python
import requests
from bs4 import BeautifulSoup
import time
import random

def make_request(url, headers=None, timeout=10):
    """ì•ˆì „í•œ HTTP ìš”ì²­ í•¨ìˆ˜"""
    default_headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    if headers:
        default_headers.update(headers)
    
    try:
        response = requests.get(url, headers=default_headers, timeout=timeout)
        response.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        return response
    except requests.RequestException as e:
        print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

# ì‚¬ìš© ì˜ˆì œ
url = 'https://httpbin.org/html'
response = make_request(url)

if response:
    print(f"ìƒíƒœ ì½”ë“œ: {response.status_code}")
    print(f"ì‘ë‹µ í—¤ë”: {response.headers}")
    print(f"ì½˜í…ì¸  íƒ€ì…: {response.headers.get('content-type')}")
```

#### BeautifulSoupì„ ì´ìš©í•œ HTML íŒŒì‹±
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_quotes():
    """ëª…ì–¸ ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì˜ˆì œ"""
    base_url = "http://quotes.toscrape.com"
    quotes_data = []
    page = 1
    
    while True:
        url = f"{base_url}/page/{page}/"
        response = make_request(url)
        
        if not response:
            break
            
        soup = BeautifulSoup(response.content, 'html.parser')
        quotes = soup.find_all('div', class_='quote')
        
        if not quotes:  # ë” ì´ìƒ ëª…ì–¸ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
            break
        
        for quote in quotes:
            text = quote.find('span', class_='text').get_text()
            author = quote.find('small', class_='author').get_text()
            tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]
            
            quotes_data.append({
                'text': text,
                'author': author,
                'tags': ', '.join(tags)
            })
        
        print(f"í˜ì´ì§€ {page} ì™„ë£Œ: {len(quotes)}ê°œ ëª…ì–¸ ìˆ˜ì§‘")
        page += 1
        
        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
        time.sleep(random.uniform(1, 3))
    
    return pd.DataFrame(quotes_data)

# ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
quotes_df = scrape_quotes()
print(f"ì´ {len(quotes_df)}ê°œì˜ ëª…ì–¸ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
quotes_df.to_csv('quotes.csv', index=False, encoding='utf-8')
```

> **ì°¸ê³ **: ì›¹ í¬ë¡¤ë§ ì‹œ robots.txtë¥¼ í™•ì¸í•˜ê³  ì„œë²„ì— ê³¼ë„í•œ ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šë„ë¡ ì ì ˆí•œ ì§€ì—° ì‹œê°„ì„ ë‘ì„¸ìš”.

### 3.3 Seleniumì„ ì´ìš©í•œ ë™ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§

JavaScriptê°€ ë§ì´ ì‚¬ìš©ëœ ë™ì  ì›¹í˜ì´ì§€ë‚˜ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•œ ê²½ìš° Seleniumì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import time

def setup_driver(headless=True):
    """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    chrome_options = Options()
    
    if headless:
        chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€
    
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    
    # User-Agent ì„¤ì •
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
    
    driver = webdriver.Chrome(options=chrome_options)
    return driver

def scrape_dynamic_content():
    """ë™ì  ì½˜í…ì¸  í¬ë¡¤ë§ ì˜ˆì œ"""
    driver = setup_driver(headless=False)
    
    try:
        # í˜ì´ì§€ ë¡œë“œ
        driver.get("https://example.com")
        
        # íŠ¹ì • ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        wait = WebDriverWait(driver, 10)
        element = wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "content"))
        )
        
        # ìš”ì†Œ ì°¾ê¸° (ë‹¤ì–‘í•œ ë°©ë²•)
        title = driver.find_element(By.TAG_NAME, "h1").text
        links = driver.find_elements(By.TAG_NAME, "a")
        
        # ìŠ¤í¬ë¡¤ (ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€ì˜ ê²½ìš°)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        
        # í´ë¦­ ì´ë²¤íŠ¸
        button = driver.find_element(By.ID, "load-more")
        if button.is_enabled():
            button.click()
            time.sleep(2)
        
        # í¼ ì…ë ¥
        search_box = driver.find_element(By.NAME, "search")
        search_box.send_keys("ë”¥ëŸ¬ë‹")
        search_box.submit()
        
        # ê²°ê³¼ ëŒ€ê¸°
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "results")))
        
        return {"title": title, "links": [link.get_attribute("href") for link in links]}
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
        
    finally:
        driver.quit()

# ì‚¬ìš© ì˜ˆì œ
result = scrape_dynamic_content()
if result:
    print(f"ì œëª©: {result['title']}")
    print(f"ë§í¬ ìˆ˜: {len(result['links'])}")
``` 

---

## 4. í†µê³„í•™ ê¸°ì´ˆ

### 4.1 í†µê³„í•™ì˜ í•„ìš”ì„±

ë°ì´í„° ë¶„ì„ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ í†µê³„í•™ì€ í•„ìˆ˜ì ì¸ ê¸°ì´ˆ ì§€ì‹ì…ë‹ˆë‹¤. ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ë©°, ê²°ê³¼ë¥¼ í•´ì„í•˜ëŠ” ë° í†µê³„í•™ì  ì§€ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤.

#### í†µê³„í•™ì˜ ë¶„ë¥˜
- **ê¸°ìˆ í†µê³„í•™(Descriptive Statistics)**: ë°ì´í„°ì˜ ì •ë¦¬, ìš”ì•½, ì‹œê°í™”
- **ì¶”ë¡ í†µê³„í•™(Inferential Statistics)**: í‘œë³¸ì„ í†µí•œ ëª¨ì§‘ë‹¨ì˜ íŠ¹ì„± ì¶”ì •

#### ëª¨ì§‘ë‹¨ê³¼ í‘œë³¸
- **ëª¨ì§‘ë‹¨(Population)**: ì—°êµ¬ ëŒ€ìƒì´ ë˜ëŠ” ì „ì²´ ì§‘ë‹¨
- **í‘œë³¸(Sample)**: ëª¨ì§‘ë‹¨ì—ì„œ ì„ íƒëœ ì¼ë¶€ ê°œì²´ë“¤
- **ëª¨ìˆ˜(Parameter)**: ëª¨ì§‘ë‹¨ì˜ íŠ¹ì„±ê°’ (Î¼, Ïƒ ë“±)
- **í†µê³„ëŸ‰(Statistic)**: í‘œë³¸ì˜ íŠ¹ì„±ê°’ (xÌ„, s ë“±)

### 4.2 ëŒ€í‘œê°’ê³¼ ì‚°í¬ë„

#### ì¤‘ì‹¬ê²½í–¥ì„± ì¸¡ë„
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
np.random.seed(42)
data = np.random.normal(100, 15, 1000)  # í‰ê·  100, í‘œì¤€í¸ì°¨ 15
data_with_outliers = np.append(data, [150, 160, 170])  # ì´ìƒì¹˜ ì¶”ê°€

def calculate_central_tendency(data):
    """ì¤‘ì‹¬ê²½í–¥ì„± ì¸¡ë„ ê³„ì‚°"""
    return {
        'ì‚°ìˆ í‰ê· ': np.mean(data),
        'ì¤‘ìœ„ê°’': np.median(data),
        'ìµœë¹ˆê°’': stats.mode(data, keepdims=True)[0][0],
        'ê¸°í•˜í‰ê· ': stats.gmean(data[data > 0]),  # ì–‘ìˆ˜ë§Œ
        'ì¡°í™”í‰ê· ': stats.hmean(data[data > 0])   # ì–‘ìˆ˜ë§Œ
    }

# ì •ìƒ ë°ì´í„°ì™€ ì´ìƒì¹˜ í¬í•¨ ë°ì´í„° ë¹„êµ
normal_stats = calculate_central_tendency(data)
outlier_stats = calculate_central_tendency(data_with_outliers)

print("ì •ìƒ ë°ì´í„°:")
for key, value in normal_stats.items():
    print(f"  {key}: {value:.2f}")

print("\nì´ìƒì¹˜ í¬í•¨ ë°ì´í„°:")
for key, value in outlier_stats.items():
    print(f"  {key}: {value:.2f}")
```

#### ì‚°í¬ë„ ì¸¡ë„
```python
def calculate_dispersion(data):
    """ì‚°í¬ë„ ì¸¡ë„ ê³„ì‚°"""
    return {
        'ë²”ìœ„': np.max(data) - np.min(data),
        'ë¶„ì‚°': np.var(data, ddof=1),  # í‘œë³¸ë¶„ì‚°
        'í‘œì¤€í¸ì°¨': np.std(data, ddof=1),  # í‘œë³¸í‘œì¤€í¸ì°¨
        'ë³€ë™ê³„ìˆ˜': np.std(data, ddof=1) / np.mean(data) * 100,
        'ì‚¬ë¶„ìœ„ë²”ìœ„': np.percentile(data, 75) - np.percentile(data, 25),
        'í‰ê· ì ˆëŒ€í¸ì°¨': np.mean(np.abs(data - np.mean(data)))
    }

dispersion_stats = calculate_dispersion(data)
print("ì‚°í¬ë„ ì¸¡ë„:")
for key, value in dispersion_stats.items():
    print(f"  {key}: {value:.2f}")
```

#### ë¶„í¬ì˜ í˜•íƒœ
```python
def analyze_distribution_shape(data):
    """ë¶„í¬ì˜ í˜•íƒœ ë¶„ì„"""
    return {
        'ì™œë„(Skewness)': stats.skew(data),      # ë¹„ëŒ€ì¹­ì„±
        'ì²¨ë„(Kurtosis)': stats.kurtosis(data),  # ë¾°ì¡±í•¨
        'ì •ê·œì„± ê²€ì •(p-value)': stats.normaltest(data)[1]
    }

shape_stats = analyze_distribution_shape(data)
print("ë¶„í¬ì˜ í˜•íƒœ:")
for key, value in shape_stats.items():
    print(f"  {key}: {value:.4f}")

# ì •ê·œì„± í•´ì„
if shape_stats['ì •ê·œì„± ê²€ì •(p-value)'] > 0.05:
    print("  -> ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Î±=0.05)")
else:
    print("  -> ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (Î±=0.05)")
```

### 4.3 í™•ë¥ ë¶„í¬

#### ì£¼ìš” í™•ë¥ ë¶„í¬
```python
import matplotlib.pyplot as plt
from scipy.stats import norm, binom, poisson, uniform

# í•œê¸€ í°íŠ¸ ì„¤ì • (í¬ë¡œìŠ¤ í”Œë«í¼ ëŒ€ì‘)
try:
    # Windows
    plt.rcParams['font.family'] = 'Malgun Gothic'
except:
    try:
        # macOS
        plt.rcParams['font.family'] = 'AppleGothic'
    except:
        # Linux ë˜ëŠ” ê¸°íƒ€
        plt.rcParams['font.family'] = 'DejaVu Sans'

plt.rcParams['axes.unicode_minus'] = False

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# ì •ê·œë¶„í¬
x = np.linspace(-4, 4, 1000)
axes[0, 0].plot(x, norm.pdf(x, 0, 1), 'b-', label='N(0,1)')
axes[0, 0].plot(x, norm.pdf(x, 0, 2), 'r-', label='N(0,2)')
axes[0, 0].set_title('ì •ê·œë¶„í¬')
axes[0, 0].legend()
axes[0, 0].grid(True)

# ì´í•­ë¶„í¬
n, p = 20, 0.3
x_binom = np.arange(0, n+1)
axes[0, 1].bar(x_binom, binom.pmf(x_binom, n, p), alpha=0.7)
axes[0, 1].set_title(f'ì´í•­ë¶„í¬ B({n}, {p})')
axes[0, 1].grid(True)

# í¬ì•„ì†¡ë¶„í¬
lam = 3
x_poisson = np.arange(0, 15)
axes[1, 0].bar(x_poisson, poisson.pmf(x_poisson, lam), alpha=0.7, color='green')
axes[1, 0].set_title(f'í¬ì•„ì†¡ë¶„í¬ Po({lam})')
axes[1, 0].grid(True)

# ê· ë“±ë¶„í¬
a, b = 0, 5
x_uniform = np.linspace(a-1, b+1, 1000)
axes[1, 1].plot(x_uniform, uniform.pdf(x_uniform, a, b-a), 'purple', linewidth=2)
axes[1, 1].set_title(f'ê· ë“±ë¶„í¬ U({a}, {b})')
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
```

### 4.4 ê°€ì„¤ê²€ì •

```python
from scipy.stats import ttest_1samp, ttest_ind, chi2_contingency

def perform_hypothesis_tests():
    """ë‹¤ì–‘í•œ ê°€ì„¤ê²€ì • ì˜ˆì œ"""
    
    # 1. ì¼í‘œë³¸ t-ê²€ì •
    sample = np.random.normal(100, 15, 50)
    t_stat, p_value = ttest_1samp(sample, 95)
    
    print("1. ì¼í‘œë³¸ t-ê²€ì • (H0: Î¼ = 95)")
    print(f"   t-í†µê³„ëŸ‰: {t_stat:.4f}")
    print(f"   p-ê°’: {p_value:.4f}")
    print(f"   ê²°ë¡ : {'H0 ê¸°ê°' if p_value < 0.05 else 'H0 ì±„íƒ'} (Î±=0.05)")
    
    # 2. ì´í‘œë³¸ t-ê²€ì •
    group1 = np.random.normal(100, 15, 50)
    group2 = np.random.normal(105, 15, 50)
    t_stat, p_value = ttest_ind(group1, group2)
    
    print("\n2. ì´í‘œë³¸ t-ê²€ì • (H0: Î¼1 = Î¼2)")
    print(f"   t-í†µê³„ëŸ‰: {t_stat:.4f}")
    print(f"   p-ê°’: {p_value:.4f}")
    print(f"   ê²°ë¡ : {'ë‘ ê·¸ë£¹ í‰ê· ì— ì°¨ì´ ìˆìŒ' if p_value < 0.05 else 'ë‘ ê·¸ë£¹ í‰ê· ì— ì°¨ì´ ì—†ìŒ'} (Î±=0.05)")
    
    # 3. ì¹´ì´ì œê³± ë…ë¦½ì„± ê²€ì •
    observed = np.array([[10, 10, 20], [20, 20, 40]])
    chi2_stat, p_value, dof, expected = chi2_contingency(observed)
    
    print("\n3. ì¹´ì´ì œê³± ë…ë¦½ì„± ê²€ì •")
    print(f"   ì¹´ì´ì œê³± í†µê³„ëŸ‰: {chi2_stat:.4f}")
    print(f"   p-ê°’: {p_value:.4f}")
    print(f"   ììœ ë„: {dof}")
    print(f"   ê²°ë¡ : {'ë³€ìˆ˜ë“¤ì´ ë…ë¦½ì ì´ì§€ ì•ŠìŒ' if p_value < 0.05 else 'ë³€ìˆ˜ë“¤ì´ ë…ë¦½ì ì„'} (Î±=0.05)")

perform_hypothesis_tests()
```

> **ì°¸ê³ **: ê°€ì„¤ê²€ì •ì—ì„œ p-ê°’ì€ ê·€ë¬´ê°€ì„¤ì´ ì°¸ì¼ ë•Œ ê´€ì°°ëœ ê²°ê³¼ ë˜ëŠ” ë” ê·¹ë‹¨ì ì¸ ê²°ê³¼ê°€ ë‚˜ì˜¬ í™•ë¥ ì…ë‹ˆë‹¤.

---

## 5. ìˆ˜í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©

### 5.1 NumPy ì‹¬í™”

NumPyëŠ” ë‹¤ì°¨ì› ë°°ì—´ê³¼ ìˆ˜í•™ ì—°ì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤.

#### ë°°ì—´ ìƒì„±ê³¼ ì¡°ì‘
```python
import numpy as np

# ë‹¤ì–‘í•œ ë°°ì—´ ìƒì„± ë°©ë²•
def create_arrays():
    """ë‹¤ì–‘í•œ NumPy ë°°ì—´ ìƒì„± ì˜ˆì œ"""
    
    # ê¸°ë³¸ ë°°ì—´
    arr1 = np.array([1, 2, 3, 4, 5])
    arr2 = np.array([[1, 2], [3, 4], [5, 6]])
    
    # íŠ¹ìˆ˜ ë°°ì—´
    zeros = np.zeros((3, 4), dtype=np.float32)
    ones = np.ones((2, 3), dtype=np.int32)
    identity = np.eye(4)  # ë‹¨ìœ„í–‰ë ¬
    diagonal = np.diag([1, 2, 3, 4])  # ëŒ€ê°í–‰ë ¬
    
    # ë²”ìœ„ ë°°ì—´
    arange = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
    linspace = np.linspace(0, 1, 11)  # 0ë¶€í„° 1ê¹Œì§€ 11ê°œ ê· ë“±ë¶„í• 
    logspace = np.logspace(0, 2, 5)  # 10^0ë¶€í„° 10^2ê¹Œì§€ 5ê°œ ë¡œê·¸ìŠ¤ì¼€ì¼
    
    # ëœë¤ ë°°ì—´
    np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
    random_uniform = np.random.uniform(0, 1, (3, 3))
    random_normal = np.random.normal(0, 1, (3, 3))
    random_int = np.random.randint(0, 10, (3, 3))
    
    return {
        'basic': (arr1, arr2),
        'special': (zeros, ones, identity, diagonal),
        'range': (arange, linspace, logspace),
        'random': (random_uniform, random_normal, random_int)
    }

arrays = create_arrays()
print("ë°°ì—´ ìƒì„± ì™„ë£Œ")
```

#### ë°°ì—´ ì—°ì‚°ê³¼ ë¸Œë¡œë“œìºìŠ¤íŒ…
```python
def array_operations():
    """ë°°ì—´ ì—°ì‚° ì˜ˆì œ"""
    
    # ê¸°ë³¸ ì—°ì‚°
    a = np.array([1, 2, 3, 4])
    b = np.array([5, 6, 7, 8])
    
    print("ê¸°ë³¸ ì—°ì‚°:")
    print(f"ë§ì…ˆ: {a + b}")
    print(f"ëº„ì…ˆ: {a - b}")
    print(f"ê³±ì…ˆ: {a * b}")  # ìš”ì†Œë³„ ê³±ì…ˆ
    print(f"ë‚˜ëˆ—ì…ˆ: {a / b}")
    print(f"ê±°ë“­ì œê³±: {a ** 2}")
    
    # í–‰ë ¬ ì—°ì‚°
    A = np.array([[1, 2], [3, 4]])
    B = np.array([[5, 6], [7, 8]])
    
    print("\ní–‰ë ¬ ì—°ì‚°:")
    print(f"í–‰ë ¬ ê³±ì…ˆ:\n{np.dot(A, B)}")
    print(f"ë˜ëŠ”:\n{A @ B}")
    print(f"ì „ì¹˜í–‰ë ¬:\n{A.T}")
    print(f"ì—­í–‰ë ¬:\n{np.linalg.inv(A)}")
    print(f"í–‰ë ¬ì‹: {np.linalg.det(A)}")
    
    # ë¸Œë¡œë“œìºìŠ¤íŒ…
    print("\në¸Œë¡œë“œìºìŠ¤íŒ…:")
    matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    vector = np.array([1, 0, 1])
    
    print(f"ì›ë³¸ í–‰ë ¬:\n{matrix}")
    print(f"ë²¡í„°: {vector}")
    print(f"ë¸Œë¡œë“œìºìŠ¤íŒ… ê²°ê³¼:\n{matrix + vector}")

array_operations()
```

#### ê³ ê¸‰ ì¸ë±ì‹±ê³¼ ìŠ¬ë¼ì´ì‹±
```python
def advanced_indexing():
    """ê³ ê¸‰ ì¸ë±ì‹± ì˜ˆì œ"""
    
    # 2D ë°°ì—´ ìƒì„±
    arr = np.arange(24).reshape(4, 6)
    print(f"ì›ë³¸ ë°°ì—´:\n{arr}")
    
    # ê¸°ë³¸ ìŠ¬ë¼ì´ì‹±
    print(f"\nì²« ë‘ í–‰: \n{arr[:2]}")
    print(f"ë§ˆì§€ë§‰ ë‘ ì—´: \n{arr[:, -2:]}")
    print(f"2í–‰ 3ì—´ë¶€í„° 4í–‰ 5ì—´ê¹Œì§€: \n{arr[1:3, 2:5]}")
    
    # ë¶ˆë¦° ì¸ë±ì‹±
    mask = arr > 10
    print(f"\n10ë³´ë‹¤ í° ìš”ì†Œë“¤: {arr[mask]}")
    
    # íŒ¬ì‹œ ì¸ë±ì‹±
    indices = [0, 2, 3]
    print(f"0, 2, 3ë²ˆ í–‰: \n{arr[indices]}")
    
    # ì¡°ê±´ë¶€ ì„ íƒ
    even_elements = arr[arr % 2 == 0]
    print(f"ì§ìˆ˜ ìš”ì†Œë“¤: {even_elements}")

advanced_indexing()
```

### 5.2 Pandas ì‹¬í™”

PandasëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ê³ ìˆ˜ì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

#### DataFrame ê³ ê¸‰ ì¡°ì‘
```python
import pandas as pd
import numpy as np

def create_sample_dataframe():
    """ìƒ˜í”Œ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
    np.random.seed(42)
    
    data = {
        'name': ['ê¹€ì² ìˆ˜', 'ì´ì˜í¬', 'ë°•ë¯¼ìˆ˜', 'ìµœì§€ì˜', 'ì •í˜¸ì¤€'] * 20,
        'age': np.random.randint(20, 60, 100),
        'city': np.random.choice(['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼'], 100),
        'salary': np.random.normal(50000, 15000, 100),
        'department': np.random.choice(['IT', 'ë§ˆì¼€íŒ…', 'ì˜ì—…', 'ì¸ì‚¬'], 100),
        'experience': np.random.randint(0, 20, 100),
        'performance': np.random.uniform(0.5, 1.0, 100)
    }
    
    df = pd.DataFrame(data)
    df['salary'] = df['salary'].round(0).astype(int)
    df['performance'] = df['performance'].round(3)
    
    return df

def dataframe_operations():
    """DataFrame ê³ ê¸‰ ì¡°ì‘ ì˜ˆì œ"""
    df = create_sample_dataframe()
    
    print("ë°ì´í„°í”„ë ˆì„ ê¸°ë³¸ ì •ë³´:")
    print(df.info())
    print(f"\nê¸°ìˆ í†µê³„:\n{df.describe()}")
    
    # ê·¸ë£¹í™”ì™€ ì§‘ê³„
    print("\në¶€ì„œë³„ í‰ê·  ì—°ë´‰:")
    dept_salary = df.groupby('department')['salary'].agg(['mean', 'std', 'count'])
    print(dept_salary)
    
    # ë‹¤ì¤‘ ê·¸ë£¹í™”
    print("\në¶€ì„œë³„, ë„ì‹œë³„ í‰ê·  ì„±ê³¼:")
    multi_group = df.groupby(['department', 'city'])['performance'].mean().unstack()
    print(multi_group)
    
    # í”¼ë²— í…Œì´ë¸”
    print("\ní”¼ë²— í…Œì´ë¸” (ë¶€ì„œ x ë„ì‹œë³„ í‰ê·  ì—°ë´‰):")
    pivot = pd.pivot_table(df, values='salary', index='department', 
                          columns='city', aggfunc='mean', fill_value=0)
    print(pivot)
    
    # ì¡°ê±´ë¶€ í•„í„°ë§
    high_performers = df[(df['performance'] > 0.8) & (df['experience'] > 5)]
    print(f"\nê³ ì„±ê³¼ì (ì„±ê³¼ > 0.8, ê²½ë ¥ > 5ë…„): {len(high_performers)}ëª…")
    
    return df

df = dataframe_operations()
```

#### ë°ì´í„° ì „ì²˜ë¦¬
```python
def data_preprocessing(df):
    """ë°ì´í„° ì „ì²˜ë¦¬ ì˜ˆì œ"""
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    df_copy = df.copy()
    
    # ì¸ìœ„ì ìœ¼ë¡œ ê²°ì¸¡ê°’ ìƒì„±
    np.random.seed(42)
    missing_indices = np.random.choice(df_copy.index, size=10, replace=False)
    df_copy.loc[missing_indices, 'salary'] = np.nan
    
    print("ê²°ì¸¡ê°’ í™•ì¸:")
    print(df_copy.isnull().sum())
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬ ë°©ë²•ë“¤
    df_copy['salary_filled_mean'] = df_copy['salary'].fillna(df_copy['salary'].mean())
    df_copy['salary_filled_median'] = df_copy['salary'].fillna(df_copy['salary'].median())
    df_copy['salary_filled_forward'] = df_copy['salary'].fillna(method='ffill')
    
    # ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)
    Q1 = df_copy['salary'].quantile(0.25)
    Q3 = df_copy['salary'].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df_copy[(df_copy['salary'] < lower_bound) | (df_copy['salary'] > upper_bound)]
    print(f"\nì´ìƒì¹˜ ê°œìˆ˜: {len(outliers)}")
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    df_encoded = pd.get_dummies(df_copy, columns=['city', 'department'], prefix=['city', 'dept'])
    
    print(f"\nì›ë³¸ ì»¬ëŸ¼ ìˆ˜: {len(df_copy.columns)}")
    print(f"ì¸ì½”ë”© í›„ ì»¬ëŸ¼ ìˆ˜: {len(df_encoded.columns)}")
    
    return df_encoded

df_processed = data_preprocessing(df)
```

### 5.3 Matplotlibê³¼ Seaborn ì‹œê°í™”

#### ê³ ê¸‰ ì‹œê°í™” ê¸°ë²•
```python
import matplotlib.pyplot as plt
import seaborn as sns

def advanced_visualization(df):
    """ê³ ê¸‰ ì‹œê°í™” ì˜ˆì œ"""
    
    # ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # ì„œë¸Œí”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ì§ì› ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ', fontsize=16, fontweight='bold')
    
    # 1. ì—°ë´‰ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    axes[0, 0].hist(df['salary'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].axvline(df['salary'].mean(), color='red', linestyle='--', 
                      label=f'í‰ê· : {df["salary"].mean():.0f}')
    axes[0, 0].set_title('ì—°ë´‰ ë¶„í¬')
    axes[0, 0].set_xlabel('ì—°ë´‰')
    axes[0, 0].set_ylabel('ë¹ˆë„')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ë¶€ì„œë³„ ì—°ë´‰ ë°•ìŠ¤í”Œë¡¯
    sns.boxplot(data=df, x='department', y='salary', ax=axes[0, 1])
    axes[0, 1].set_title('ë¶€ì„œë³„ ì—°ë´‰ ë¶„í¬')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. ê²½ë ¥ê³¼ ì—°ë´‰ì˜ ì‚°ì ë„
    scatter = axes[0, 2].scatter(df['experience'], df['salary'], 
                                c=df['performance'], cmap='viridis', alpha=0.6)
    axes[0, 2].set_title('ê²½ë ¥ vs ì—°ë´‰ (ì„±ê³¼ë³„ ìƒ‰ìƒ)')
    axes[0, 2].set_xlabel('ê²½ë ¥ (ë…„)')
    axes[0, 2].set_ylabel('ì—°ë´‰')
    plt.colorbar(scatter, ax=axes[0, 2], label='ì„±ê³¼')
    
    # 4. ë„ì‹œë³„ ì§ì› ìˆ˜
    city_counts = df['city'].value_counts()
    axes[1, 0].pie(city_counts.values, labels=city_counts.index, autopct='%1.1f%%')
    axes[1, 0].set_title('ë„ì‹œë³„ ì§ì› ë¶„í¬')
    
    # 5. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    numeric_cols = ['age', 'salary', 'experience', 'performance']
    corr_matrix = df[numeric_cols].corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                ax=axes[1, 1], square=True)
    axes[1, 1].set_title('ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„')
    
    # 6. ë‚˜ì´ëŒ€ë³„ ì„±ê³¼ ë¶„í¬
    df['age_group'] = pd.cut(df['age'], bins=[20, 30, 40, 50, 60], 
                            labels=['20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€'])
    sns.violinplot(data=df, x='age_group', y='performance', ax=axes[1, 2])
    axes[1, 2].set_title('ë‚˜ì´ëŒ€ë³„ ì„±ê³¼ ë¶„í¬')
    
    plt.tight_layout()
    plt.show()
    
    # ì¶”ê°€: ì¸í„°ë™í‹°ë¸Œ í”Œë¡¯ (plotly ì‚¬ìš©)
    try:
        import plotly.express as px
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        
        # 3D ì‚°ì ë„
        fig_3d = px.scatter_3d(df, x='age', y='experience', z='salary',
                              color='department', size='performance',
                              title='3D ì‚°ì ë„: ë‚˜ì´, ê²½ë ¥, ì—°ë´‰')
        fig_3d.show()
        
    except ImportError:
        print("Plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install plotlyë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

advanced_visualization(df)
```

> **ì°¸ê³ **: ì‹œê°í™”ëŠ” ë°ì´í„°ì˜ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ëŠ” ì¤‘ìš”í•œ ë„êµ¬ì…ë‹ˆë‹¤. ëª©ì ì— ë§ëŠ” ì ì ˆí•œ ì°¨íŠ¸ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”. 

---

## 6. ì´ë¯¸ì§€ ì²˜ë¦¬

### 6.1 ì´ë¯¸ì§€ ê¸°ì´ˆ ê°œë…

#### ë””ì§€í„¸ ì´ë¯¸ì§€ì˜ êµ¬ì¡°
- **í”½ì…€(Pixel)**: ì´ë¯¸ì§€ì˜ ìµœì†Œ ë‹¨ìœ„, Picture Elementì˜ ì¤„ì„ë§
- **í•´ìƒë„**: ì´ë¯¸ì§€ì˜ í¬ê¸° (ê°€ë¡œí”½ì…€ Ã— ì„¸ë¡œí”½ì…€)
- **ë¹„íŠ¸ ê¹Šì´**: í”½ì…€ë‹¹ ì €ì¥ë˜ëŠ” ë¹„íŠ¸ ìˆ˜ (8bit = 256 ìƒ‰ìƒ)
- **ì±„ë„**: ìƒ‰ìƒ ì •ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì°¨ì› ìˆ˜

#### ì£¼ìš” ìƒ‰ê³µê°„
1. **ê·¸ë ˆì´ìŠ¤ì¼€ì¼**: 0(ê²€ì •)~255(í°ìƒ‰)ì˜ ëª…ë„ê°’ í•˜ë‚˜
2. **RGB**: Red, Green, Blue ê°ê° 0~255 (3ì±„ë„)
3. **HSV**: Hue(ìƒ‰ìƒ), Saturation(ì±„ë„), Value(ëª…ë„)
4. **CMYK**: Cyan, Magenta, Yellow, Black (ì¸ì‡„ìš©)

### 6.2 ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### OpenCV ê¸°ì´ˆ
```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

def image_processing_basics():
    """OpenCV ê¸°ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬"""
    
    # ì´ë¯¸ì§€ ìƒì„± (ì˜ˆì œìš©)
    # ì‹¤ì œë¡œëŠ” cv2.imread('image_path.jpg')ë¡œ ì´ë¯¸ì§€ ë¡œë“œ
    height, width = 300, 400
    
    # ê·¸ë¼ë””ì–¸íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    img = np.zeros((height, width, 3), dtype=np.uint8)
    for i in range(height):
        img[i, :, 0] = int(255 * i / height)  # Red ì±„ë„
        img[i, :, 1] = int(255 * (1 - i / height))  # Green ì±„ë„
        img[i, :, 2] = 128  # Blue ì±„ë„ ê³ ì •
    
    # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    print(f"ì´ë¯¸ì§€ í¬ê¸°: {img.shape}")
    print(f"ë°ì´í„° íƒ€ì…: {img.dtype}")
    print(f"ìµœì†Ÿê°’: {img.min()}, ìµœëŒ“ê°’: {img.max()}")
    
    # ìƒ‰ê³µê°„ ë³€í™˜
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # ì›ë³¸ ì´ë¯¸ì§€ (BGR -> RGB ë³€í™˜)
    axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    axes[0, 0].set_title('ì›ë³¸ ì´ë¯¸ì§€')
    axes[0, 0].axis('off')
    
    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
    axes[0, 1].imshow(gray, cmap='gray')
    axes[0, 1].set_title('ê·¸ë ˆì´ìŠ¤ì¼€ì¼')
    axes[0, 1].axis('off')
    
    # HSV ìƒ‰ê³µê°„
    axes[1, 0].imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB))
    axes[1, 0].set_title('HSV ìƒ‰ê³µê°„')
    axes[1, 0].axis('off')
    
    # íˆìŠ¤í† ê·¸ë¨
    axes[1, 1].hist(gray.flatten(), bins=50, alpha=0.7)
    axes[1, 1].set_title('ê·¸ë ˆì´ìŠ¤ì¼€ì¼ íˆìŠ¤í† ê·¸ë¨')
    axes[1, 1].set_xlabel('í”½ì…€ ê°’')
    axes[1, 1].set_ylabel('ë¹ˆë„')
    
    plt.tight_layout()
    plt.show()
    
    return img, gray

# ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ë³¸ í•¨ìˆ˜ ì‹¤í–‰
original_img, gray_img = image_processing_basics()
```

#### Pillow (PIL) ì‚¬ìš©ë²•
```python
from PIL import Image, ImageFilter, ImageEnhance
import numpy as np

def pillow_image_processing():
    """Pillowë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ì²˜ë¦¬"""
    
    # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œë¡œëŠ” Image.open('path')ë¡œ ë¡œë“œ)
    # ê·¸ë¼ë””ì–¸íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    width, height = 400, 300
    img_array = np.zeros((height, width, 3), dtype=np.uint8)
    
    for i in range(height):
        for j in range(width):
            img_array[i, j] = [
                int(255 * j / width),      # Red
                int(255 * i / height),     # Green
                int(255 * (i + j) / (height + width))  # Blue
            ]
    
    img = Image.fromarray(img_array)
    
    # ì´ë¯¸ì§€ ì •ë³´
    print(f"ì´ë¯¸ì§€ í¬ê¸°: {img.size}")
    print(f"ì´ë¯¸ì§€ ëª¨ë“œ: {img.mode}")
    
    # ê¸°ë³¸ ë³€í™˜
    resized = img.resize((200, 150))
    rotated = img.rotate(45, expand=True)
    cropped = img.crop((50, 50, 250, 200))
    
    # í•„í„° ì ìš©
    blurred = img.filter(ImageFilter.BLUR)
    sharpened = img.filter(ImageFilter.SHARPEN)
    edge_enhanced = img.filter(ImageFilter.EDGE_ENHANCE)
    
    # ìƒ‰ìƒ ì¡°ì •
    enhancer = ImageEnhance.Brightness(img)
    brighter = enhancer.enhance(1.5)  # 1.5ë°° ë°ê²Œ
    
    enhancer = ImageEnhance.Contrast(img)
    high_contrast = enhancer.enhance(2.0)  # 2ë°° ëŒ€ë¹„
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    
    images = [
        (img, 'ì›ë³¸'),
        (resized, 'í¬ê¸° ë³€ê²½'),
        (rotated, 'íšŒì „'),
        (cropped, 'ìë¥´ê¸°'),
        (blurred, 'ë¸”ëŸ¬'),
        (sharpened, 'ìƒ¤í”„ë‹'),
        (edge_enhanced, 'ì—£ì§€ ê°•í™”'),
        (brighter, 'ë°ê¸° ì¦ê°€'),
        (high_contrast, 'ëŒ€ë¹„ ì¦ê°€')
    ]
    
    for i, (image, title) in enumerate(images):
        row, col = i // 3, i % 3
        axes[row, col].imshow(image)
        axes[row, col].set_title(title)
        axes[row, col].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return img

processed_img = pillow_image_processing()
```

### 6.3 ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê¸°ë²•

```python
def advanced_image_preprocessing():
    """ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê¸°ë²•"""
    
    # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± (ë…¸ì´ì¦ˆê°€ í¬í•¨ëœ ì´ë¯¸ì§€)
    np.random.seed(42)
    clean_img = np.zeros((200, 200), dtype=np.float32)
    
    # ê¸°í•˜í•™ì  íŒ¨í„´ ìƒì„±
    center_x, center_y = 100, 100
    for i in range(200):
        for j in range(200):
            distance = np.sqrt((i - center_x)**2 + (j - center_y)**2)
            clean_img[i, j] = 255 * np.exp(-distance / 50)
    
    # ë…¸ì´ì¦ˆ ì¶”ê°€
    noise = np.random.normal(0, 20, clean_img.shape)
    noisy_img = np.clip(clean_img + noise, 0, 255).astype(np.uint8)
    
    # 1. ë…¸ì´ì¦ˆ ì œê±° í•„í„°
    # ê°€ìš°ì‹œì•ˆ í•„í„°
    gaussian_filtered = cv2.GaussianBlur(noisy_img, (5, 5), 0)
    
    # ë¯¸ë””ì•ˆ í•„í„° (ì  ë…¸ì´ì¦ˆ ì œê±°ì— íš¨ê³¼ì )
    median_filtered = cv2.medianBlur(noisy_img, 5)
    
    # ì–‘ë°©í–¥ í•„í„° (ì—£ì§€ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆ ì œê±°)
    bilateral_filtered = cv2.bilateralFilter(noisy_img, 9, 75, 75)
    
    # 2. ì—£ì§€ ê²€ì¶œ
    # Canny ì—£ì§€ ê²€ì¶œ
    edges_canny = cv2.Canny(noisy_img, 50, 150)
    
    # Sobel ì—£ì§€ ê²€ì¶œ
    sobel_x = cv2.Sobel(noisy_img, cv2.CV_64F, 1, 0, ksize=3)
    sobel_y = cv2.Sobel(noisy_img, cv2.CV_64F, 0, 1, ksize=3)
    sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)
    
    # 3. ëª¨í´ë¡œì§€ ì—°ì‚°
    kernel = np.ones((3, 3), np.uint8)
    
    # ì¹¨ì‹ê³¼ íŒ½ì°½
    erosion = cv2.erode(noisy_img, kernel, iterations=1)
    dilation = cv2.dilate(noisy_img, kernel, iterations=1)
    
    # ì—´ë¦¼ê³¼ ë‹«í˜
    opening = cv2.morphologyEx(noisy_img, cv2.MORPH_OPEN, kernel)
    closing = cv2.morphologyEx(noisy_img, cv2.MORPH_CLOSE, kernel)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    
    images = [
        (clean_img.astype(np.uint8), 'ì›ë³¸ (ê¹¨ë—í•œ ì´ë¯¸ì§€)'),
        (noisy_img, 'ë…¸ì´ì¦ˆ ì¶”ê°€'),
        (gaussian_filtered, 'ê°€ìš°ì‹œì•ˆ í•„í„°'),
        (median_filtered, 'ë¯¸ë””ì•ˆ í•„í„°'),
        (bilateral_filtered, 'ì–‘ë°©í–¥ í•„í„°'),
        (edges_canny, 'Canny ì—£ì§€'),
        (sobel_combined.astype(np.uint8), 'Sobel ì—£ì§€'),
        (erosion, 'ì¹¨ì‹'),
        (dilation, 'íŒ½ì°½'),
        (opening, 'ì—´ë¦¼'),
        (closing, 'ë‹«í˜'),
        (np.zeros_like(noisy_img), 'ì—¬ìœ  ê³µê°„')
    ]
    
    for i, (image, title) in enumerate(images):
        row, col = i // 4, i % 4
        if i < 11:  # ë§ˆì§€ë§‰ ì—¬ìœ  ê³µê°„ ì œì™¸
            axes[row, col].imshow(image, cmap='gray')
        axes[row, col].set_title(title)
        axes[row, col].axis('off')
    
    plt.tight_layout()
    plt.show()

advanced_image_preprocessing()
```

> **ì°¸ê³ **: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë°ì´í„°ì˜ íŠ¹ì„±ì— ë§ëŠ” ì ì ˆí•œ ì „ì²˜ë¦¬ ê¸°ë²•ì„ ì„ íƒí•˜ì„¸ìš”.

---

## 7. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ

### 7.1 ë¨¸ì‹ ëŸ¬ë‹ ê°œë…ê³¼ ë¶„ë¥˜

**ë¨¸ì‹ ëŸ¬ë‹**ì€ ëª…ì‹œì ì¸ í”„ë¡œê·¸ë˜ë° ì—†ì´ ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

#### AI, ML, DLì˜ ê´€ê³„
```
ì¸ê³µì§€ëŠ¥ (Artificial Intelligence)
â”œâ”€â”€ ë¨¸ì‹ ëŸ¬ë‹ (Machine Learning)
â”‚   â”œâ”€â”€ ë”¥ëŸ¬ë‹ (Deep Learning)
â”‚   â”‚   â”œâ”€â”€ CNN (Convolutional Neural Networks)
â”‚   â”‚   â”œâ”€â”€ RNN (Recurrent Neural Networks)
â”‚   â”‚   â”œâ”€â”€ Transformer
â”‚   â”‚   â””â”€â”€ GAN (Generative Adversarial Networks)
â”‚   â”œâ”€â”€ ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ (Decision Tree)
â”‚   â”œâ”€â”€ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  (SVM)
â”‚   â”œâ”€â”€ ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)
â”‚   â””â”€â”€ k-ìµœê·¼ì ‘ ì´ì›ƒ (k-NN)
â”œâ”€â”€ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ (Expert Systems)
â””â”€â”€ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œ (Rule-based Systems)
```

### 7.2 ë¨¸ì‹ ëŸ¬ë‹ ìœ í˜•

#### ì§€ë„í•™ìŠµ (Supervised Learning)
ì…ë ¥(X)ê³¼ ì •ë‹µ(y)ì´ ëª¨ë‘ ì£¼ì–´ì§„ ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•

**ë¶„ë¥˜ (Classification)**
```python
from sklearn.datasets import make_classification, load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

def classification_example():
    """ë¶„ë¥˜ ë¬¸ì œ ì˜ˆì œ"""
    
    # ë°ì´í„° ë¡œë“œ
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # ì—¬ëŸ¬ ë¶„ë¥˜ ëª¨ë¸ ë¹„êµ
    models = {
        'Logistic Regression': LogisticRegression(random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'SVM': SVC(kernel='rbf', random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        # ëª¨ë¸ í›ˆë ¨
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)
        
        # ì„±ëŠ¥ í‰ê°€
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        results[name] = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, average='weighted'),
            'recall': recall_score(y_test, y_pred, average='weighted'),
            'f1': f1_score(y_test, y_pred, average='weighted')
        }
        
        print(f"\n{name} ì„±ëŠ¥:")
        print(classification_report(y_test, y_pred, target_names=iris.target_names))
    
    # ê²°ê³¼ ë¹„êµ
    import pandas as pd
    results_df = pd.DataFrame(results).T
    print("\nëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
    print(results_df.round(4))
    
    # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    for i, (name, model) in enumerate(models.items()):
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        
        sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], 
                   xticklabels=iris.target_names, 
                   yticklabels=iris.target_names)
        axes[i].set_title(f'{name} í˜¼ë™í–‰ë ¬')
        axes[i].set_xlabel('ì˜ˆì¸¡ê°’')
        axes[i].set_ylabel('ì‹¤ì œê°’')
    
    plt.tight_layout()
    plt.show()
    
    return models, results_df

models, performance = classification_example()
```

**íšŒê·€ (Regression)**
```python
from sklearn.datasets import make_regression, load_boston
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

def regression_example():
    """íšŒê·€ ë¬¸ì œ ì˜ˆì œ"""
    
    # í•©ì„± ë°ì´í„° ìƒì„±
    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # ì—¬ëŸ¬ íšŒê·€ ëª¨ë¸ ë¹„êµ
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(alpha=1.0),
        'Lasso Regression': Lasso(alpha=1.0),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
    }
    
    results = {}
    predictions = {}
    
    for name, model in models.items():
        # ëª¨ë¸ í›ˆë ¨
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)
        predictions[name] = y_pred
        
        # ì„±ëŠ¥ í‰ê°€
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results[name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2
        }
        
        print(f"\n{name} ì„±ëŠ¥:")
        print(f"  MSE: {mse:.4f}")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  MAE: {mae:.4f}")
        print(f"  RÂ²: {r2:.4f}")
    
    # ê²°ê³¼ ë¹„êµ
    results_df = pd.DataFrame(results).T
    print("\nëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
    print(results_df.round(4))
    
    # ì˜ˆì¸¡ vs ì‹¤ì œê°’ ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()
    
    for i, (name, y_pred) in enumerate(predictions.items()):
        axes[i].scatter(y_test, y_pred, alpha=0.6)
        axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        axes[i].set_xlabel('ì‹¤ì œê°’')
        axes[i].set_ylabel('ì˜ˆì¸¡ê°’')
        axes[i].set_title(f'{name}\nRÂ² = {results[name]["RÂ²"]:.4f}')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return models, results_df

regression_models, regression_performance = regression_example()
```

#### ë¹„ì§€ë„í•™ìŠµ (Unsupervised Learning)
```python
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

def unsupervised_learning_example():
    """ë¹„ì§€ë„í•™ìŠµ ì˜ˆì œ"""
    
    # í´ëŸ¬ìŠ¤í„°ë§ìš© ë°ì´í„° ìƒì„±
    from sklearn.datasets import make_blobs
    X_cluster, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, 
                             random_state=42, n_features=2)
    
    # 1. í´ëŸ¬ìŠ¤í„°ë§
    clustering_models = {
        'K-Means': KMeans(n_clusters=4, random_state=42),
        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
        'Agglomerative': AgglomerativeClustering(n_clusters=4)
    }
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()
    
    # ì›ë³¸ ë°ì´í„°
    axes[0].scatter(X_cluster[:, 0], X_cluster[:, 1], alpha=0.7)
    axes[0].set_title('ì›ë³¸ ë°ì´í„°')
    axes[0].grid(True, alpha=0.3)
    
    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
    for i, (name, model) in enumerate(clustering_models.items()):
        labels = model.fit_predict(X_cluster)
        
        scatter = axes[i+1].scatter(X_cluster[:, 0], X_cluster[:, 1], 
                                   c=labels, cmap='viridis', alpha=0.7)
        axes[i+1].set_title(f'{name} í´ëŸ¬ìŠ¤í„°ë§')
        axes[i+1].grid(True, alpha=0.3)
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  í‘œì‹œ (K-Meansì˜ ê²½ìš°)
        if hasattr(model, 'cluster_centers_'):
            centers = model.cluster_centers_
            axes[i+1].scatter(centers[:, 0], centers[:, 1], 
                            c='red', marker='x', s=200, linewidths=3)
    
    plt.tight_layout()
    plt.show()
    
    # 2. ì°¨ì› ì¶•ì†Œ
    # ê³ ì°¨ì› ë°ì´í„° ìƒì„±
    X_high_dim, y_high_dim = make_classification(n_samples=1000, n_features=20, 
                                                n_informative=10, n_redundant=10, 
                                                n_clusters_per_class=1, random_state=42)
    
    # ë°ì´í„° ì •ê·œí™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_high_dim)
    
    # PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    # t-SNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    X_tsne = tsne.fit_transform(X_scaled)
    
    # ì°¨ì› ì¶•ì†Œ ê²°ê³¼ ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # PCA ê²°ê³¼
    scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_high_dim, cmap='viridis', alpha=0.7)
    axes[0].set_title(f'PCA ì°¨ì›ì¶•ì†Œ\nì„¤ëª…ëœ ë¶„ì‚°: {pca.explained_variance_ratio_.sum():.3f}')
    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')
    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')
    axes[0].grid(True, alpha=0.3)
    
    # t-SNE ê²°ê³¼
    scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_high_dim, cmap='viridis', alpha=0.7)
    axes[1].set_title('t-SNE ì°¨ì›ì¶•ì†Œ')
    axes[1].set_xlabel('t-SNE 1')
    axes[1].set_ylabel('t-SNE 2')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"ì›ë³¸ ë°ì´í„° ì°¨ì›: {X_high_dim.shape}")
    print(f"PCA í›„ ì°¨ì›: {X_pca.shape}")
    print(f"ì£¼ì„±ë¶„ë³„ ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨: {pca.explained_variance_ratio_}")
    print(f"ì´ ì„¤ëª…ëœ ë¶„ì‚°: {pca.explained_variance_ratio_.sum():.4f}")

unsupervised_learning_example()
```

### 7.3 ëª¨ë¸ í‰ê°€ì™€ ê²€ì¦

```python
from sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve
from sklearn.metrics import roc_curve, auc, precision_recall_curve

def model_evaluation_techniques():
    """ëª¨ë¸ í‰ê°€ ê¸°ë²•ë“¤"""
    
    # ë°ì´í„° ì¤€ë¹„
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•´ í´ë˜ìŠ¤ 2ê°œë§Œ ì‚¬ìš©
    binary_mask = y != 2
    X_binary = X[binary_mask]
    y_binary = y[binary_mask]
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_binary, y_binary, test_size=0.3, random_state=42, stratify=y_binary
    )
    
    # 1. êµì°¨ ê²€ì¦
    model = RandomForestClassifier(random_state=42)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    
    print("êµì°¨ ê²€ì¦ ê²°ê³¼:")
    print(f"  í‰ê·  ì •í™•ë„: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    print(f"  ê° í´ë“œ ì ìˆ˜: {cv_scores}")
    
    # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10]
    }
    
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
    print(f"ìµœì  êµì°¨ ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.4f}")
    
    # 3. í•™ìŠµ ê³¡ì„ 
    train_sizes, train_scores, val_scores = learning_curve(
        grid_search.best_estimator_, X_train, y_train,
        train_sizes=np.linspace(0.1, 1.0, 10),
        cv=5, scoring='accuracy'
    )
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='í›ˆë ¨ ì ìˆ˜')
    plt.plot(train_sizes, val_scores.mean(axis=1), 'o-', label='ê²€ì¦ ì ìˆ˜')
    plt.xlabel('í›ˆë ¨ ë°ì´í„° í¬ê¸°')
    plt.ylabel('ì •í™•ë„')
    plt.title('í•™ìŠµ ê³¡ì„ ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. ROC ê³¡ì„ 
    best_model = grid_search.best_estimator_
    y_prob = best_model.predict_proba(X_test)[:, 1]
    
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    
    plt.subplot(2, 2, 2)
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC ê³¡ì„  (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('ê±°ì§“ ì–‘ì„± ë¹„ìœ¨')
    plt.ylabel('ì°¸ ì–‘ì„± ë¹„ìœ¨')
    plt.title('ROC ê³¡ì„ ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 5. Precision-Recall ê³¡ì„ 
    precision, recall, _ = precision_recall_curve(y_test, y_prob)
    
    plt.subplot(2, 2, 3)
    plt.plot(recall, precision, color='blue', lw=2)
    plt.xlabel('ì¬í˜„ìœ¨ (Recall)')
    plt.ylabel('ì •ë°€ë„ (Precision)')
    plt.title('Precision-Recall ê³¡ì„ ')
    plt.grid(True, alpha=0.3)
    
    # 6. íŠ¹ì„± ì¤‘ìš”ë„
    feature_importance = best_model.feature_importances_
    feature_names = iris.feature_names
    
    plt.subplot(2, 2, 4)
    indices = np.argsort(feature_importance)[::-1]
    plt.bar(range(len(feature_importance)), feature_importance[indices])
    plt.xticks(range(len(feature_importance)), 
               [feature_names[i] for i in indices], rotation=45)
    plt.title('íŠ¹ì„± ì¤‘ìš”ë„')
    plt.ylabel('ì¤‘ìš”ë„')
    
    plt.tight_layout()
    plt.show()
    
    return grid_search.best_estimator_

best_model = model_evaluation_techniques()
```

> **ì°¸ê³ **: ëª¨ë¸ í‰ê°€ëŠ” ë‹¨ìˆœíˆ ì •í™•ë„ë§Œìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ê³ , ë¬¸ì œì˜ íŠ¹ì„±ì— ë§ëŠ” ë‹¤ì–‘í•œ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

---

## 8. ë”¥ëŸ¬ë‹ í•µì‹¬ ê°œë…

### 8.1 ë”¥ëŸ¬ë‹ ì†Œê°œ

**ë”¥ëŸ¬ë‹**ì€ ì¸ê°„ì˜ ë‡Œ êµ¬ì¡°ë¥¼ ëª¨ë°©í•œ ì¸ê³µì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ê¹Šê²Œ ìŒ“ì•„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.

#### ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•
- **ìë™ íŠ¹ì„± ì¶”ì¶œ**: ìˆ˜ë™ìœ¼ë¡œ íŠ¹ì„±ì„ ì„¤ê³„í•  í•„ìš” ì—†ìŒ
- **ê³„ì¸µì  í•™ìŠµ**: ì €ìˆ˜ì¤€ì—ì„œ ê³ ìˆ˜ì¤€ íŠ¹ì„±ê¹Œì§€ ë‹¨ê³„ì  í•™ìŠµ
- **ëŒ€ìš©ëŸ‰ ë°ì´í„°**: ë§ì€ ë°ì´í„°ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ ë°œíœ˜
- **GPU í™œìš©**: ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥
- **í‘œí˜„ í•™ìŠµ**: ë°ì´í„°ì˜ ë‚´ì¬ëœ êµ¬ì¡°ë¥¼ ìë™ìœ¼ë¡œ ë°œê²¬

#### ë”¥ëŸ¬ë‹ vs ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹
```python
import matplotlib.pyplot as plt
import numpy as np

def compare_ml_vs_dl():
    """ë¨¸ì‹ ëŸ¬ë‹ vs ë”¥ëŸ¬ë‹ ë¹„êµ ì‹œê°í™”"""
    
    # ë°ì´í„° ì–‘ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ (ê°€ìƒì˜ ë°ì´í„°)
    data_sizes = np.logspace(2, 6, 50)  # 100 ~ 1,000,000
    
    # ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ì„±ëŠ¥ (í¬í™” ê³¡ì„ )
    ml_performance = 85 * (1 - np.exp(-data_sizes / 50000))
    
    # ë”¥ëŸ¬ë‹ ì„±ëŠ¥ (ì§€ì†ì  ì¦ê°€)
    dl_performance = 95 * (1 - np.exp(-data_sizes / 200000)) + np.log(data_sizes) * 2
    dl_performance = np.clip(dl_performance, 0, 95)
    
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.semilogx(data_sizes, ml_performance, 'b-', linewidth=2, label='ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹')
    plt.semilogx(data_sizes, dl_performance, 'r-', linewidth=2, label='ë”¥ëŸ¬ë‹')
    plt.xlabel('ë°ì´í„° í¬ê¸°')
    plt.ylabel('ì„±ëŠ¥ (%)')
    plt.title('ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # íŠ¹ì„± ì¶”ì¶œ ê³¼ì • ë¹„êµ
    plt.subplot(2, 2, 2)
    
    # ì „í†µì  ML íŒŒì´í”„ë¼ì¸
    stages_ml = ['ì›ì‹œ ë°ì´í„°', 'íŠ¹ì„± ì¶”ì¶œ\n(ìˆ˜ë™)', 'ë¨¸ì‹ ëŸ¬ë‹\nì•Œê³ ë¦¬ì¦˜', 'ê²°ê³¼']
    y_ml = [0.7, 0.7, 0.7, 0.7]
    
    # ë”¥ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸
    stages_dl = ['ì›ì‹œ ë°ì´í„°', 'íŠ¹ì„± ì¶”ì¶œ\n(ìë™)', 'ë”¥ëŸ¬ë‹\nëª¨ë¸', 'ê²°ê³¼']
    y_dl = [0.3, 0.3, 0.3, 0.3]
    
    for i in range(len(stages_ml)-1):
        plt.arrow(i, y_ml[i], 0.8, 0, head_width=0.05, head_length=0.1, fc='blue', ec='blue')
        plt.arrow(i, y_dl[i], 0.8, 0, head_width=0.05, head_length=0.1, fc='red', ec='red')
    
    for i, stage in enumerate(stages_ml):
        plt.text(i, y_ml[i]+0.1, stage, ha='center', va='bottom', fontsize=9, color='blue')
        plt.text(i, y_dl[i]-0.1, stages_dl[i], ha='center', va='top', fontsize=9, color='red')
    
    plt.xlim(-0.5, 3.5)
    plt.ylim(0, 1)
    plt.title('ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¹„êµ')
    plt.axis('off')
    
    # ë³µì¡ë„ì™€ ì„±ëŠ¥
    plt.subplot(2, 2, 3)
    complexity = np.linspace(1, 10, 100)
    ml_complexity = 70 + 10 * np.log(complexity) - 0.5 * complexity
    dl_complexity = 50 + 8 * complexity - 0.3 * complexity**2
    
    plt.plot(complexity, ml_complexity, 'b-', linewidth=2, label='ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹')
    plt.plot(complexity, dl_complexity, 'r-', linewidth=2, label='ë”¥ëŸ¬ë‹')
    plt.xlabel('ë¬¸ì œ ë³µì¡ë„')
    plt.ylabel('ì„±ëŠ¥')
    plt.title('ë¬¸ì œ ë³µì¡ë„ì— ë”°ë¥¸ ì„±ëŠ¥')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # í•™ìŠµ ì‹œê°„ ë¹„êµ
    plt.subplot(2, 2, 4)
    model_sizes = ['ì‘ì€ ëª¨ë¸', 'ì¤‘ê°„ ëª¨ë¸', 'í° ëª¨ë¸', 'ë§¤ìš° í° ëª¨ë¸']
    ml_time = [1, 5, 20, 50]
    dl_time = [10, 100, 1000, 10000]
    
    x = np.arange(len(model_sizes))
    width = 0.35
    
    plt.bar(x - width/2, ml_time, width, label='ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹', alpha=0.8)
    plt.bar(x + width/2, dl_time, width, label='ë”¥ëŸ¬ë‹', alpha=0.8)
    
    plt.xlabel('ëª¨ë¸ í¬ê¸°')
    plt.ylabel('í•™ìŠµ ì‹œê°„ (ìƒëŒ€ì )')
    plt.title('ëª¨ë¸ í¬ê¸°ë³„ í•™ìŠµ ì‹œê°„')
    plt.yscale('log')
    plt.xticks(x, model_sizes, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

compare_ml_vs_dl()
```

### 8.2 ì‹ ê²½ë§ì˜ ê¸°ë³¸ êµ¬ì¡°

#### í¼ì…‰íŠ¸ë¡  (Perceptron)
```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    """ë‹¨ìˆœ í¼ì…‰íŠ¸ë¡  êµ¬í˜„"""
    
    def __init__(self, learning_rate=0.1, n_iterations=100):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.errors = []
    
    def fit(self, X, y):
        """í¼ì…‰íŠ¸ë¡  í•™ìŠµ"""
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        n_features = X.shape[1]
        self.weights = np.random.normal(0, 0.1, n_features)
        self.bias = 0
        
        # í•™ìŠµ ê³¼ì •
        for i in range(self.n_iterations):
            errors = 0
            for xi, yi in zip(X, y):
                # ì˜ˆì¸¡
                prediction = self.predict_single(xi)
                
                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                if prediction != yi:
                    self.weights += self.learning_rate * yi * xi
                    self.bias += self.learning_rate * yi
                    errors += 1
            
            self.errors.append(errors)
            
            # ìˆ˜ë ´ í™•ì¸
            if errors == 0:
                print(f"ìˆ˜ë ´ ì™„ë£Œ: {i+1}ë²ˆì§¸ ë°˜ë³µ")
                break
    
    def predict_single(self, x):
        """ë‹¨ì¼ ìƒ˜í”Œ ì˜ˆì¸¡"""
        return 1 if np.dot(x, self.weights) + self.bias > 0 else 0
    
    def predict(self, X):
        """ë‹¤ì¤‘ ìƒ˜í”Œ ì˜ˆì¸¡"""
        return np.array([self.predict_single(x) for x in X])

def perceptron_example():
    """í¼ì…‰íŠ¸ë¡  ì˜ˆì œ"""
    
    # AND ê²Œì´íŠ¸ ë°ì´í„°
    X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_and = np.array([0, 0, 0, 1])
    
    # OR ê²Œì´íŠ¸ ë°ì´í„°
    X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_or = np.array([0, 1, 1, 1])
    
    # XOR ê²Œì´íŠ¸ ë°ì´í„° (ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥)
    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_xor = np.array([0, 1, 1, 0])
    
    datasets = [
        (X_and, y_and, 'AND ê²Œì´íŠ¸'),
        (X_or, y_or, 'OR ê²Œì´íŠ¸'),
        (X_xor, y_xor, 'XOR ê²Œì´íŠ¸')
    ]
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    for i, (X, y, title) in enumerate(datasets):
        # í¼ì…‰íŠ¸ë¡  í•™ìŠµ
        perceptron = Perceptron(learning_rate=0.1, n_iterations=100)
        perceptron.fit(X, y)
        
        # ê²°ê³¼ ì‹œê°í™”
        ax1 = axes[0, i]
        ax2 = axes[1, i]
        
        # ë°ì´í„° í¬ì¸íŠ¸ ì‹œê°í™”
        colors = ['red' if label == 0 else 'blue' for label in y]
        ax1.scatter(X[:, 0], X[:, 1], c=colors, s=100, alpha=0.8)
        
        # ê²°ì • ê²½ê³„ ê·¸ë¦¬ê¸° (XORì€ ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥)
        if title != 'XOR ê²Œì´íŠ¸':
            if perceptron.weights[1] != 0:
                x_line = np.linspace(-0.5, 1.5, 100)
                y_line = -(perceptron.weights[0] * x_line + perceptron.bias) / perceptron.weights[1]
                ax1.plot(x_line, y_line, 'g--', linewidth=2, label='ê²°ì • ê²½ê³„')
        
        ax1.set_xlim(-0.5, 1.5)
        ax1.set_ylim(-0.5, 1.5)
        ax1.set_xlabel('ì…ë ¥ 1')
        ax1.set_ylabel('ì…ë ¥ 2')
        ax1.set_title(f'{title} - ê²°ì • ê²½ê³„')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # í•™ìŠµ ê³¡ì„ 
        ax2.plot(range(1, len(perceptron.errors) + 1), perceptron.errors, 'b-o')
        ax2.set_xlabel('ë°˜ë³µ íšŸìˆ˜')
        ax2.set_ylabel('ì˜¤ë¥˜ ê°œìˆ˜')
        ax2.set_title(f'{title} - í•™ìŠµ ê³¡ì„ ')
        ax2.grid(True, alpha=0.3)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        predictions = perceptron.predict(X)
        accuracy = np.mean(predictions == y)
        print(f"\n{title} ê²°ê³¼:")
        print(f"  ì •í™•ë„: {accuracy:.2f}")
        print(f"  ì˜ˆì¸¡ê°’: {predictions}")
        print(f"  ì‹¤ì œê°’: {y}")
    
    plt.tight_layout()
    plt.show()

perceptron_example()
```

#### ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (Multi-Layer Perceptron)
```python
class MLPFromScratch:
    """ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ì§ì ‘ êµ¬í˜„"""
    
    def __init__(self, layers, learning_rate=0.01, epochs=1000):
        self.layers = layers  # [ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ1, ì€ë‹‰ì¸µ2, ..., ì¶œë ¥ì¸µ]
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = []
        self.biases = []
        self.losses = []
        
        # ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì´ˆê¸°í™”
        for i in range(len(layers) - 1):
            w = np.random.randn(layers[i], layers[i+1]) * 0.1
            b = np.zeros((1, layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def sigmoid(self, x):
        """ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜"""
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def sigmoid_derivative(self, x):
        """ì‹œê·¸ëª¨ì´ë“œ ë„í•¨ìˆ˜"""
        return x * (1 - x)
    
    def forward(self, X):
        """ìˆœì „íŒŒ"""
        self.activations = [X]
        
        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            a = self.sigmoid(z)
            self.activations.append(a)
        
        return self.activations[-1]
    
    def backward(self, X, y, output):
        """ì—­ì „íŒŒ"""
        m = X.shape[0]
        
        # ì¶œë ¥ì¸µ ì˜¤ì°¨
        dA = output - y
        
        # ì—­ì „íŒŒ ìˆ˜í–‰
        for i in reversed(range(len(self.weights))):
            dZ = dA * self.sigmoid_derivative(self.activations[i+1])
            dW = np.dot(self.activations[i].T, dZ) / m
            db = np.sum(dZ, axis=0, keepdims=True) / m
            
            if i > 0:
                dA = np.dot(dZ, self.weights[i].T)
            
            # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            self.weights[i] -= self.learning_rate * dW
            self.biases[i] -= self.learning_rate * db
    
    def fit(self, X, y):
        """ëª¨ë¸ í•™ìŠµ"""
        for epoch in range(self.epochs):
            # ìˆœì „íŒŒ
            output = self.forward(X)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = np.mean((output - y) ** 2)
            self.losses.append(loss)
            
            # ì—­ì „íŒŒ
            self.backward(X, y, output)
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.6f}")
    
    def predict(self, X):
        """ì˜ˆì¸¡"""
        return self.forward(X)

def mlp_xor_example():
    """MLPë¡œ XOR ë¬¸ì œ í•´ê²°"""
    
    # XOR ë°ì´í„°
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])
    
    # MLP ëª¨ë¸ ìƒì„± (2-4-1 êµ¬ì¡°)
    mlp = MLPFromScratch([2, 4, 1], learning_rate=1.0, epochs=1000)
    
    print("XOR ë¬¸ì œë¥¼ MLPë¡œ í•´ê²°:")
    mlp.fit(X, y)
    
    # ì˜ˆì¸¡
    predictions = mlp.predict(X)
    
    print("\nê²°ê³¼:")
    for i in range(len(X)):
        print(f"ì…ë ¥: {X[i]}, ì˜ˆì¸¡: {predictions[i][0]:.4f}, ì‹¤ì œ: {y[i][0]}")
    
    # ê²°ê³¼ ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # í•™ìŠµ ê³¡ì„ 
    axes[0].plot(mlp.losses)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('í•™ìŠµ ê³¡ì„ ')
    axes[0].grid(True, alpha=0.3)
    
    # ê²°ì • ê²½ê³„ ì‹œê°í™”
    h = 0.01
    x_min, x_max = -0.5, 1.5
    y_min, y_max = -0.5, 1.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    Z = mlp.predict(grid_points)
    Z = Z.reshape(xx.shape)
    
    axes[1].contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
    colors = ['red' if label[0] == 0 else 'blue' for label in y]
    axes[1].scatter(X[:, 0], X[:, 1], c=colors, s=100, edgecolors='black')
    axes[1].set_xlabel('ì…ë ¥ 1')
    axes[1].set_ylabel('ì…ë ¥ 2')
    axes[1].set_title('MLP ê²°ì • ê²½ê³„ (XOR)')
    
    plt.tight_layout()
    plt.show()

mlp_xor_example()
```

### 8.3 í™œì„±í™” í•¨ìˆ˜

```python
def activation_functions_comparison():
    """ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜ ë¹„êµ"""
    
    x = np.linspace(-5, 5, 1000)
    
    # í™œì„±í™” í•¨ìˆ˜ë“¤ ì •ì˜
    def sigmoid(x):
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def tanh(x):
        return np.tanh(x)
    
    def relu(x):
        return np.maximum(0, x)
    
    def leaky_relu(x, alpha=0.01):
        return np.where(x > 0, x, alpha * x)
    
    def elu(x, alpha=1.0):
        return np.where(x > 0, x, alpha * (np.exp(x) - 1))
    
    def swish(x):
        return x * sigmoid(x)
    
    # ë„í•¨ìˆ˜ë“¤
    def sigmoid_derivative(x):
        s = sigmoid(x)
        return s * (1 - s)
    
    def tanh_derivative(x):
        return 1 - np.tanh(x)**2
    
    def relu_derivative(x):
        return np.where(x > 0, 1, 0)
    
    def leaky_relu_derivative(x, alpha=0.01):
        return np.where(x > 0, 1, alpha)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    functions = [
        (sigmoid, sigmoid_derivative, 'Sigmoid'),
        (tanh, tanh_derivative, 'Tanh'),
        (relu, relu_derivative, 'ReLU'),
        (leaky_relu, leaky_relu_derivative, 'Leaky ReLU'),
        (elu, lambda x: np.where(x > 0, 1, elu(x) + 1), 'ELU'),
        (swish, lambda x: sigmoid(x) + x * sigmoid_derivative(x), 'Swish')
    ]
    
    for i, (func, derivative, name) in enumerate(functions):
        row, col = i // 3, i % 3
        
        # í•¨ìˆ˜ ê·¸ë˜í”„
        y = func(x)
        axes[row, col].plot(x, y, 'b-', linewidth=2, label=f'{name}')
        
        # ë„í•¨ìˆ˜ ê·¸ë˜í”„
        dy = derivative(x)
        axes[row, col].plot(x, dy, 'r--', linewidth=2, label=f"{name} ë„í•¨ìˆ˜")
        
        axes[row, col].set_xlabel('x')
        axes[row, col].set_ylabel('f(x)')
        axes[row, col].set_title(f'{name} í™œì„±í™” í•¨ìˆ˜')
        axes[row, col].grid(True, alpha=0.3)
        axes[row, col].legend()
        axes[row, col].axhline(y=0, color='k', linewidth=0.5)
        axes[row, col].axvline(x=0, color='k', linewidth=0.5)
    
    plt.tight_layout()
    plt.show()
    
    # í™œì„±í™” í•¨ìˆ˜ íŠ¹ì„± ë¹„êµí‘œ
    print("í™œì„±í™” í•¨ìˆ˜ íŠ¹ì„± ë¹„êµ:")
    print("=" * 80)
    print(f"{'í•¨ìˆ˜':<12} {'ë²”ìœ„':<15} {'ë¯¸ë¶„ê°€ëŠ¥':<8} {'ê¸°ìš¸ê¸°ì†Œì‹¤':<10} {'íŠ¹ì§•'}")
    print("-" * 80)
    print(f"{'Sigmoid':<12} {'(0, 1)':<15} {'Yes':<8} {'High':<10} {'í™•ë¥  ì¶œë ¥, ì´ì§„ë¶„ë¥˜'}")
    print(f"{'Tanh':<12} {'(-1, 1)':<15} {'Yes':<8} {'High':<10} {'0 ì¤‘ì‹¬, ì€ë‹‰ì¸µ'}")
    print(f"{'ReLU':<12} {'[0, âˆ)':<15} {'No':<8} {'Low':<10} {'ë¹ ë¥¸ê³„ì‚°, ì£½ì€ë‰´ëŸ°'}")
    print(f"{'Leaky ReLU':<12} {'(-âˆ, âˆ)':<15} {'No':<8} {'Low':<10} {'ì£½ì€ë‰´ëŸ° í•´ê²°'}")
    print(f"{'ELU':<12} {'(-Î±, âˆ)':<15} {'No':<8} {'Low':<10} {'ë¶€ë“œëŸ¬ìš´ ìŒìˆ˜'}")
    print(f"{'Swish':<12} {'(-âˆ, âˆ)':<15} {'Yes':<8} {'Low':<10} {'ìê¸°ì¡°ì •, ìµœì‹ '}")

activation_functions_comparison()
```

> **ì°¸ê³ **: í™œì„±í™” í•¨ìˆ˜ ì„ íƒì€ ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì€ë‹‰ì¸µì—ëŠ” ReLU, ì¶œë ¥ì¸µì—ëŠ” ë¬¸ì œ ìœ í˜•ì— ë§ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 

---

## 9. ì‹ ê²½ë§ êµ¬ì¡°

### 9.1 TensorFlow/Keras ê¸°ì´ˆ

#### ëª¨ë¸ ìƒì„± ë°©ë²•

**Sequential ëª¨ë¸**:
```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Sequential ëª¨ë¸ (ìˆœì°¨ì  ì¸µ ìŒ“ê¸°)
def create_sequential_model():
    """Sequential ëª¨ë¸ ìƒì„± ì˜ˆì œ"""
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return model

# Functional API ëª¨ë¸
def create_functional_model():
    """Functional API ëª¨ë¸ ìƒì„± ì˜ˆì œ"""
    
    inputs = tf.keras.Input(shape=(784,))
    x = tf.keras.layers.Dense(128, activation='relu')(inputs)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model

# ëª¨ë¸ ë¹„êµ
sequential_model = create_sequential_model()
functional_model = create_functional_model()

print("Sequential ëª¨ë¸ êµ¬ì¡°:")
sequential_model.summary()

print("\nFunctional API ëª¨ë¸ êµ¬ì¡°:")
functional_model.summary()
```

#### ëª¨ë¸ ì»´íŒŒì¼ê³¼ í›ˆë ¨
```python
def compile_and_train_model():
    """ëª¨ë¸ ì»´íŒŒì¼ ë° í›ˆë ¨ ì˜ˆì œ"""
    
    # MNIST ë°ì´í„° ë¡œë“œ
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0
    X_train = X_train.reshape(-1, 784)
    X_test = X_test.reshape(-1, 784)
    
    # ëª¨ë¸ ìƒì„±
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=3,
            min_lr=1e-7
        )
    ]
    
    # ëª¨ë¸ í›ˆë ¨
    history = model.fit(
        X_train, y_train,
        batch_size=128,
        epochs=20,
        validation_split=0.1,
        callbacks=callbacks,
        verbose=1
    )
    
    # ëª¨ë¸ í‰ê°€
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤')
    plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤')
    plt.title('ëª¨ë¸ ì†ì‹¤')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„')
    plt.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„')
    plt.title('ëª¨ë¸ ì •í™•ë„')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return model, history

trained_model, training_history = compile_and_train_model()
```

---

## 10. CNN (í•©ì„±ê³± ì‹ ê²½ë§)

### 10.1 CNN ê°œë…ê³¼ êµ¬ì¡°

**CNN(Convolutional Neural Network)**ì€ ì´ë¯¸ì§€ ì²˜ë¦¬ì— íŠ¹í™”ëœ ì‹ ê²½ë§ìœ¼ë¡œ, ì´ë¯¸ì§€ì˜ ê³µê°„ì  ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

#### CNNì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œ

```python
def demonstrate_cnn_operations():
    """CNN ì—°ì‚° ì‹œì—°"""
    
    # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±
    image = np.random.rand(1, 28, 28, 1)  # ë°°ì¹˜í¬ê¸°, ë†’ì´, ë„ˆë¹„, ì±„ë„
    
    # 1. í•©ì„±ê³± ì¸µ
    conv_layer = tf.keras.layers.Conv2D(
        filters=32,
        kernel_size=(3, 3),
        activation='relu',
        padding='same'
    )
    
    conv_output = conv_layer(image)
    print(f"í•©ì„±ê³± ê²°ê³¼ í¬ê¸°: {conv_output.shape}")
    
    # 2. í’€ë§ ì¸µ
    pool_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))
    pool_output = pool_layer(conv_output)
    print(f"í’€ë§ ê²°ê³¼ í¬ê¸°: {pool_output.shape}")
    
    # 3. ë°°ì¹˜ ì •ê·œí™”
    bn_layer = tf.keras.layers.BatchNormalization()
    bn_output = bn_layer(pool_output)
    print(f"ë°°ì¹˜ ì •ê·œí™” ê²°ê³¼ í¬ê¸°: {bn_output.shape}")
    
    # CNN ëª¨ë¸ ì „ì²´ êµ¬ì¡°
    model = tf.keras.Sequential([
        # ì²« ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Dropout(0.25),
        
        # ë‘ ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Dropout(0.25),
        
        # ì„¸ ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.25),
        
        # ì™„ì „ì—°ê²°ì¸µ
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    model.summary()
    return model

cnn_model = demonstrate_cnn_operations()
```

### 10.2 CIFAR-10 ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸

```python
def cifar10_classification():
    """CIFAR-10 ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸"""
    
    # ë°ì´í„° ë¡œë“œ
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
    
    # í´ë˜ìŠ¤ ì´ë¦„
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck']
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0
    
    # ì¼ë¶€ ì´ë¯¸ì§€ ì‹œê°í™”
    plt.figure(figsize=(12, 8))
    for i in range(20):
        plt.subplot(4, 5, i + 1)
        plt.imshow(X_train[i])
        plt.title(f'{class_names[y_train[i][0]]}')
        plt.axis('off')
    plt.suptitle('CIFAR-10 ìƒ˜í”Œ ì´ë¯¸ì§€')
    plt.tight_layout()
    plt.show()
    
    # ë°ì´í„° ì¦ê°•
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.2,
        shear_range=0.2,
        fill_mode='nearest'
    )
    
    # CNN ëª¨ë¸ ìƒì„±
    model = tf.keras.Sequential([
        # ì²« ë²ˆì§¸ ë¸”ë¡
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Dropout(0.25),
        
        # ë‘ ë²ˆì§¸ ë¸”ë¡
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Dropout(0.25),
        
        # ì„¸ ë²ˆì§¸ ë¸”ë¡
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.Dropout(0.25),
        
        # ì™„ì „ì—°ê²°ì¸µ
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5)
    ]
    
    # ëª¨ë¸ í›ˆë ¨
    history = model.fit(
        datagen.flow(X_train, y_train, batch_size=32),
        epochs=50,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    # ì„±ëŠ¥ í‰ê°€
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    predictions = model.predict(X_test[:20])
    predicted_classes = np.argmax(predictions, axis=1)
    
    plt.figure(figsize=(15, 8))
    for i in range(20):
        plt.subplot(4, 5, i + 1)
        plt.imshow(X_test[i])
        actual = class_names[y_test[i][0]]
        predicted = class_names[predicted_classes[i]]
        confidence = predictions[i][predicted_classes[i]]
        
        color = 'green' if actual == predicted else 'red'
        plt.title(f'ì‹¤ì œ: {actual}\nì˜ˆì¸¡: {predicted}\n({confidence:.2f})', 
                 color=color, fontsize=8)
        plt.axis('off')
    
    plt.suptitle('CIFAR-10 ì˜ˆì¸¡ ê²°ê³¼')
    plt.tight_layout()
    plt.show()
    
    return model, history

cifar_model, cifar_history = cifar10_classification()
```

---

## 11. RNN (ìˆœí™˜ ì‹ ê²½ë§)

### 11.1 RNN ê¸°ë³¸ ê°œë…

**RNN(Recurrent Neural Network)**ì€ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì‹ ê²½ë§ìœ¼ë¡œ, ì‹œê°„ì  ì˜ì¡´ì„±ì„ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
def demonstrate_rnn_concepts():
    """RNN ê°œë… ì‹œì—°"""
    
    # ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    def create_time_series_data(n_samples=1000, seq_length=50):
        """ì‚¬ì¸íŒŒ ê¸°ë°˜ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±"""
        time = np.linspace(0, 100, n_samples + seq_length)
        data = np.sin(time) + 0.5 * np.sin(3 * time) + 0.1 * np.random.randn(len(time))
        
        X, y = [], []
        for i in range(n_samples):
            X.append(data[i:i + seq_length])
            y.append(data[i + seq_length])
        
        return np.array(X), np.array(y)
    
    X, y = create_time_series_data()
    X = X.reshape(X.shape[0], X.shape[1], 1)  # (samples, timesteps, features)
    
    # ë°ì´í„° ë¶„í• 
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    print(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}")
    
    # ë‹¤ì–‘í•œ RNN ëª¨ë¸ ë¹„êµ
    models = {}
    
    # 1. ë‹¨ìˆœ RNN
    models['Simple RNN'] = tf.keras.Sequential([
        tf.keras.layers.SimpleRNN(50, activation='tanh', input_shape=(50, 1)),
        tf.keras.layers.Dense(25),
        tf.keras.layers.Dense(1)
    ])
    
    # 2. LSTM
    models['LSTM'] = tf.keras.Sequential([
        tf.keras.layers.LSTM(50, input_shape=(50, 1)),
        tf.keras.layers.Dense(25),
        tf.keras.layers.Dense(1)
    ])
    
    # 3. GRU
    models['GRU'] = tf.keras.Sequential([
        tf.keras.layers.GRU(50, input_shape=(50, 1)),
        tf.keras.layers.Dense(25),
        tf.keras.layers.Dense(1)
    ])
    
    # 4. ì–‘ë°©í–¥ LSTM
    models['Bidirectional LSTM'] = tf.keras.Sequential([
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(25), input_shape=(50, 1)),
        tf.keras.layers.Dense(25),
        tf.keras.layers.Dense(1)
    ])
    
    # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
    results = {}
    
    plt.figure(figsize=(16, 12))
    
    for i, (name, model) in enumerate(models.items()):
        print(f"\n{name} í›ˆë ¨ ì¤‘...")
        
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        history = model.fit(X_train, y_train, epochs=50, batch_size=32, 
                          validation_data=(X_test, y_test), verbose=0)
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)
        mse = np.mean((y_test - y_pred.flatten())**2)
        results[name] = mse
        
        # ì‹œê°í™”
        plt.subplot(2, 2, i + 1)
        plt.plot(y_test[:100], label='ì‹¤ì œê°’', alpha=0.7)
        plt.plot(y_pred[:100], label='ì˜ˆì¸¡ê°’', alpha=0.7)
        plt.title(f'{name}\nMSE: {mse:.6f}')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ê²°ê³¼ ë¹„êµ
    print("\nëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (MSE):")
    for name, mse in sorted(results.items(), key=lambda x: x[1]):
        print(f"{name}: {mse:.6f}")
    
    return models, results

rnn_models, rnn_results = demonstrate_rnn_concepts()
```

### 11.2 ìì—°ì–´ ì²˜ë¦¬ ì˜ˆì œ

```python
def sentiment_analysis_example():
    """ê°ì • ë¶„ì„ ì˜ˆì œ"""
    
    # IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°
    max_features = 10000
    maxlen = 200
    
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(
        num_words=max_features
    )
    
    print(f"í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {len(X_train)}")
    print(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(X_test)}")
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)
    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)
    
    # ëª¨ë¸ ìƒì„±
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(max_features, 128, input_length=maxlen),
        tf.keras.layers.LSTM(64, dropout=0.5, recurrent_dropout=0.5),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    print("ëª¨ë¸ êµ¬ì¡°:")
    model.summary()
    
    # í›ˆë ¨
    history = model.fit(
        X_train, y_train,
        batch_size=32,
        epochs=10,
        validation_data=(X_test, y_test),
        verbose=1
    )
    
    # í‰ê°€
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤')
    plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤')
    plt.title('ëª¨ë¸ ì†ì‹¤')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„')
    plt.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„')
    plt.title('ëª¨ë¸ ì •í™•ë„')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return model, history

sentiment_model, sentiment_history = sentiment_analysis_example()
```

---

## 12. ê³ ê¸‰ ë”¥ëŸ¬ë‹ ê¸°ë²•

### 12.1 ì „ì´í•™ìŠµ (Transfer Learning)

```python
def transfer_learning_example():
    """ì „ì´í•™ìŠµ ì˜ˆì œ"""
    
    # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
    base_model = tf.keras.applications.VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
    
    # ê¸°ë³¸ ëª¨ë¸ ë™ê²°
    base_model.trainable = False
    
    # ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ì¶”ê°€
    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')  # CIFAR-10ìš©
    ])
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print("ì „ì´í•™ìŠµ ëª¨ë¸ êµ¬ì¡°:")
    model.summary()
    
    return model

transfer_model = transfer_learning_example()
```

### 12.2 ì •ê·œí™” ê¸°ë²•

```python
def regularization_techniques():
    """ì •ê·œí™” ê¸°ë²•ë“¤"""
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0
    X_train = X_train.reshape(-1, 784)
    X_test = X_test.reshape(-1, 784)
    
    # ë‹¤ì–‘í•œ ì •ê·œí™” ê¸°ë²•ì„ ì ìš©í•œ ëª¨ë¸ë“¤
    models = {}
    
    # 1. ê¸°ë³¸ ëª¨ë¸ (ì •ê·œí™” ì—†ìŒ)
    models['ê¸°ë³¸ ëª¨ë¸'] = tf.keras.Sequential([
        tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # 2. ë“œë¡­ì•„ì›ƒ
    models['ë“œë¡­ì•„ì›ƒ'] = tf.keras.Sequential([
        tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # 3. ë°°ì¹˜ ì •ê·œí™”
    models['ë°°ì¹˜ ì •ê·œí™”'] = tf.keras.Sequential([
        tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # 4. L2 ì •ê·œí™”
    models['L2 ì •ê·œí™”'] = tf.keras.Sequential([
        tf.keras.layers.Dense(512, activation='relu', input_shape=(784,),
                            kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dense(256, activation='relu',
                            kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # 5. ëª¨ë“  ê¸°ë²• ê²°í•©
    models['ëª¨ë“  ê¸°ë²•'] = tf.keras.Sequential([
        tf.keras.layers.Dense(512, activation='relu', input_shape=(784,),
                            kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(256, activation='relu',
                            kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # ëª¨ë¸ í›ˆë ¨ ë° ë¹„êµ
    results = {}
    histories = {}
    
    for name, model in models.items():
        print(f"\n{name} í›ˆë ¨ ì¤‘...")
        
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        history = model.fit(
            X_train, y_train,
            batch_size=128,
            epochs=20,
            validation_data=(X_test, y_test),
            verbose=0
        )
        
        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
        results[name] = test_acc
        histories[name] = history
        
        print(f"{name} í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(15, 10))
    
    # ì •í™•ë„ ë¹„êµ
    plt.subplot(2, 3, 1)
    names = list(results.keys())
    accuracies = list(results.values())
    bars = plt.bar(names, accuracies)
    plt.title('ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ì •í™•ë„')
    plt.ylabel('ì •í™•ë„')
    plt.xticks(rotation=45)
    
    # ê° ë§‰ëŒ€ì— ê°’ í‘œì‹œ
    for bar, acc in zip(bars, accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{acc:.3f}', ha='center', va='bottom')
    
    # í•™ìŠµ ê³¡ì„ ë“¤
    colors = ['blue', 'red', 'green', 'orange', 'purple']
    for i, (name, history) in enumerate(histories.items()):
        plt.subplot(2, 3, i + 2)
        plt.plot(history.history['accuracy'], label='í›ˆë ¨', color=colors[i % len(colors)])
        plt.plot(history.history['val_accuracy'], label='ê²€ì¦', 
                color=colors[i % len(colors)], linestyle='--')
        plt.title(f'{name} í•™ìŠµ ê³¡ì„ ')
        plt.xlabel('Epoch')
        plt.ylabel('ì •í™•ë„')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return models, results

reg_models, reg_results = regularization_techniques()
```

---

## 13. ëª¨ë¸ í‰ê°€ ë° ìµœì í™”

### 13.1 ì„±ëŠ¥ ì§€í‘œ

```python
def comprehensive_model_evaluation():
    """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
    
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
    
    # CIFAR-10 ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
    X_test = X_test.astype('float32') / 255.0
    
    # ê°„ë‹¨í•œ CNN ëª¨ë¸ ìƒì„± ë° í›ˆë ¨ (ì˜ˆì‹œ)
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    # ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´ ì ì€ ì—í¬í¬ ì‚¬ìš©
    model.fit(X_train[:5000], y_train[:5000], epochs=5, verbose=0)
    
    # ì˜ˆì¸¡
    y_pred_proba = model.predict(X_test[:1000])
    y_pred = np.argmax(y_pred_proba, axis=1)
    y_true = y_test[:1000].flatten()
    
    # 1. ê¸°ë³¸ ì§€í‘œë“¤
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    
    print("ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ:")
    print(f"ì •í™•ë„ (Accuracy): {accuracy:.4f}")
    print(f"ì •ë°€ë„ (Precision): {precision:.4f}")
    print(f"ì¬í˜„ìœ¨ (Recall): {recall:.4f}")
    print(f"F1 ì ìˆ˜: {f1:.4f}")
    
    # 2. í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¦¬í¬íŠ¸
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck']
    
    print("\ní´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
    print(classification_report(y_true, y_pred, target_names=class_names))
    
    # 3. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(12, 10))
    
    plt.subplot(2, 2, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('í˜¼ë™ í–‰ë ¬')
    plt.xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤')
    plt.ylabel('ì‹¤ì œ í´ë˜ìŠ¤')
    
    # 4. í´ë˜ìŠ¤ë³„ ì •í™•ë„
    class_accuracy = cm.diagonal() / cm.sum(axis=1)
    
    plt.subplot(2, 2, 2)
    bars = plt.bar(class_names, class_accuracy)
    plt.title('í´ë˜ìŠ¤ë³„ ì •í™•ë„')
    plt.ylabel('ì •í™•ë„')
    plt.xticks(rotation=45)
    
    for bar, acc in zip(bars, class_accuracy):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{acc:.2f}', ha='center', va='bottom')
    
    # 5. ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„í¬
    max_probs = np.max(y_pred_proba, axis=1)
    
    plt.subplot(2, 2, 3)
    plt.hist(max_probs, bins=30, alpha=0.7, edgecolor='black')
    plt.title('ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„í¬')
    plt.xlabel('ìµœëŒ€ í™•ë¥ ')
    plt.ylabel('ë¹ˆë„')
    plt.grid(True, alpha=0.3)
    
    # 6. í‹€ë¦° ì˜ˆì¸¡ ë¶„ì„
    wrong_predictions = y_true != y_pred
    wrong_confidences = max_probs[wrong_predictions]
    correct_confidences = max_probs[~wrong_predictions]
    
    plt.subplot(2, 2, 4)
    plt.hist(correct_confidences, bins=20, alpha=0.7, label='ì •í™•í•œ ì˜ˆì¸¡', color='green')
    plt.hist(wrong_confidences, bins=20, alpha=0.7, label='í‹€ë¦° ì˜ˆì¸¡', color='red')
    plt.title('ì •í™•/í‹€ë¦° ì˜ˆì¸¡ì˜ í™•ì‹ ë„ ë¹„êµ')
    plt.xlabel('í™•ì‹ ë„')
    plt.ylabel('ë¹ˆë„')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 7. ì„±ëŠ¥ ìš”ì•½
    print(f"\nì„±ëŠ¥ ìš”ì•½:")
    print(f"ì „ì²´ ì •í™•ë„: {accuracy:.4f}")
    print(f"í‰ê·  í´ë˜ìŠ¤ ì •í™•ë„: {np.mean(class_accuracy):.4f}")
    print(f"ìµœê³  í´ë˜ìŠ¤ ì •í™•ë„: {np.max(class_accuracy):.4f} ({class_names[np.argmax(class_accuracy)]})")
    print(f"ìµœì € í´ë˜ìŠ¤ ì •í™•ë„: {np.min(class_accuracy):.4f} ({class_names[np.argmin(class_accuracy)]})")
    print(f"í‰ê·  ì˜ˆì¸¡ í™•ì‹ ë„: {np.mean(max_probs):.4f}")
    
    return model, {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'class_accuracy': class_accuracy,
        'confusion_matrix': cm
    }

eval_model, eval_metrics = comprehensive_model_evaluation()
```

### 13.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
def hyperparameter_tuning_example():
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì˜ˆì œ"""
    
    # Keras Tuner ì‚¬ìš© (ì„¤ì¹˜ í•„ìš”: pip install keras-tuner)
    try:
        import keras_tuner as kt
        
        def build_model(hp):
            """í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì•„ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
            model = tf.keras.Sequential()
            
            # ì²« ë²ˆì§¸ ì¸µ
            model.add(tf.keras.layers.Dense(
                units=hp.Int('units_1', min_value=32, max_value=512, step=32),
                activation=hp.Choice('activation_1', ['relu', 'tanh']),
                input_shape=(784,)
            ))
            
            # ë“œë¡­ì•„ì›ƒ
            model.add(tf.keras.layers.Dropout(
                rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)
            ))
            
            # ë‘ ë²ˆì§¸ ì¸µ (ì„ íƒì )
            if hp.Boolean('use_second_layer'):
                model.add(tf.keras.layers.Dense(
                    units=hp.Int('units_2', min_value=32, max_value=256, step=32),
                    activation=hp.Choice('activation_2', ['relu', 'tanh'])
                ))
                model.add(tf.keras.layers.Dropout(
                    rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)
                ))
            
            # ì¶œë ¥ì¸µ
            model.add(tf.keras.layers.Dense(10, activation='softmax'))
            
            # ì»´íŒŒì¼
            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')
                ),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
            return model
        
        # ë°ì´í„° ì¤€ë¹„
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
        X_train = X_train.astype('float32') / 255.0
        X_test = X_test.astype('float32') / 255.0
        X_train = X_train.reshape(-1, 784)
        X_test = X_test.reshape(-1, 784)
        
        # ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ íŠœë‹
        X_train_small = X_train[:5000]
        y_train_small = y_train[:5000]
        
        # íŠœë„ˆ ì„¤ì •
        tuner = kt.RandomSearch(
            build_model,
            objective='val_accuracy',
            max_trials=10,  # ì‹¤ì œë¡œëŠ” ë” ë§ì´ ì„¤ì •
            directory='hyperparameter_tuning',
            project_name='mnist_tuning'
        )
        
        print("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
        tuner.search(X_train_small, y_train_small,
                    epochs=5,
                    validation_split=0.2,
                    verbose=0)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
        
        print("ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        print(f"ì²« ë²ˆì§¸ ì¸µ ìœ ë‹› ìˆ˜: {best_hps.get('units_1')}")
        print(f"ì²« ë²ˆì§¸ ì¸µ í™œì„±í™” í•¨ìˆ˜: {best_hps.get('activation_1')}")
        print(f"ì²« ë²ˆì§¸ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨: {best_hps.get('dropout_1')}")
        print(f"ë‘ ë²ˆì§¸ ì¸µ ì‚¬ìš©: {best_hps.get('use_second_layer')}")
        if best_hps.get('use_second_layer'):
            print(f"ë‘ ë²ˆì§¸ ì¸µ ìœ ë‹› ìˆ˜: {best_hps.get('units_2')}")
            print(f"ë‘ ë²ˆì§¸ ì¸µ í™œì„±í™” í•¨ìˆ˜: {best_hps.get('activation_2')}")
            print(f"ë‘ ë²ˆì§¸ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨: {best_hps.get('dropout_2')}")
        print(f"í•™ìŠµë¥ : {best_hps.get('learning_rate')}")
        
        # ìµœì  ëª¨ë¸ë¡œ ì¬í›ˆë ¨
        best_model = tuner.hypermodel.build(best_hps)
        history = best_model.fit(X_train, y_train, epochs=10, 
                                validation_data=(X_test, y_test), verbose=0)
        
        test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)
        print(f"\nìµœì  ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
        
        return best_model, best_hps
        
    except ImportError:
        print("Keras Tunerê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜í•˜ë ¤ë©´: pip install keras-tuner")
        
        # ìˆ˜ë™ ê·¸ë¦¬ë“œ ì„œì¹˜ ì˜ˆì œ
        print("\nìˆ˜ë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì˜ˆì œ:")
        
        # ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ì„œì¹˜
        param_grid = {
            'units': [64, 128, 256],
            'dropout': [0.2, 0.3, 0.5],
            'learning_rate': [0.001, 0.01]
        }
        
        best_score = 0
        best_params = {}
        
        for units in param_grid['units']:
            for dropout in param_grid['dropout']:
                for lr in param_grid['learning_rate']:
                    print(f"í…ŒìŠ¤íŠ¸ ì¤‘: units={units}, dropout={dropout}, lr={lr}")
                    
                    model = tf.keras.Sequential([
                        tf.keras.layers.Dense(units, activation='relu', input_shape=(784,)),
                        tf.keras.layers.Dropout(dropout),
                        tf.keras.layers.Dense(10, activation='softmax')
                    ])
                    
                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                                loss='sparse_categorical_crossentropy',
                                metrics=['accuracy'])
                    
                    # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ ì ì€ ë°ì´í„°ì™€ ì—í¬í¬ ì‚¬ìš©
                    history = model.fit(X_train[:1000], y_train[:1000], 
                                      epochs=3, validation_split=0.2, verbose=0)
                    
                    val_acc = max(history.history['val_accuracy'])
                    
                    if val_acc > best_score:
                        best_score = val_acc
                        best_params = {'units': units, 'dropout': dropout, 'learning_rate': lr}
        
        print(f"\nìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_score:.4f}")
        
        return None, best_params

tuning_result = hyperparameter_tuning_example()
```

---

## 14. ì‹¤ì „ í”„ë¡œì íŠ¸

### 14.1 ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸

```python
def complete_image_classification_project():
    """ì™„ì „í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸"""
    
    print("=== ê°œ/ê³ ì–‘ì´ ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸ ===")
    
    # 1. ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œë¡œëŠ” ì™¸ë¶€ ë°ì´í„° ì‚¬ìš©)
    # ì—¬ê¸°ì„œëŠ” CIFAR-10ì—ì„œ ê³ ì–‘ì´(3)ì™€ ê°œ(5)ë§Œ ì‚¬ìš©
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
    
    # ê³ ì–‘ì´(3)ì™€ ê°œ(5)ë§Œ í•„í„°ë§
    cat_dog_filter_train = (y_train == 3) | (y_train == 5)
    cat_dog_filter_test = (y_test == 3) | (y_test == 5)
    
    X_train = X_train[cat_dog_filter_train.flatten()]
    y_train = y_train[cat_dog_filter_train.flatten()]
    X_test = X_test[cat_dog_filter_test.flatten()]
    y_test = y_test[cat_dog_filter_test.flatten()]
    
    # ë¼ë²¨ ì¬ì¡°ì • (3->0: ê³ ì–‘ì´, 5->1: ê°œ)
    y_train = (y_train == 5).astype(int)
    y_test = (y_test == 5).astype(int)
    
    # ë°ì´í„° ì •ê·œí™”
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0
    
    print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}, ë¼ë²¨: {y_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}, ë¼ë²¨: {y_test.shape}")
    
    # 2. ë°ì´í„° ì¦ê°•
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.2,
        shear_range=0.2,
        fill_mode='nearest'
    )
    
    # 3. ëª¨ë¸ ì„¤ê³„
    def create_cnn_model():
        model = tf.keras.Sequential([
            # ì²« ë²ˆì§¸ ë¸”ë¡
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
            tf.keras.layers.MaxPooling2D((2, 2)),
            tf.keras.layers.Dropout(0.25),
            
            # ë‘ ë²ˆì§¸ ë¸”ë¡
            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
            tf.keras.layers.MaxPooling2D((2, 2)),
            tf.keras.layers.Dropout(0.25),
            
            # ì„¸ ë²ˆì§¸ ë¸”ë¡
            tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(0.25),
            
            # ë¶„ë¥˜ê¸°
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(512, activation='relu'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])
        
        return model
    
    model = create_cnn_model()
    
    # 4. ëª¨ë¸ ì»´íŒŒì¼
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    print("\nëª¨ë¸ êµ¬ì¡°:")
    model.summary()
    
    # 5. ì½œë°± ì„¤ì •
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=5,
            min_lr=1e-7
        ),
        tf.keras.callbacks.ModelCheckpoint(
            'best_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            mode='max'
        )
    ]
    
    # 6. ëª¨ë¸ í›ˆë ¨
    print("\nëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    history = model.fit(
        datagen.flow(X_train, y_train, batch_size=32),
        epochs=30,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    # 7. ê²°ê³¼ í‰ê°€
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
    
    # 8. ì˜ˆì¸¡ ë° ì‹œê°í™”
    predictions = model.predict(X_test)
    predicted_classes = (predictions > 0.5).astype(int).flatten()
    
    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_test, predicted_classes)
    
    plt.figure(figsize=(15, 12))
    
    # í•™ìŠµ ê³¡ì„ 
    plt.subplot(2, 3, 1)
    plt.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤')
    plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤')
    plt.title('ëª¨ë¸ ì†ì‹¤')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 3, 2)
    plt.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„')
    plt.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„')
    plt.title('ëª¨ë¸ ì •í™•ë„')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # í˜¼ë™ í–‰ë ¬
    plt.subplot(2, 3, 3)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['ê³ ì–‘ì´', 'ê°œ'], yticklabels=['ê³ ì–‘ì´', 'ê°œ'])
    plt.title('í˜¼ë™ í–‰ë ¬')
    plt.xlabel('ì˜ˆì¸¡')
    plt.ylabel('ì‹¤ì œ')
    
    # ì˜ˆì¸¡ ì˜ˆì‹œ
    sample_indices = np.random.choice(len(X_test), 9, replace=False)
    
    for i, idx in enumerate(sample_indices):
        plt.subplot(2, 3, 4 + (i % 6))
        if i >= 6:
            break
            
        plt.imshow(X_test[idx])
        actual = 'ê°œ' if y_test[idx] == 1 else 'ê³ ì–‘ì´'
        predicted = 'ê°œ' if predicted_classes[idx] == 1 else 'ê³ ì–‘ì´'
        confidence = predictions[idx][0] if predicted_classes[idx] == 1 else 1 - predictions[idx][0]
        
        color = 'green' if actual == predicted else 'red'
        plt.title(f'ì‹¤ì œ: {actual}\nì˜ˆì¸¡: {predicted}\ní™•ì‹ ë„: {confidence:.2f}', 
                 color=color, fontsize=8)
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # 9. ì„±ëŠ¥ ë¶„ì„
    from sklearn.metrics import classification_report
    print("\nìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, predicted_classes, target_names=['ê³ ì–‘ì´', 'ê°œ']))
    
    return model, history

# í”„ë¡œì íŠ¸ ì‹¤í–‰
final_model, final_history = complete_image_classification_project()
```

### 14.2 ëª¨ë¸ ë°°í¬

```python
def model_deployment_example():
    """ëª¨ë¸ ë°°í¬ ì˜ˆì œ"""
    
    # Flask API ì„œë²„ ì½”ë“œ ìƒì„±
    flask_code = '''
from flask import Flask, request, jsonify
import tensorflow as tf
import numpy as np
from PIL import Image
import io
import base64

app = Flask(__name__)

# ëª¨ë¸ ë¡œë“œ
model = tf.keras.models.load_model('best_model.h5')

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "healthy"})

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # ì´ë¯¸ì§€ ë°ì´í„° ë°›ê¸°
        data = request.get_json()
        
        if 'image' not in data:
            return jsonify({"error": "ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        # Base64 ë””ì½”ë”©
        image_data = base64.b64decode(data['image'])
        image = Image.open(io.BytesIO(image_data))
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image = image.resize((32, 32))
        image_array = np.array(image) / 255.0
        image_array = np.expand_dims(image_array, axis=0)
        
        # ì˜ˆì¸¡
        prediction = model.predict(image_array)
        probability = float(prediction[0][0])
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            "prediction": "ê°œ" if probability > 0.5 else "ê³ ì–‘ì´",
            "probability": probability,
            "confidence": max(probability, 1 - probability)
        }
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
'''
    
    # íŒŒì¼ë¡œ ì €ì¥
    with open('flask_api.py', 'w', encoding='utf-8') as f:
        f.write(flask_code)
    
    print("Flask API ì„œë²„ ì½”ë“œê°€ 'flask_api.py'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # Docker ì„¤ì • íŒŒì¼
    dockerfile_content = '''
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "flask_api.py"]
'''
    
    requirements_content = '''
flask==2.3.3
tensorflow==2.13.0
pillow==10.0.0
numpy==1.24.3
'''
    
    with open('Dockerfile', 'w') as f:
        f.write(dockerfile_content)
    
    with open('requirements.txt', 'w') as f:
        f.write(requirements_content)
    
    print("Docker ì„¤ì • íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ ì½”ë“œ
    client_code = '''
import requests
import base64
import json

def test_api():
    # API ì—”ë“œí¬ì¸íŠ¸
    url = "http://localhost:5000/predict"
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ)
    # with open('test_image.jpg', 'rb') as f:
    #     image_data = f.read()
    
    # ì„ì‹œë¡œ ë”ë¯¸ ë°ì´í„° ì‚¬ìš©
    image_data = b"dummy_image_data"
    
    # Base64 ì¸ì½”ë”©
    encoded_image = base64.b64encode(image_data).decode('utf-8')
    
    # ìš”ì²­ ë°ì´í„°
    data = {
        "image": encoded_image
    }
    
    try:
        # API í˜¸ì¶œ
        response = requests.post(url, json=data)
        
        if response.status_code == 200:
            result = response.json()
            print(f"ì˜ˆì¸¡ ê²°ê³¼: {result['prediction']}")
            print(f"í™•ì‹ ë„: {result['confidence']:.2f}")
        else:
            print(f"ì˜¤ë¥˜: {response.json()}")
            
    except requests.exceptions.ConnectionError:
        print("API ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    test_api()
'''
    
    with open('test_client.py', 'w', encoding='utf-8') as f:
        f.write(client_code)
    
    print("í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ ì½”ë“œê°€ 'test_client.py'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("\në°°í¬ ê°€ì´ë“œ:")
    print("1. ëª¨ë¸ ì €ì¥: model.save('best_model.h5')")
    print("2. Flask ì„œë²„ ì‹¤í–‰: python flask_api.py")
    print("3. Docker ë¹Œë“œ: docker build -t ml-api .")
    print("4. Docker ì‹¤í–‰: docker run -p 5000:5000 ml-api")
    print("5. í…ŒìŠ¤íŠ¸: python test_client.py")

model_deployment_example()
```

---

## ì¶”ê°€ í•™ìŠµ ìë£Œ

### ê¶Œì¥ ë„ì„œ
- **"ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹"** - ì‚¬ì´í†  ê³ í‚¤
- **"í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹"** - ì˜¤ë ë¦¬ì•™ ì œë¡±
- **"íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œ"** - ê¶Œì² ë¯¼
- **"Deep Learning"** - Ian Goodfellow, Yoshua Bengio, Aaron Courville
- **"Pattern Recognition and Machine Learning"** - Christopher Bishop

### ì˜¨ë¼ì¸ ê°•ì˜
- **Coursera**: Deep Learning Specialization (Andrew Ng)
- **edX**: MIT Introduction to Machine Learning
- **Udacity**: Deep Learning Nanodegree
- **Fast.ai**: Practical Deep Learning for Coders
- **Stanford CS231n**: Convolutional Neural Networks for Visual Recognition

### ì‹¤ìŠµ í”Œë«í¼
- **Kaggle**: ë°ì´í„° ê³¼í•™ ê²½ì§„ëŒ€íšŒ í”Œë«í¼
- **Google Colab**: ë¬´ë£Œ GPU ì œê³µ Jupyter í™˜ê²½
- **Papers with Code**: ìµœì‹  ë…¼ë¬¸ê³¼ ì½”ë“œ
- **Towards Data Science**: ë¨¸ì‹ ëŸ¬ë‹ ë¸”ë¡œê·¸ í”Œë«í¼
- **GitHub**: ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸

### ìœ ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬**: TensorFlow, PyTorch, Keras
- **ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹**: Scikit-learn, XGBoost, LightGBM
- **ì»´í“¨í„° ë¹„ì „**: OpenCV, PIL/Pillow, Albumentations
- **ìì—°ì–´ ì²˜ë¦¬**: NLTK, spaCy, Transformers (Hugging Face)
- **ë°ì´í„° ì²˜ë¦¬**: Pandas, NumPy, Dask
- **ì‹œê°í™”**: Matplotlib, Seaborn, Plotly, Bokeh

### ìµœì‹  ë™í–¥
- **Transformer ì•„í‚¤í…ì²˜**: BERT, GPT, T5
- **ì»´í“¨í„° ë¹„ì „**: Vision Transformer, EfficientNet
- **ìƒì„± ëª¨ë¸**: GAN, VAE, Diffusion Models
- **ê°•í™”í•™ìŠµ**: Deep Q-Learning, Policy Gradient
- **ë©”íƒ€í•™ìŠµ**: Few-shot Learning, MAML
- **ì„¤ëª… ê°€ëŠ¥í•œ AI**: LIME, SHAP, GradCAM

---

## í•™ìŠµ ë¡œë“œë§µ

### ì´ˆê¸‰ (1-2ê°œì›”)
1. **Python ê¸°ì´ˆ ë¬¸ë²• ìˆ™ë‹¬**
   - ë³€ìˆ˜, í•¨ìˆ˜, í´ë˜ìŠ¤, ëª¨ë“ˆ
   - ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬, íŠœí”Œ í™œìš©
   - íŒŒì¼ ì…ì¶œë ¥, ì˜ˆì™¸ ì²˜ë¦¬

2. **ìˆ˜í•™ ë° í†µê³„ ê¸°ì´ˆ**
   - ì„ í˜•ëŒ€ìˆ˜: ë²¡í„°, í–‰ë ¬ ì—°ì‚°
   - í™•ë¥ ê³¼ í†µê³„: ë¶„í¬, ê°€ì„¤ê²€ì •
   - ë¯¸ì ë¶„: í¸ë¯¸ë¶„, ì—°ì‡„ë²•ì¹™

3. **ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©**
   - NumPy: ë°°ì—´ ì—°ì‚°
   - Pandas: ë°ì´í„° ì¡°ì‘
   - Matplotlib: ì‹œê°í™”

4. **ê°„ë‹¨í•œ í”„ë¡œì íŠ¸**
   - ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
   - ê¸°ë³¸ì ì¸ ë¶„ë¥˜/íšŒê·€ ë¬¸ì œ

### ì¤‘ê¸‰ (3-4ê°œì›”)
1. **ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ**
   - ì§€ë„/ë¹„ì§€ë„/ê°•í™”í•™ìŠµ ê°œë…
   - ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ì´í•´
   - ëª¨ë¸ í‰ê°€ ë° ê²€ì¦

2. **ë”¥ëŸ¬ë‹ ì…ë¬¸**
   - ì‹ ê²½ë§ ê¸°ì´ˆ ì´ë¡ 
   - TensorFlow/Keras í™œìš©
   - ì†ì‹¤í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜

3. **ì‹¤ì „ í”„ë¡œì íŠ¸**
   - MNIST ì†ê¸€ì”¨ ë¶„ë¥˜
   - ì´ë¯¸ì§€ ë¶„ë¥˜ (CIFAR-10)
   - ê°„ë‹¨í•œ ìì—°ì–´ ì²˜ë¦¬

4. **ëª¨ë¸ ê°œì„ **
   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
   - ì •ê·œí™” ê¸°ë²•
   - ë°ì´í„° ì¦ê°•

### ê³ ê¸‰ (5-6ê°œì›”)
1. **ê³ ê¸‰ ì•„í‚¤í…ì²˜**
   - CNN ì‹¬í™” (ResNet, DenseNet)
   - RNN ì‹¬í™” (LSTM, GRU, Attention)
   - Transformer ì´í•´

2. **ì „ë¬¸ ë¶„ì•¼**
   - ì»´í“¨í„° ë¹„ì „: ê°ì²´ ê²€ì¶œ, ì„¸ê·¸ë©˜í…Œì´ì…˜
   - ìì—°ì–´ ì²˜ë¦¬: ì–¸ì–´ ëª¨ë¸, ê¸°ê³„ ë²ˆì—­
   - ìƒì„± ëª¨ë¸: GAN, VAE

3. **ì‹¤ë¬´ ê¸°ìˆ **
   - ëª¨ë¸ ë°°í¬ (Flask, FastAPI)
   - í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ (AWS, GCP, Azure)
   - MLOps: ëª¨ë¸ ê´€ë¦¬, ëª¨ë‹ˆí„°ë§

4. **ì—°êµ¬ ë° ê°œë°œ**
   - ìµœì‹  ë…¼ë¬¸ ì½ê¸°
   - ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬
   - ê°œì¸ í”„ë¡œì íŠ¸ í¬íŠ¸í´ë¦¬ì˜¤

### ì „ë¬¸ê°€ (6ê°œì›” ì´ìƒ)
1. **íŠ¹í™” ë¶„ì•¼ ì‹¬í™”**
   - ì„ íƒí•œ ë¶„ì•¼ì˜ ìµœì‹  ê¸°ìˆ 
   - ì‚°ì—…ë³„ ì‘ìš© ì‚¬ë¡€
   - ì—°êµ¬ ë™í–¥ íŒŒì•…

2. **ë¦¬ë”ì‹­ ë° í˜‘ì—…**
   - íŒ€ í”„ë¡œì íŠ¸ ë¦¬ë”©
   - ê¸°ìˆ  ë©˜í† ë§
   - ì»¨í¼ëŸ°ìŠ¤ ë°œí‘œ

3. **ì§€ì†ì  í•™ìŠµ**
   - ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ í•™ìŠµ
   - ë‹¤ë¥¸ ë¶„ì•¼ì™€ì˜ ìœµí•©
   - ì°½ì˜ì  ë¬¸ì œ í•´ê²°

> **ì°¸ê³ **: ê¾¸ì¤€í•œ ì‹¤ìŠµê³¼ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë¡ ê³¼ ì‹¤ìŠµì˜ ê· í˜•ì„ ë§ì¶° í•™ìŠµí•˜ê³ , ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²½í—˜ì„ ìŒ“ìœ¼ì„¸ìš”!