# 🧠 Deep Learning 정리

##### 🗓️ 2025.08.07
##### 📝 Writer : Moon19ht

---

## 📚 목차

1. [개요](#📋-개요)
2. [Practice 1: KoNLPy 활용 한글 자연어 처리](#📖-practice-1-konlpy-활용-한글-자연어-처리)
3. [Practice 2: 텍스트 벡터화 (Text Vectorization)](#📖-practice-2-텍스트-벡터화-text-vectorization)
4. [Practice 3: NSMC 감정분석 데이터 파이프라인](#📖-practice-3-nsmc-감정분석-데이터-파이프라인)
5. [전체 학습 성과](#🎓-전체-학습-성과)

---

## 📋 개요

이번 세션에서는 **한글 자연어 처리(Korean NLP)**를 딥러닝 기법과 결합하여 학습했습니다. 3개의 실습 노트북을 통해 기초적인 한글 텍스트 처리부터 실제 데이터셋을 활용한 고급 감정분석 파이프라인까지 포괄적으로 다뤘습니다.

### 학습 진행 과정
1. **Practice 1**: 기초 - KoNLPy를 이용한 한글 텍스트 전처리
2. **Practice 2**: 통합 - TensorFlow TextVectorization과 한글 텍스트 연동
3. **Practice 3**: 응용 - 실제 NSMC 데이터셋을 이용한 감정분석 파이프라인 구축

---

## 📖 Practice 1: KoNLPy 활용 한글 자연어 처리

### 🎯 주요 학습 목표
- KoNLPy를 이용한 한글 형태소 분석 마스터
- 한글 텍스트 전처리 기법 이해
- 정규식을 활용한 한글 텍스트 정제 학습

### 🔧 핵심 기술 스택
- **KoNLPy**: 한글 자연어 처리 전용 라이브러리
- **Okt (Open Korean Text)**: 트위터 기반 한글 토크나이저
- **정규표현식**: 텍스트 정제 및 정규화

### 📊 핵심 개념

#### 1. KoNLPy 환경 설정
```bash
# 설치 요구사항
1. Java 설치 및 환경변수 등록
2. CMD 관리자 권한으로 실행
3. conda activate mytensorflow
4. pip install konlpy
```

#### 2. 한글 텍스트 분석 방법들
- **형태소 분석 (Morphological Analysis)**: 텍스트를 의미 단위로 분리
- **품사 태깅 (POS Tagging)**: 문법적 범주 할당
- **명사 추출 (Noun Extraction)**: 명사만 추출
- **정규화 (Normalization)**: 비표준어를 표준어로 변환
- **어간 추출 (Stemming)**: 동사/형용사의 원형 복원

#### 3. 한글 전용 전처리 파이프라인
```python
def clean_text(text):
    text = text.lower()  # 소문자 변환
    # 한글, 영어, 숫자, 공백만 유지
    text = re.sub(r"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\s]", "", text)
    return text
```

### 🧪 실습 예시
- **샘플 텍스트**: "한글 자연어 처리는 매우 어렵습니다. ㅜㅜ 네이버 영화평론 분석을 해보겠습니다"
- **형태소 분석 결과**: ['한글', '자연어', '처리', '는', '매우', '어렵습니다', '.', 'ㅜㅜ', '네이버', '영화평론', '분석', '을', '해보겠습니다']
- **품사 태깅 결과**: [('한글', 'Noun'), ('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('매우', 'Noun'), ('어렵습니다', 'Adjective')]
- **명사만 추출**: ['한글', '자연어', '처리', '매우', '네이버', '영화평론', '분석']

### 💡 핵심 학습 성과
- 한글 언어구조와 형태론에 대한 이해
- 한글 텍스트를 위한 실용적 정제 기법
- 한글 NLP 전처리 파이프라인의 기초 마련

---

## 📖 Practice 2: 텍스트 벡터화 (Text Vectorization)

### 🎯 주요 학습 목표
- KoNLPy와 TensorFlow TextVectorization 통합
- 어휘 사전 구축 및 텍스트-숫자 변환 학습
- 딥러닝을 위한 시퀀스 패딩 및 자르기 이해

### 🔧 핵심 기술 스택
- **TensorFlow 2.15.1**: 딥러닝 프레임워크
- **TextVectorization**: TensorFlow의 텍스트 전처리 레이어
- **KoNLPy + TensorFlow 통합**: 커스텀 전처리 파이프라인

### 📊 핵심 개념

#### 1. 텍스트 벡터화 파이프라인
```
원본 텍스트 → 텍스트 정제 → 형태소 분석 → 토큰화 → 어휘 사전 구축 → 정수 시퀀스
```

#### 2. TextVectorization 설정
- **max_tokens**: 1000 (어휘 사전 크기 제한)
- **output_mode**: "int" (정수 시퀀스)
- **output_sequence_length**: 20 (고정 길이 시퀀스)
- **standardize**: 커스텀 한글 전처리 함수

#### 3. 커스텀 표준화 함수
```python
def custom_standardization_fn(text):
    # 1단계: 텍스트 정제
    cleaned_text = clean_text(text)
    # 2단계: 형태소 분석 및 토큰화
    tokens = okt.morphs(cleaned_text)
    # 3단계: 토큰들을 공백으로 결합
    return " ".join(tokens)
```

#### 4. 어휘 사전 분석
- **전체 어휘 수**: 샘플 데이터에서 25개 고유 토큰
- **특수 토큰**: '' (패딩), '[UNK]' (미지의 단어)
- **단어-인덱스 매핑**: 토큰 검색을 위한 딕셔너리

### 🧪 실습 예시
- **샘플 텍스트**: AI 챗봇, 자연어처리, 대화시스템 관련 한국어 문장
- **벡터화 결과**: 패딩이 적용된 고정 길이 정수 시퀀스
- **배치 처리**: 여러 텍스트를 동시에 처리

### 💡 핵심 학습 성과
- 한글 NLP와 딥러닝 프레임워크의 통합
- 신경망을 위한 텍스트 벡터화 이해
- 실용적인 어휘 관리 및 시퀀스 처리

---

## 📖 Practice 3: NSMC 감정분석 데이터 파이프라인

### 🎯 주요 학습 목표
- 한국어 감정분석을 위한 완전한 데이터 파이프라인 구축
- 실제 NSMC (네이버 영화 감상평 코퍼스) 데이터셋 활용
- 프로덕션 레벨 TensorFlow 데이터 처리 구현

### 🔧 핵심 기술 스택
- **NSMC 데이터셋**: 20만 개 한국어 영화 리뷰
- **Korpora**: 한국어 코퍼스 관리 라이브러리
- **TensorFlow tf.data API**: 효율적인 데이터 로딩 및 처리
- **성능 최적화**: 캐싱 및 프리페칭

### 📊 데이터셋 상세 정보
- **총 크기**: 20만 개 한국어 영화 리뷰
- **훈련 세트**: 15만 개 샘플
- **테스트 세트**: 5만 개 샘플
- **레이블**: 이진 분류 (0=부정, 1=긍정)
- **언어**: 비표준어, 슬랭, 오타를 포함한 한국어

### 🏗️ 완전한 파이프라인 구조

#### 1. 데이터 구조 조직화
```
NSMC/
├── train/
│   ├── positive/  # 73,827개 긍정 리뷰
│   └── negative/  # 74,173개 부정 리뷰
├── validation/    # 1,000개 샘플
└── test/         # 50,000개 샘플
```

#### 2. TensorFlow 데이터셋 설정
- **배치 크기**: 32
- **레이블 모드**: 이진 (int)
- **검증 분할**: 커스텀 검증 세트
- **로딩 방법**: `text_dataset_from_directory`

#### 3. 한국어 텍스트 전처리 파이프라인
```python
# KoNLPy 통합 한국어 전용 전처리
- 텍스트 정제 및 정규화
- Okt 토크나이저를 이용한 형태소 분석
- TensorFlow 호환 배치 처리
- 메모리 효율적인 전처리
```

#### 4. 프로덕션용 TextVectorization
- **어휘 사전 크기**: 10,000개 토큰 (최고 빈도 단어)
- **시퀀스 길이**: 20개 토큰 (패딩/자르기)
- **출력 모드**: 정수 시퀀스
- **특수 토큰**: 미지의 단어 처리

#### 5. 성능 최적화 전략
- **데이터셋 캐싱**: `.cache()`로 반복 접근 최적화
- **프리페칭**: `.prefetch()`로 GPU 유휴 시간 최소화
- **병렬 처리**: 최적화된 데이터 로딩
- **메모리 관리**: 효율적인 배치 처리

### 🤖 모델 구조 계획
```python
# 감정분석 모델 예시
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(10000, 128),
    tf.keras.layers.LSTM(64, dropout=0.5, return_sequences=False),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

### 📈 성능 지표
- **전처리 속도**: 최적화된 배치 처리
- **메모리 효율성**: 캐시 및 프리페치 적용 데이터셋
- **확장성**: 20만 개 이상 샘플 효율적 처리
- **GPU 활용률**: 프리페칭을 통한 유휴 시간 최소화

### 💡 핵심 학습 성과
- **프로덕션 ML 파이프라인**: 종단 간 데이터 처리 워크플로
- **대규모 한국어 NLP**: 대용량 한국어 텍스트 데이터셋 처리
- **TensorFlow 고급 기능**: tf.data API 최적화 기법
- **실제 환경 도전**: 노이즈가 많은 비공식적인 한국어 텍스트 처리

---

## 🎓 전체 학습 성과

### 습득한 기술 역량

#### 1. 한국어 NLP 전문성
- 형태소 분석 및 토큰화
- 한국어 전용 전처리 기법
- 한국어 언어학적 복잡성 처리

#### 2. 딥러닝 통합
- 신경망을 위한 텍스트 벡터화
- 어휘 관리 및 시퀀스 처리
- TensorFlow TextVectorization 마스터

#### 3. 프로덕션 파이프라인 개발
- 확장 가능한 데이터 처리 워크플로
- 성능 최적화 기법
- 메모리 효율적인 배치 처리

#### 4. 실제 환경 적용
- 대규모 한국어 데이터셋 작업
- 감정분석 파이프라인 개발
- 프로덕션 준비 모델 전처리

### 학습 진행 요약
- **기초** → **통합** → **응용**
- **이론** → **실습** → **프로덕션**
- **간단한 예제** → **복합 파이프라인** → **실제 데이터**

### 다음 단계 및 응용 방안
1. **모델 훈련**: 준비된 파이프라인으로 실제 감정분석 훈련
2. **고급 구조**: 한국어용 Transformer 기반 모델 구현
3. **도메인 적응**: 다른 한국어 텍스트 분류 작업에 기법 적용
4. **프로덕션 배포**: 실제 환경용 파이프라인 확장
