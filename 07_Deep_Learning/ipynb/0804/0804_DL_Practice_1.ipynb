{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로이터 뉴스 분류 - Word2Vec 임베딩 활용\n",
    "\n",
    "이 노트북에서는 로이터 뉴스 데이터셋을 사용하여 텍스트 분류 모델을 구축합니다.\n",
    "사전 훈련된 Word2Vec 임베딩을 활용하여 성능을 향상시키는 방법을 학습합니다.\n",
    "\n",
    "## 목차\n",
    "1. [환경 설정 및 라이브러리 임포트](#1-환경-설정-및-라이브러리-임포트)\n",
    "2. [데이터셋 구성](#2-데이터셋-구성)\n",
    "3. [데이터 로딩 및 전처리](#3-데이터-로딩-및-전처리)\n",
    "4. [Word2Vec 임베딩 설정](#4-word2vec-임베딩-설정)\n",
    "5. [모델 구축 및 훈련](#5-모델-구축-및-훈련)\n",
    "6. [결과 평가](#6-결과-평가)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트\n",
    "\n",
    "### 필수 설치 사항\n",
    "- **중요**: KoNLPy와 Korpora는 pip로 설치해야 합니다\n",
    "- Gensim: Word2Vec 모델 로딩을 위해 필요\n",
    "\n",
    "```bash\n",
    "pip install konlpy korpora gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 핵심 라이브러리\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# 자연어 처리\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 기본 라이브러리\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import subprocess\n",
    "import requests\n",
    "\n",
    "# 데이터 처리 및 분석\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 구성\n",
    "\n",
    "로이터 뉴스 데이터를 디렉토리 구조로 정리하는 함수입니다.\n",
    "각 카테고리별로 폴더를 생성하고 텍스트 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(base_dir=\"../../data/reuter\"):\n",
    "    \"\"\"\n",
    "    로이터 뉴스 데이터를 디렉토리 구조로 정리\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): 데이터를 저장할 기본 디렉토리명\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 기존 디렉토리가 있으면 삭제\n",
    "    if os.path.exists(base_dir):\n",
    "        try:\n",
    "            shutil.rmtree(base_dir)\n",
    "            print(f\"기존 데이터셋 디렉토리 삭제: {base_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"디렉토리 삭제 오류 {base_dir}: {e}\")\n",
    "            pass\n",
    "\n",
    "    file_count = 0\n",
    "    \n",
    "    # 훈련 데이터 처리\n",
    "    print(\"훈련 데이터 처리 중...\")\n",
    "    try:\n",
    "        with open(\"../../data/r8-train-all-terms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if '\\t' in line:\n",
    "                    label, sentence = line.strip().split(\"\\t\", 1)\n",
    "                    file_count += 1\n",
    "                    \n",
    "                    # 레이블별 디렉토리 생성\n",
    "                    label_dir = os.path.join(base_dir, label)\n",
    "                    os.makedirs(label_dir, exist_ok=True)\n",
    "                    \n",
    "                    # 파일 저장\n",
    "                    with open(os.path.join(label_dir, f\"{label}_{file_count}.txt\"), \n",
    "                             \"w\", encoding=\"utf-8\") as output_file:\n",
    "                        output_file.write(sentence)\n",
    "                    \n",
    "                    if file_count % 1000 == 0:\n",
    "                        print(f\"처리된 파일 수: {file_count}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"훈련 데이터 파일을 찾을 수 없습니다: ../../data/r8-train-all-terms.txt\")\n",
    "        return\n",
    "    \n",
    "    # 테스트 데이터 처리\n",
    "    print(\"테스트 데이터 처리 중...\")\n",
    "    try:\n",
    "        with open(\"../../data/r8-test-all-terms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if '\\t' in line:\n",
    "                    label, sentence = line.strip().split(\"\\t\", 1)\n",
    "                    file_count += 1\n",
    "                    \n",
    "                    # 레이블별 디렉토리 생성\n",
    "                    label_dir = os.path.join(base_dir, label)\n",
    "                    os.makedirs(label_dir, exist_ok=True)\n",
    "                    \n",
    "                    # 파일 저장\n",
    "                    with open(os.path.join(label_dir, f\"{label}_{file_count}.txt\"), \n",
    "                             \"w\", encoding=\"utf-8\") as output_file:\n",
    "                        output_file.write(sentence)\n",
    "                        \n",
    "                    if file_count % 1000 == 0:\n",
    "                        print(f\"처리된 파일 수: {file_count}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"테스트 데이터 파일을 찾을 수 없습니다: ../../data/r8-test-all-terms.txt\")\n",
    "        return\n",
    "    \n",
    "    print(f\"데이터셋 구성 완료: 총 {file_count}개 파일 생성\")\n",
    "\n",
    "# 데이터셋 생성 (필요시 주석 해제)\n",
    "create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_info(base_dir=\"../../data/reuter\"):\n",
    "    \"\"\"\n",
    "    데이터셋의 구조와 각 카테고리별 파일 수를 확인\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): 데이터가 저장된 디렉토리명\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"디렉토리 '{base_dir}'가 존재하지 않습니다.\")\n",
    "        print(\"먼저 create_dataset() 함수를 실행해주세요.\")\n",
    "        return\n",
    "    \n",
    "    labelnames = os.listdir(base_dir)\n",
    "    print(\"=== 로이터 뉴스 데이터셋 정보 ===\")\n",
    "    print(f\"카테고리 목록: {labelnames}\")\n",
    "    print(\"\\n카테고리별 파일 수:\")\n",
    "    \n",
    "    total_cnt = 0\n",
    "    category_info = []\n",
    "    \n",
    "    for label in labelnames:\n",
    "        label_path = os.path.join(base_dir, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            cnt = len(os.listdir(label_path))\n",
    "            total_cnt += cnt\n",
    "            category_info.append((label, cnt))\n",
    "            print(f\"  {label}: {cnt:,}개\")\n",
    "    \n",
    "    print(f\"\\n전체 파일 수: {total_cnt:,}개\")\n",
    "    print(f\"카테고리 수: {len(labelnames)}개\")\n",
    "    \n",
    "    return category_info\n",
    "\n",
    "# 데이터셋 정보 확인\n",
    "dataset_info = load_file_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예상 데이터셋 구성\n",
    "```\n",
    "acq: 2,292개        # 인수합병\n",
    "crude: 374개        # 원유\n",
    "earn: 3,923개       # 수익\n",
    "grain: 51개         # �곡물\n",
    "interest: 271개     # 금리\n",
    "money-fx: 293개     # 환율\n",
    "ship: 144개         # 운송\n",
    "trade: 326개        # 무역\n",
    "전체: 7,674개\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 로딩 및 전처리\n",
    "\n",
    "TensorFlow의 `text_dataset_from_directory`를 사용하여 데이터를 로딩하고,\n",
    "텍스트 벡터화를 위한 전처리를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 32\n",
    "MAX_TOKENS = 20000\n",
    "SEQUENCE_LENGTH = 600\n",
    "\n",
    "print(\"=== 데이터 로딩 설정 ===\")\n",
    "print(f\"배치 크기: {BATCH_SIZE}\")\n",
    "print(f\"최대 토큰 수: {MAX_TOKENS:,}\")\n",
    "print(f\"시퀀스 길이: {SEQUENCE_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로딩\n",
    "try:\n",
    "    train_ds = keras.utils.text_dataset_from_directory(\n",
    "        \"../../data/reuter\", \n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    val_ds = keras.utils.text_dataset_from_directory(\n",
    "        \"../../data/reuter\", \n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"데이터셋 로딩 성공\")\n",
    "    print(f\"클래스 이름: {train_ds.class_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"데이터셋 로딩 실패: {e}\")\n",
    "    print(\"먼저 create_dataset() 함수를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트만 추출 (라벨 제외)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "print(\"=== 데이터 샘플 확인 ===\")\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):  # 처음 3개 샘플만 확인\n",
    "        print(f\"\\n샘플 {i+1}:\")\n",
    "        print(f\"레이블: {train_ds.class_names[label_batch[i]]}\")\n",
    "        print(f\"텍스트 (처음 100자): {text_batch[i].numpy().decode('utf-8')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 벡터화 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 벡터화 레이어 생성\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "# 텍스트 데이터에 맞춰 벡터화 레이어 적응\n",
    "print(\"텍스트 벡터화 레이어 적응 중...\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# 어휘 사전 생성\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "print(f\"어휘 사전 크기: {len(vocabulary):,}개\")\n",
    "print(f\"상위 10개 단어: {vocabulary[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터를 정수 시퀀스로 변환\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# 성능 최적화를 위한 데이터셋 설정\n",
    "int_train_ds = int_train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "int_val_ds = int_val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"데이터 전처리 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vec 임베딩 설정\n",
    "\n",
    "사전 훈련된 Word2Vec 모델을 로딩하고 임베딩 행렬을 생성합니다.\n",
    "\n",
    "### Word2Vec 모델 다운로드\n",
    "Google News Vector: https://code.google.com/archive/p/word2vec/\n",
    "FastText 벡터: https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델 파일 경로 (실제 경로로 수정 필요)\n",
    "PATH_TO_WORD2VEC_FILE = \"../../data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "print(\"=== Word2Vec 모델 로딩 ===\")\n",
    "print(f\"모델 파일 경로: {PATH_TO_WORD2VEC_FILE}\")\n",
    "\n",
    "# Word2Vec 모델 로딩\n",
    "try:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(\n",
    "        PATH_TO_WORD2VEC_FILE, \n",
    "        binary=True,\n",
    "        limit=500000  # 메모리 사용량 제한\n",
    "    )\n",
    "    print(f\"Word2Vec 모델 로딩 성공\")\n",
    "    print(f\"벡터 차원: {word2vec_model.vector_size}\")\n",
    "    print(f\"어휘 사전 크기: {len(word2vec_model.key_to_index):,}개\")\n",
    "    \n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Word2Vec 모델 파일을 찾을 수 없습니다.\")\n",
    "    print(\"다음 중 하나를 다운로드하세요:\")\n",
    "    print(\"1. GoogleNews-vectors-negative300.bin.gz (구글 검색)\")\n",
    "    print(\"2. 압축 해제 후 경로를 올바르게 설정\")\n",
    "    \n",
    "    # 대안: 랜덤 임베딩 사용\n",
    "    print(\"\\n대안으로 랜덤 임베딩을 사용합니다.\")\n",
    "    embedding_dim = 300\n",
    "    word2vec_model = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Word2Vec 모델 로딩 오류: {e}\")\n",
    "    embedding_dim = 300\n",
    "    word2vec_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 행렬 초기화\n",
    "embedding_matrix = np.zeros((MAX_TOKENS, embedding_dim))\n",
    "\n",
    "if word2vec_model is not None:\n",
    "    print(\"\\n=== 임베딩 행렬 생성 ===\")\n",
    "    print(\"Word2Vec 벡터로 임베딩 행렬 채우는 중...\")\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i < MAX_TOKENS:\n",
    "            try:\n",
    "                # Word2Vec에서 벡터 가져오기\n",
    "                embedding_vector = word2vec_model[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                hits += 1\n",
    "            except KeyError:\n",
    "                # 단어가 Word2Vec 어휘에 없는 경우\n",
    "                misses += 1\n",
    "                # 랜덤 벡터로 초기화 (선택사항)\n",
    "                embedding_matrix[i] = np.random.normal(\n",
    "                    scale=0.1, size=(embedding_dim,)\n",
    "                )\n",
    "    \n",
    "    print(f\"✅ 임베딩 벡터 적용: {hits:,}개 단어\")\n",
    "    print(f\"❌ 누락된 단어: {misses:,}개\")\n",
    "    print(f\"📊 적용률: {hits/(hits+misses)*100:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n=== 랜덤 임베딩 행렬 생성 ===\")\n",
    "    # Word2Vec 모델이 없는 경우 랜덤 초기화\n",
    "    embedding_matrix = np.random.normal(\n",
    "        scale=0.1, size=(MAX_TOKENS, embedding_dim)\n",
    "    )\n",
    "    print(f\"랜덤 임베딩 행렬 생성 완료: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 레이어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 레이어 생성\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,  # 사전 훈련된 임베딩 고정\n",
    "    mask_zero=True,   # 패딩 토큰 마스킹\n",
    "    name=\"embedding_layer\"\n",
    ")\n",
    "\n",
    "print(f\"임베딩 레이어 생성 완료\")\n",
    "print(f\"  - 입력 차원: {MAX_TOKENS:,}\")\n",
    "print(f\"  - 출력 차원: {embedding_dim}\")\n",
    "print(f\"  - 훈련 가능: {embedding_layer.trainable}\")\n",
    "print(f\"  - 마스킹: {embedding_layer.mask_zero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 구축 및 훈련\n",
    "\n",
    "Bidirectional LSTM을 사용한 텍스트 분류 모델을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축\n",
    "def build_model(num_classes):\n",
    "    \"\"\"\n",
    "    Word2Vec 임베딩을 사용한 텍스트 분류 모델 구축\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): 분류할 클래스 수\n",
    "    \n",
    "    Returns:\n",
    "        keras.Model: 컴파일된 모델\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"text_input\")\n",
    "    \n",
    "    # 임베딩 레이어\n",
    "    embedded = embedding_layer(inputs)\n",
    "    \n",
    "    # Bidirectional LSTM\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        name=\"bidirectional_lstm\"\n",
    "    )(embedded)\n",
    "    \n",
    "    # 드롭아웃 정규화\n",
    "    x = layers.Dropout(0.5, name=\"dropout\")(x)\n",
    "    \n",
    "    # 출력 레이어\n",
    "    outputs = layers.Dense(\n",
    "        num_classes, \n",
    "        activation=\"softmax\", \n",
    "        name=\"classification_output\"\n",
    "    )(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name=\"reuters_classifier\")\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 클래스 수 확인\n",
    "num_classes = len(train_ds.class_names)\n",
    "print(f\"분류할 클래스 수: {num_classes}개\")\n",
    "print(f\"클래스 목록: {train_ds.class_names}\")\n",
    "\n",
    "# 모델 생성\n",
    "model = build_model(num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 설정\n",
    "EPOCHS = 10\n",
    "MODEL_CHECKPOINT_PATH = \"word2vec_embeddings_sequence_model.keras\"\n",
    "\n",
    "# 콜백 설정\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        MODEL_CHECKPOINT_PATH,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=== 모델 훈련 시작 ===\")\n",
    "print(f\"에포크 수: {EPOCHS}\")\n",
    "print(f\"모델 저장 경로: {MODEL_CHECKPOINT_PATH}\")\n",
    "print(f\"사용 임베딩: {'Word2Vec' if word2vec_model else '랜덤 초기화'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ 모델 훈련 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 평가\n",
    "\n",
    "훈련된 모델의 성능을 평가하고 결과를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 성능 모델 로딩\n",
    "print(\"=== 모델 평가 ===\")\n",
    "try:\n",
    "    best_model = keras.models.load_model(MODEL_CHECKPOINT_PATH)\n",
    "    print(f\"최고 성능 모델 로딩 완료: {MODEL_CHECKPOINT_PATH}\")\n",
    "except:\n",
    "    best_model = model\n",
    "    print(\"저장된 모델을 찾을 수 없어 현재 모델을 사용합니다.\")\n",
    "\n",
    "# 검증 데이터로 평가\n",
    "val_loss, val_accuracy = best_model.evaluate(int_val_ds, verbose=0)\n",
    "print(f\"\\n📊 최종 성능:\")\n",
    "print(f\"  - 검증 손실: {val_loss:.4f}\")\n",
    "print(f\"  - 검증 정확도: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 훈련 히스토리 시각화\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    훈련 과정을 시각화\n",
    "    \n",
    "    Args:\n",
    "        history: 훈련 히스토리 객체\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # 정확도 그래프\n",
    "    ax1.plot(history.history['accuracy'], label='훈련 정확도', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='검증 정확도', marker='s')\n",
    "    ax1.set_title('모델 정확도')\n",
    "    ax1.set_xlabel('에포크')\n",
    "    ax1.set_ylabel('정확도')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 손실 그래프\n",
    "    ax2.plot(history.history['loss'], label='훈련 손실', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='검증 손실', marker='s')\n",
    "    ax2.set_title('모델 손실')\n",
    "    ax2.set_xlabel('에포크')\n",
    "    ax2.set_ylabel('손실')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 최고 성능 정보\n",
    "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    print(f\"\\n🏆 최고 성능:\")\n",
    "    print(f\"  - 에포크: {best_epoch + 1}\")\n",
    "    print(f\"  - 검증 정확도: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "\n",
    "# 훈련 과정 시각화\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 함수\n",
    "def predict_news_category(text, model, vectorizer, class_names):\n",
    "    \"\"\"\n",
    "    텍스트의 뉴스 카테고리를 예측\n",
    "    \n",
    "    Args:\n",
    "        text (str): 예측할 텍스트\n",
    "        model: 훈련된 모델\n",
    "        vectorizer: 텍스트 벡터화 레이어\n",
    "        class_names: 클래스 이름 목록\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (예측 카테고리, 신뢰도, 모든 확률)\n",
    "    \"\"\"\n",
    "    # 텍스트 전처리 및 벡터화\n",
    "    text_vector = vectorizer([text])\n",
    "    \n",
    "    # 예측\n",
    "    predictions = model.predict(text_vector, verbose=0)\n",
    "    predicted_class_idx = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class_idx]\n",
    "    \n",
    "    return class_names[predicted_class_idx], confidence, predictions[0]\n",
    "\n",
    "# 테스트 예시\n",
    "test_texts = [\n",
    "    \"The company reported strong quarterly earnings with revenue up 15%\",\n",
    "    \"Oil prices rose sharply following geopolitical tensions in the Middle East\",\n",
    "    \"The Federal Reserve announced an interest rate hike of 0.25%\",\n",
    "    \"Wheat futures declined due to favorable weather conditions\"\n",
    "]\n",
    "\n",
    "print(\"=== 예측 결과 ===\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    category, confidence, all_probs = predict_news_category(\n",
    "        text, best_model, text_vectorization, train_ds.class_names\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{i}. 텍스트: {text[:60]}...\")\n",
    "    print(f\"   예측 카테고리: {category}\")\n",
    "    print(f\"   신뢰도: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "    \n",
    "    # 상위 3개 확률 표시\n",
    "    top3_idx = np.argsort(all_probs)[-3:][::-1]\n",
    "    print(f\"   상위 3개 확률:\")\n",
    "    for idx in top3_idx:\n",
    "        print(f\"     {train_ds.class_names[idx]}: {all_probs[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론 및 요약\n",
    "\n",
    "### 주요 성과\n",
    "1. **데이터 전처리**: 로이터 뉴스 데이터를 효율적으로 구조화\n",
    "2. **Word2Vec 활용**: 사전 훈련된 임베딩으로 성능 향상\n",
    "3. **모델 구조**: Bidirectional LSTM으로 양방향 컨텍스트 학습\n",
    "4. **정규화**: 드롭아웃과 조기 종료로 과적합 방지\n",
    "\n",
    "### 개선 방향\n",
    "1. **하이퍼파라미터 튜닝**: 배치 크기, 학습률, 히든 유닛 수 최적화\n",
    "2. **모델 구조 실험**: Transformer, CNN 등 다른 아키텍처 시도\n",
    "3. **앙상블**: 여러 모델의 예측을 결합하여 성능 향상\n",
    "4. **도메인 특화**: 금융/뉴스 도메인 특화 임베딩 활용\n",
    "\n",
    "### 학습 내용\n",
    "- 텍스트 분류 파이프라인 구축\n",
    "- 사전 훈련된 임베딩 활용법\n",
    "- 시퀀스 모델링과 정규화 기법\n",
    "- 모델 평가 및 해석 방법"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
