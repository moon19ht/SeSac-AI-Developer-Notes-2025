{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¡œì´í„° ë‰´ìŠ¤ ë¶„ë¥˜ - Word2Vec ì„ë² ë”© í™œìš©\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë¡œì´í„° ë‰´ìŠ¤ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "ì‚¬ì „ í›ˆë ¨ëœ Word2Vec ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸](#1-í™˜ê²½-ì„¤ì •-ë°-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì„í¬íŠ¸)\n",
    "2. [ë°ì´í„°ì…‹ êµ¬ì„±](#2-ë°ì´í„°ì…‹-êµ¬ì„±)\n",
    "3. [ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬](#3-ë°ì´í„°-ë¡œë”©-ë°-ì „ì²˜ë¦¬)\n",
    "4. [Word2Vec ì„ë² ë”© ì„¤ì •](#4-word2vec-ì„ë² ë”©-ì„¤ì •)\n",
    "5. [ëª¨ë¸ êµ¬ì¶• ë° í›ˆë ¨](#5-ëª¨ë¸-êµ¬ì¶•-ë°-í›ˆë ¨)\n",
    "6. [ê²°ê³¼ í‰ê°€](#6-ê²°ê³¼-í‰ê°€)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "### í•„ìˆ˜ ì„¤ì¹˜ ì‚¬í•­\n",
    "- **ì¤‘ìš”**: KoNLPyì™€ KorporaëŠ” pipë¡œ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤\n",
    "- Gensim: Word2Vec ëª¨ë¸ ë¡œë”©ì„ ìœ„í•´ í•„ìš”\n",
    "\n",
    "```bash\n",
    "pip install konlpy korpora gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# ìì—°ì–´ ì²˜ë¦¬\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import subprocess\n",
    "import requests\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "\n",
    "ë¡œì´í„° ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ í´ë”ë¥¼ ìƒì„±í•˜ê³  í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(base_dir=\"../../data/reuter\"):\n",
    "    \"\"\"\n",
    "    ë¡œì´í„° ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ì •ë¦¬\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): ë°ì´í„°ë¥¼ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬ëª…\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ\n",
    "    if os.path.exists(base_dir):\n",
    "        try:\n",
    "            shutil.rmtree(base_dir)\n",
    "            print(f\"ê¸°ì¡´ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ì‚­ì œ: {base_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"ë””ë ‰í† ë¦¬ ì‚­ì œ ì˜¤ë¥˜ {base_dir}: {e}\")\n",
    "            pass\n",
    "\n",
    "    file_count = 0\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬\n",
    "    print(\"í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬ ì¤‘...\")\n",
    "    try:\n",
    "        with open(\"../../data/r8-train-all-terms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if '\\t' in line:\n",
    "                    label, sentence = line.strip().split(\"\\t\", 1)\n",
    "                    file_count += 1\n",
    "                    \n",
    "                    # ë ˆì´ë¸”ë³„ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "                    label_dir = os.path.join(base_dir, label)\n",
    "                    os.makedirs(label_dir, exist_ok=True)\n",
    "                    \n",
    "                    # íŒŒì¼ ì €ì¥\n",
    "                    with open(os.path.join(label_dir, f\"{label}_{file_count}.txt\"), \n",
    "                             \"w\", encoding=\"utf-8\") as output_file:\n",
    "                        output_file.write(sentence)\n",
    "                    \n",
    "                    if file_count % 1000 == 0:\n",
    "                        print(f\"ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {file_count}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"í›ˆë ¨ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ../../data/r8-train-all-terms.txt\")\n",
    "        return\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬\n",
    "    print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ì¤‘...\")\n",
    "    try:\n",
    "        with open(\"../../data/r8-test-all-terms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if '\\t' in line:\n",
    "                    label, sentence = line.strip().split(\"\\t\", 1)\n",
    "                    file_count += 1\n",
    "                    \n",
    "                    # ë ˆì´ë¸”ë³„ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "                    label_dir = os.path.join(base_dir, label)\n",
    "                    os.makedirs(label_dir, exist_ok=True)\n",
    "                    \n",
    "                    # íŒŒì¼ ì €ì¥\n",
    "                    with open(os.path.join(label_dir, f\"{label}_{file_count}.txt\"), \n",
    "                             \"w\", encoding=\"utf-8\") as output_file:\n",
    "                        output_file.write(sentence)\n",
    "                        \n",
    "                    if file_count % 1000 == 0:\n",
    "                        print(f\"ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {file_count}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ../../data/r8-test-all-terms.txt\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ: ì´ {file_count}ê°œ íŒŒì¼ ìƒì„±\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„± (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
    "create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_info(base_dir=\"../../data/reuter\"):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ì˜ êµ¬ì¡°ì™€ ê° ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ìˆ˜ë¥¼ í™•ì¸\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ëª…\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"ë””ë ‰í† ë¦¬ '{base_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ë¨¼ì € create_dataset() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    labelnames = os.listdir(base_dir)\n",
    "    print(\"=== ë¡œì´í„° ë‰´ìŠ¤ ë°ì´í„°ì…‹ ì •ë³´ ===\")\n",
    "    print(f\"ì¹´í…Œê³ ë¦¬ ëª©ë¡: {labelnames}\")\n",
    "    print(\"\\nì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ìˆ˜:\")\n",
    "    \n",
    "    total_cnt = 0\n",
    "    category_info = []\n",
    "    \n",
    "    for label in labelnames:\n",
    "        label_path = os.path.join(base_dir, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            cnt = len(os.listdir(label_path))\n",
    "            total_cnt += cnt\n",
    "            category_info.append((label, cnt))\n",
    "            print(f\"  {label}: {cnt:,}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nì „ì²´ íŒŒì¼ ìˆ˜: {total_cnt:,}ê°œ\")\n",
    "    print(f\"ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(labelnames)}ê°œ\")\n",
    "    \n",
    "    return category_info\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸\n",
    "dataset_info = load_file_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜ˆìƒ ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "```\n",
    "acq: 2,292ê°œ        # ì¸ìˆ˜í•©ë³‘\n",
    "crude: 374ê°œ        # ì›ìœ \n",
    "earn: 3,923ê°œ       # ìˆ˜ìµ\n",
    "grain: 51ê°œ         # ï¿½ê³¡ë¬¼\n",
    "interest: 271ê°œ     # ê¸ˆë¦¬\n",
    "money-fx: 293ê°œ     # í™˜ìœ¨\n",
    "ship: 144ê°œ         # ìš´ì†¡\n",
    "trade: 326ê°œ        # ë¬´ì—­\n",
    "ì „ì²´: 7,674ê°œ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\n",
    "\n",
    "TensorFlowì˜ `text_dataset_from_directory`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³ ,\n",
    "í…ìŠ¤íŠ¸ ë²¡í„°í™”ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 32\n",
    "MAX_TOKENS = 20000\n",
    "SEQUENCE_LENGTH = 600\n",
    "\n",
    "print(\"=== ë°ì´í„° ë¡œë”© ì„¤ì • ===\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\"ìµœëŒ€ í† í° ìˆ˜: {MAX_TOKENS:,}\")\n",
    "print(f\"ì‹œí€€ìŠ¤ ê¸¸ì´: {SEQUENCE_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¡œë”©\n",
    "try:\n",
    "    train_ds = keras.utils.text_dataset_from_directory(\n",
    "        \"../../data/reuter\", \n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    val_ds = keras.utils.text_dataset_from_directory(\n",
    "        \"../../data/reuter\", \n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"ë°ì´í„°ì…‹ ë¡œë”© ì„±ê³µ\")\n",
    "    print(f\"í´ë˜ìŠ¤ ì´ë¦„: {train_ds.class_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ë¨¼ì € create_dataset() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ë¼ë²¨ ì œì™¸)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "print(\"=== ë°ì´í„° ìƒ˜í”Œ í™•ì¸ ===\")\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):  # ì²˜ìŒ 3ê°œ ìƒ˜í”Œë§Œ í™•ì¸\n",
    "        print(f\"\\nìƒ˜í”Œ {i+1}:\")\n",
    "        print(f\"ë ˆì´ë¸”: {train_ds.class_names[label_batch[i]]}\")\n",
    "        print(f\"í…ìŠ¤íŠ¸ (ì²˜ìŒ 100ì): {text_batch[i].numpy().decode('utf-8')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ë²¡í„°í™” ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë²¡í„°í™” ë ˆì´ì–´ ìƒì„±\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ì— ë§ì¶° ë²¡í„°í™” ë ˆì´ì–´ ì ì‘\n",
    "print(\"í…ìŠ¤íŠ¸ ë²¡í„°í™” ë ˆì´ì–´ ì ì‘ ì¤‘...\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# ì–´íœ˜ ì‚¬ì „ ìƒì„±\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "print(f\"ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(vocabulary):,}ê°œ\")\n",
    "print(f\"ìƒìœ„ 10ê°œ ë‹¨ì–´: {vocabulary[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ë°ì´í„°ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ì„¤ì •\n",
    "int_train_ds = int_train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "int_val_ds = int_val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vec ì„ë² ë”© ì„¤ì •\n",
    "\n",
    "ì‚¬ì „ í›ˆë ¨ëœ Word2Vec ëª¨ë¸ì„ ë¡œë”©í•˜ê³  ì„ë² ë”© í–‰ë ¬ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Word2Vec ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "Google News Vector: https://code.google.com/archive/p/word2vec/\n",
    "FastText ë²¡í„°: https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
    "PATH_TO_WORD2VEC_FILE = \"../../data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "print(\"=== Word2Vec ëª¨ë¸ ë¡œë”© ===\")\n",
    "print(f\"ëª¨ë¸ íŒŒì¼ ê²½ë¡œ: {PATH_TO_WORD2VEC_FILE}\")\n",
    "\n",
    "# Word2Vec ëª¨ë¸ ë¡œë”©\n",
    "try:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(\n",
    "        PATH_TO_WORD2VEC_FILE, \n",
    "        binary=True,\n",
    "        limit=500000  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ\n",
    "    )\n",
    "    print(f\"Word2Vec ëª¨ë¸ ë¡œë”© ì„±ê³µ\")\n",
    "    print(f\"ë²¡í„° ì°¨ì›: {word2vec_model.vector_size}\")\n",
    "    print(f\"ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(word2vec_model.key_to_index):,}ê°œ\")\n",
    "    \n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Word2Vec ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:\")\n",
    "    print(\"1. GoogleNews-vectors-negative300.bin.gz (êµ¬ê¸€ ê²€ìƒ‰)\")\n",
    "    print(\"2. ì••ì¶• í•´ì œ í›„ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •\")\n",
    "    \n",
    "    # ëŒ€ì•ˆ: ëœë¤ ì„ë² ë”© ì‚¬ìš©\n",
    "    print(\"\\nëŒ€ì•ˆìœ¼ë¡œ ëœë¤ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    embedding_dim = 300\n",
    "    word2vec_model = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Word2Vec ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "    embedding_dim = 300\n",
    "    word2vec_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì„ë² ë”© í–‰ë ¬ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© í–‰ë ¬ ì´ˆê¸°í™”\n",
    "embedding_matrix = np.zeros((MAX_TOKENS, embedding_dim))\n",
    "\n",
    "if word2vec_model is not None:\n",
    "    print(\"\\n=== ì„ë² ë”© í–‰ë ¬ ìƒì„± ===\")\n",
    "    print(\"Word2Vec ë²¡í„°ë¡œ ì„ë² ë”© í–‰ë ¬ ì±„ìš°ëŠ” ì¤‘...\")\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i < MAX_TOKENS:\n",
    "            try:\n",
    "                # Word2Vecì—ì„œ ë²¡í„° ê°€ì ¸ì˜¤ê¸°\n",
    "                embedding_vector = word2vec_model[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                hits += 1\n",
    "            except KeyError:\n",
    "                # ë‹¨ì–´ê°€ Word2Vec ì–´íœ˜ì— ì—†ëŠ” ê²½ìš°\n",
    "                misses += 1\n",
    "                # ëœë¤ ë²¡í„°ë¡œ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)\n",
    "                embedding_matrix[i] = np.random.normal(\n",
    "                    scale=0.1, size=(embedding_dim,)\n",
    "                )\n",
    "    \n",
    "    print(f\"âœ… ì„ë² ë”© ë²¡í„° ì ìš©: {hits:,}ê°œ ë‹¨ì–´\")\n",
    "    print(f\"âŒ ëˆ„ë½ëœ ë‹¨ì–´: {misses:,}ê°œ\")\n",
    "    print(f\"ğŸ“Š ì ìš©ë¥ : {hits/(hits+misses)*100:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n=== ëœë¤ ì„ë² ë”© í–‰ë ¬ ìƒì„± ===\")\n",
    "    # Word2Vec ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ëœë¤ ì´ˆê¸°í™”\n",
    "    embedding_matrix = np.random.normal(\n",
    "        scale=0.1, size=(MAX_TOKENS, embedding_dim)\n",
    "    )\n",
    "    print(f\"ëœë¤ ì„ë² ë”© í–‰ë ¬ ìƒì„± ì™„ë£Œ: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì„ë² ë”© ë ˆì´ì–´ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ë ˆì´ì–´ ìƒì„±\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,  # ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© ê³ ì •\n",
    "    mask_zero=True,   # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹\n",
    "    name=\"embedding_layer\"\n",
    ")\n",
    "\n",
    "print(f\"ì„ë² ë”© ë ˆì´ì–´ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  - ì…ë ¥ ì°¨ì›: {MAX_TOKENS:,}\")\n",
    "print(f\"  - ì¶œë ¥ ì°¨ì›: {embedding_dim}\")\n",
    "print(f\"  - í›ˆë ¨ ê°€ëŠ¥: {embedding_layer.trainable}\")\n",
    "print(f\"  - ë§ˆìŠ¤í‚¹: {embedding_layer.mask_zero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ êµ¬ì¶• ë° í›ˆë ¨\n",
    "\n",
    "Bidirectional LSTMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ êµ¬ì¶•\n",
    "def build_model(num_classes):\n",
    "    \"\"\"\n",
    "    Word2Vec ì„ë² ë”©ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶•\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): ë¶„ë¥˜í•  í´ë˜ìŠ¤ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        keras.Model: ì»´íŒŒì¼ëœ ëª¨ë¸\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"text_input\")\n",
    "    \n",
    "    # ì„ë² ë”© ë ˆì´ì–´\n",
    "    embedded = embedding_layer(inputs)\n",
    "    \n",
    "    # Bidirectional LSTM\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        name=\"bidirectional_lstm\"\n",
    "    )(embedded)\n",
    "    \n",
    "    # ë“œë¡­ì•„ì›ƒ ì •ê·œí™”\n",
    "    x = layers.Dropout(0.5, name=\"dropout\")(x)\n",
    "    \n",
    "    # ì¶œë ¥ ë ˆì´ì–´\n",
    "    outputs = layers.Dense(\n",
    "        num_classes, \n",
    "        activation=\"softmax\", \n",
    "        name=\"classification_output\"\n",
    "    )(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name=\"reuters_classifier\")\n",
    "    \n",
    "    # ëª¨ë¸ ì»´íŒŒì¼\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# í´ë˜ìŠ¤ ìˆ˜ í™•ì¸\n",
    "num_classes = len(train_ds.class_names)\n",
    "print(f\"ë¶„ë¥˜í•  í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ\")\n",
    "print(f\"í´ë˜ìŠ¤ ëª©ë¡: {train_ds.class_names}\")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = build_model(num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í›ˆë ¨ ì„¤ì • ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ì„¤ì •\n",
    "EPOCHS = 10\n",
    "MODEL_CHECKPOINT_PATH = \"word2vec_embeddings_sequence_model.keras\"\n",
    "\n",
    "# ì½œë°± ì„¤ì •\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        MODEL_CHECKPOINT_PATH,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=== ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===\")\n",
    "print(f\"ì—í¬í¬ ìˆ˜: {EPOCHS}\")\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_CHECKPOINT_PATH}\")\n",
    "print(f\"ì‚¬ìš© ì„ë² ë”©: {'Word2Vec' if word2vec_model else 'ëœë¤ ì´ˆê¸°í™”'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í›ˆë ¨\n",
    "history = model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê²°ê³¼ í‰ê°€\n",
    "\n",
    "í›ˆë ¨ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë”©\n",
    "print(\"=== ëª¨ë¸ í‰ê°€ ===\")\n",
    "try:\n",
    "    best_model = keras.models.load_model(MODEL_CHECKPOINT_PATH)\n",
    "    print(f\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {MODEL_CHECKPOINT_PATH}\")\n",
    "except:\n",
    "    best_model = model\n",
    "    print(\"ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "val_loss, val_accuracy = best_model.evaluate(int_val_ds, verbose=0)\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥:\")\n",
    "print(f\"  - ê²€ì¦ ì†ì‹¤: {val_loss:.4f}\")\n",
    "print(f\"  - ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í›ˆë ¨ ê³¼ì • ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì‹œê°í™”\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    í›ˆë ¨ ê³¼ì •ì„ ì‹œê°í™”\n",
    "    \n",
    "    Args:\n",
    "        history: í›ˆë ¨ íˆìŠ¤í† ë¦¬ ê°ì²´\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # ì •í™•ë„ ê·¸ë˜í”„\n",
    "    ax1.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„', marker='s')\n",
    "    ax1.set_title('ëª¨ë¸ ì •í™•ë„')\n",
    "    ax1.set_xlabel('ì—í¬í¬')\n",
    "    ax1.set_ylabel('ì •í™•ë„')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ì†ì‹¤ ê·¸ë˜í”„\n",
    "    ax2.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤', marker='s')\n",
    "    ax2.set_title('ëª¨ë¸ ì†ì‹¤')\n",
    "    ax2.set_xlabel('ì—í¬í¬')\n",
    "    ax2.set_ylabel('ì†ì‹¤')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ì •ë³´\n",
    "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥:\")\n",
    "    print(f\"  - ì—í¬í¬: {best_epoch + 1}\")\n",
    "    print(f\"  - ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "\n",
    "# í›ˆë ¨ ê³¼ì • ì‹œê°í™”\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜ˆì¸¡ ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_news_category(text, model, vectorizer, class_names):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì˜ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡\n",
    "    \n",
    "    Args:\n",
    "        text (str): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸\n",
    "        model: í›ˆë ¨ëœ ëª¨ë¸\n",
    "        vectorizer: í…ìŠ¤íŠ¸ ë²¡í„°í™” ë ˆì´ì–´\n",
    "        class_names: í´ë˜ìŠ¤ ì´ë¦„ ëª©ë¡\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬, ì‹ ë¢°ë„, ëª¨ë“  í™•ë¥ )\n",
    "    \"\"\"\n",
    "    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë²¡í„°í™”\n",
    "    text_vector = vectorizer([text])\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    predictions = model.predict(text_vector, verbose=0)\n",
    "    predicted_class_idx = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class_idx]\n",
    "    \n",
    "    return class_names[predicted_class_idx], confidence, predictions[0]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ\n",
    "test_texts = [\n",
    "    \"The company reported strong quarterly earnings with revenue up 15%\",\n",
    "    \"Oil prices rose sharply following geopolitical tensions in the Middle East\",\n",
    "    \"The Federal Reserve announced an interest rate hike of 0.25%\",\n",
    "    \"Wheat futures declined due to favorable weather conditions\"\n",
    "]\n",
    "\n",
    "print(\"=== ì˜ˆì¸¡ ê²°ê³¼ ===\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    category, confidence, all_probs = predict_news_category(\n",
    "        text, best_model, text_vectorization, train_ds.class_names\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{i}. í…ìŠ¤íŠ¸: {text[:60]}...\")\n",
    "    print(f\"   ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬: {category}\")\n",
    "    print(f\"   ì‹ ë¢°ë„: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ í™•ë¥  í‘œì‹œ\n",
    "    top3_idx = np.argsort(all_probs)[-3:][::-1]\n",
    "    print(f\"   ìƒìœ„ 3ê°œ í™•ë¥ :\")\n",
    "    for idx in top3_idx:\n",
    "        print(f\"     {train_ds.class_names[idx]}: {all_probs[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê²°ë¡  ë° ìš”ì•½\n",
    "\n",
    "### ì£¼ìš” ì„±ê³¼\n",
    "1. **ë°ì´í„° ì „ì²˜ë¦¬**: ë¡œì´í„° ë‰´ìŠ¤ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì¡°í™”\n",
    "2. **Word2Vec í™œìš©**: ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
    "3. **ëª¨ë¸ êµ¬ì¡°**: Bidirectional LSTMìœ¼ë¡œ ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ\n",
    "4. **ì •ê·œí™”**: ë“œë¡­ì•„ì›ƒê³¼ ì¡°ê¸° ì¢…ë£Œë¡œ ê³¼ì í•© ë°©ì§€\n",
    "\n",
    "### ê°œì„  ë°©í–¥\n",
    "1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: ë°°ì¹˜ í¬ê¸°, í•™ìŠµë¥ , íˆë“  ìœ ë‹› ìˆ˜ ìµœì í™”\n",
    "2. **ëª¨ë¸ êµ¬ì¡° ì‹¤í—˜**: Transformer, CNN ë“± ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ ì‹œë„\n",
    "3. **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ\n",
    "4. **ë„ë©”ì¸ íŠ¹í™”**: ê¸ˆìœµ/ë‰´ìŠ¤ ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”© í™œìš©\n",
    "\n",
    "### í•™ìŠµ ë‚´ìš©\n",
    "- í…ìŠ¤íŠ¸ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
    "- ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© í™œìš©ë²•\n",
    "- ì‹œí€€ìŠ¤ ëª¨ë¸ë§ê³¼ ì •ê·œí™” ê¸°ë²•\n",
    "- ëª¨ë¸ í‰ê°€ ë° í•´ì„ ë°©ë²•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
