{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 자연어 처리: 텍스트 벡터화 (Text Vectorization)\n",
    "\n",
    "## 개요\n",
    "이 노트북에서는 KoNLPy와 TensorFlow의 TextVectorization을 결합하여 한글 텍스트를 딥러닝 모델에서 사용할 수 있는 숫자 형태로 변환하는 과정을 학습합니다.\n",
    "\n",
    "### 주요 학습 내용:\n",
    "1. 텍스트 전처리 및 정제\n",
    "2. 한글 형태소 분석과 토큰화\n",
    "3. TensorFlow TextVectorization 활용\n",
    "4. 어휘 사전(Vocabulary) 구축\n",
    "5. 텍스트의 숫자 벡터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 버전: 2.15.1\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 import\n",
    "from konlpy.tag import Okt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 샘플 데이터 준비\n",
    "\n",
    "자연어 처리 학습을 위한 한글 텍스트 데이터를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 원본 텍스트 데이터 ===\n",
      "1. 나는 AI 챗봇을 사용합니다.\n",
      "2. 자연어 처리는 어렵습니다\n",
      "3. 챗봇과 대화하는것이 즐겁습니다\n",
      "4. AI로 인해 사람들이 일자리가 사라지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학습용 텍스트 데이터 준비\n",
    "sample_texts = [\n",
    "    \"나는 AI 챗봇을 사용합니다.\",\n",
    "    \"자연어 처리는 어렵습니다\",\n",
    "    \"챗봇과 대화하는것이 즐겁습니다\",\n",
    "    \"AI로 인해 사람들이 일자리가 사라지고 있습니다.\"\n",
    "]\n",
    "\n",
    "print(\"=== 원본 텍스트 데이터 ===\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 전처리 함수 정의\n",
    "\n",
    "### 3.1 기본 텍스트 정제 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 텍스트 정제 결과 ===\n",
      "1. 원본: 나는 AI 챗봇을 사용합니다.\n",
      "   정제: 나는 ai 챗봇을 사용합니다\n",
      "\n",
      "2. 원본: 자연어 처리는 어렵습니다\n",
      "   정제: 자연어 처리는 어렵습니다\n",
      "\n",
      "3. 원본: 챗봇과 대화하는것이 즐겁습니다\n",
      "   정제: 챗봇과 대화하는것이 즐겁습니다\n",
      "\n",
      "4. 원본: AI로 인해 사람들이 일자리가 사라지고 있습니다.\n",
      "   정제: ai로 인해 사람들이 일자리가 사라지고 있습니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    텍스트 정제 함수\n",
    "    - 대문자를 소문자로 변환\n",
    "    - 한글, 영문, 숫자, 공백만 유지하고 나머지 특수문자 제거\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        str: 정제된 텍스트\n",
    "    \"\"\"\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 정규식을 사용하여 필요한 문자만 유지\n",
    "    # [^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\s]: 한글, 영문, 숫자, 공백 외 모든 문자 제거\n",
    "    text = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 텍스트 정제 결과 확인\n",
    "print(\"=== 텍스트 정제 결과 ===\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    cleaned = clean_text(text)\n",
    "    print(f\"{i}. 원본: {text}\")\n",
    "    print(f\"   정제: {cleaned}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 한글 형태소 분석 및 토큰화\n",
    "\n",
    "### 4.1 형태소 분석기 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 형태소 분석 결과 ===\n",
      "1. 텍스트: 나는 ai 챗봇을 사용합니다\n",
      "   형태소: ['나', '는', 'ai', '챗봇', '을', '사용', '합니다']\n",
      "\n",
      "2. 텍스트: 자연어 처리는 어렵습니다\n",
      "   형태소: ['자연어', '처리', '는', '어렵습니다']\n",
      "\n",
      "3. 텍스트: 챗봇과 대화하는것이 즐겁습니다\n",
      "   형태소: ['챗봇', '과', '대화', '하는것이', '즐겁습니다']\n",
      "\n",
      "4. 텍스트: ai로 인해 사람들이 일자리가 사라지고 있습니다\n",
      "   형태소: ['ai', '로', '인해', '사람', '들', '이', '일자리', '가', '사라지고', '있습니다']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KoNLPy Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "print(\"=== 형태소 분석 결과 ===\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    cleaned = clean_text(text)\n",
    "    morphs = okt.morphs(cleaned)\n",
    "    print(f\"{i}. 텍스트: {cleaned}\")\n",
    "    print(f\"   형태소: {morphs}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 커스텀 표준화 함수 정의\n",
    "\n",
    "TextVectorization에서 사용할 전처리 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 커스텀 표준화 함수 결과 ===\n",
      "1. 원본: 나는 AI 챗봇을 사용합니다.\n",
      "   표준화: 나 는 ai 챗봇 을 사용 합니다\n",
      "\n",
      "2. 원본: 자연어 처리는 어렵습니다\n",
      "   표준화: 자연어 처리 는 어렵습니다\n",
      "\n",
      "3. 원본: 챗봇과 대화하는것이 즐겁습니다\n",
      "   표준화: 챗봇 과 대화 하는것이 즐겁습니다\n",
      "\n",
      "4. 원본: AI로 인해 사람들이 일자리가 사라지고 있습니다.\n",
      "   표준화: ai 로 인해 사람 들 이 일자리 가 사라지고 있습니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization_fn(text):\n",
    "    \"\"\"\n",
    "    TextVectorization용 커스텀 표준화 함수\n",
    "    1. 텍스트 정제\n",
    "    2. 형태소 분석\n",
    "    3. 토큰들을 공백으로 구분하여 재결합\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        str: 형태소 분석된 토큰들이 공백으로 구분된 문자열\n",
    "    \"\"\"\n",
    "    # 1단계: 텍스트 정제\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # 2단계: 형태소 분석하여 토큰화\n",
    "    tokens = okt.morphs(cleaned_text)\n",
    "    \n",
    "    # 3단계: 토큰들을 공백으로 구분하여 재결합\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 커스텀 표준화 함수 테스트\n",
    "print(\"=== 커스텀 표준화 함수 결과 ===\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    standardized = custom_standardization_fn(text)\n",
    "    print(f\"{i}. 원본: {text}\")\n",
    "    print(f\"   표준화: {standardized}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TextVectorization 설정 및 어휘 사전 구축\n",
    "\n",
    "### 5.1 TextVectorization 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextVectorization 설정:\n",
      "- 최대 토큰 수: 1000\n",
      "- 출력 모드: int\n",
      "- 시퀀스 길이: 20\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization 설정 변수\n",
    "MAX_TOKENS = 1000\n",
    "OUTPUT_MODE = \"int\"\n",
    "OUTPUT_SEQUENCE_LENGTH = 20\n",
    "\n",
    "# TextVectorization 객체 생성\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,                # 최대 어휘 사전 크기 (상위 1000개 빈도 단어)\n",
    "    output_mode=OUTPUT_MODE,              # 출력 형태: 정수 시퀀스\n",
    "    output_sequence_length=OUTPUT_SEQUENCE_LENGTH,  # 출력 시퀀스 최대 길이 (패딩/자르기)\n",
    "    standardize=None                      # 커스텀 표준화 함수를 외부에서 적용하므로 None\n",
    ")\n",
    "\n",
    "print(\"TextVectorization 설정:\")\n",
    "print(f\"- 최대 토큰 수: {MAX_TOKENS}\")\n",
    "print(f\"- 출력 모드: {OUTPUT_MODE}\")\n",
    "print(f\"- 시퀀스 길이: {OUTPUT_SEQUENCE_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 어휘 사전 학습 (Adaptation)\n",
    "\n",
    "사전에 정의된 텍스트 데이터로부터 어휘 사전을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 어휘 사전 학습용 데이터 ===\n",
      "1. 나 는 ai 챗봇 을 사용 합니다\n",
      "2. 자연어 처리 는 어렵습니다\n",
      "3. 챗봇 과 대화 하는것이 즐겁습니다\n",
      "4. ai 로 인해 사람 들 이 일자리 가 사라지고 있습니다\n",
      "\n",
      "어휘 사전 학습 중...\n",
      "어휘 사전 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# 표준화된 데이터 생성\n",
    "processed_data = [custom_standardization_fn(text) for text in sample_texts]\n",
    "\n",
    "print(\"=== 어휘 사전 학습용 데이터 ===\")\n",
    "for i, data in enumerate(processed_data, 1):\n",
    "    print(f\"{i}. {data}\")\n",
    "\n",
    "print(\"\\n어휘 사전 학습 중...\")\n",
    "\n",
    "# 어휘 사전 학습\n",
    "vectorizer.adapt(processed_data)\n",
    "\n",
    "print(\"어휘 사전 학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 어휘 사전 분석\n",
    "\n",
    "### 6.1 생성된 어휘 사전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 생성된 어휘 사전 ===\n",
      "총 어휘 수: 25\n",
      "\n",
      "어휘 목록 (인덱스 순):\n",
      " 0: ''\n",
      " 1: '[UNK]'\n",
      " 2: '챗봇'\n",
      " 3: '는'\n",
      " 4: 'ai'\n",
      " 5: '합니다'\n",
      " 6: '하는것이'\n",
      " 7: '처리'\n",
      " 8: '즐겁습니다'\n",
      " 9: '자연어'\n",
      "10: '있습니다'\n",
      "11: '일자리'\n",
      "12: '인해'\n",
      "13: '이'\n",
      "14: '을'\n",
      "15: '어렵습니다'\n",
      "16: '사용'\n",
      "17: '사람'\n",
      "18: '사라지고'\n",
      "19: '로'\n",
      "... (총 25개 어휘)\n"
     ]
    }
   ],
   "source": [
    "# 어휘 사전 가져오기\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "\n",
    "print(\"=== 생성된 어휘 사전 ===\")\n",
    "print(f\"총 어휘 수: {len(vocabulary)}\")\n",
    "print(\"\\n어휘 목록 (인덱스 순):\")\n",
    "for i, word in enumerate(vocabulary[:20]):  # 상위 20개만 출력\n",
    "    print(f\"{i:2d}: '{word}'\")\n",
    "\n",
    "if len(vocabulary) > 20:\n",
    "    print(f\"... (총 {len(vocabulary)}개 어휘)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 단어-인덱스 매핑 딕셔너리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 단어-인덱스 매핑 딕셔너리 ===\n",
      "형식: {단어: 인덱스}\n",
      "\n",
      "특수 토큰:\n",
      "  '': 0\n",
      "  '[UNK]': 1\n",
      "\n",
      "일반 어휘:\n",
      "  '챗봇': 2\n",
      "  '는': 3\n",
      "  'ai': 4\n",
      "  '합니다': 5\n",
      "  '하는것이': 6\n",
      "  '처리': 7\n",
      "  '즐겁습니다': 8\n",
      "  '자연어': 9\n",
      "  '있습니다': 10\n",
      "  '일자리': 11\n",
      "  ... (총 23개 일반 어휘)\n"
     ]
    }
   ],
   "source": [
    "# 단어를 키로, 인덱스를 값으로 하는 딕셔너리 생성\n",
    "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "\n",
    "print(\"=== 단어-인덱스 매핑 딕셔너리 ===\")\n",
    "print(\"형식: {단어: 인덱스}\")\n",
    "print()\n",
    "\n",
    "# 빈 문자열과 [UNK] 토큰 확인\n",
    "special_tokens = ['', '[UNK]']\n",
    "print(\"특수 토큰:\")\n",
    "for token in special_tokens:\n",
    "    if token in word_to_index:\n",
    "        print(f\"  '{token}': {word_to_index[token]}\")\n",
    "\n",
    "print(\"\\n일반 어휘:\")\n",
    "regular_words = [word for word in vocabulary if word not in special_tokens]\n",
    "for word in regular_words[:10]:  # 상위 10개만 출력\n",
    "    print(f\"  '{word}': {word_to_index[word]}\")\n",
    "\n",
    "if len(regular_words) > 10:\n",
    "    print(f\"  ... (총 {len(regular_words)}개 일반 어휘)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 텍스트 벡터화 실험\n",
    "\n",
    "### 7.1 새로운 텍스트 벡터화 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 테스트 텍스트 벡터화 ===\n",
      "\n",
      "테스트 1: 챗봇과 이야기를 합니다\n",
      "  표준화: 챗봇 과 이야기 를 합니다\n",
      "  벡터화: [ 2 23  1  1  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  토큰별 인덱스: {'챗봇': 2, '과': 23, '이야기': 1, '를': 1, '합니다': 5}\n",
      "\n",
      "테스트 2: 챗봇보다 재미나이가 더 좋아요\n",
      "  표준화: 챗봇 보다 재미나 이 가 더 좋아요\n",
      "  벡터화: [ 2  1  1 13 24  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  토큰별 인덱스: {'챗봇': 2, '보다': 1, '재미나': 1, '이': 13, '가': 24, '더': 1, '좋아요': 1}\n",
      "\n",
      "테스트 3: 자연어 처리는 어려워요\n",
      "  표준화: 자연어 처리 는 어려워요\n",
      "  벡터화: [9 7 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  토큰별 인덱스: {'자연어': 9, '처리': 7, '는': 3, '어려워요': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 새로운 텍스트들\n",
    "test_texts = [\n",
    "    \"챗봇과 이야기를 합니다\",\n",
    "    \"챗봇보다 재미나이가 더 좋아요\", \n",
    "    \"자연어 처리는 어려워요\"\n",
    "]\n",
    "\n",
    "print(\"=== 테스트 텍스트 벡터화 ===\")\n",
    "print()\n",
    "\n",
    "# 각 테스트 텍스트에 대해 단계별 처리 과정 보여주기\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"테스트 {i}: {text}\")\n",
    "    \n",
    "    # 1단계: 표준화\n",
    "    standardized = custom_standardization_fn(text)\n",
    "    print(f\"  표준화: {standardized}\")\n",
    "    \n",
    "    # 2단계: 벡터화\n",
    "    vectorized = vectorizer([standardized])\n",
    "    print(f\"  벡터화: {vectorized.numpy()[0]}\")\n",
    "    \n",
    "    # 3단계: 벡터를 단어로 역변환하여 확인\n",
    "    tokens = standardized.split()\n",
    "    token_indices = []\n",
    "    for token in tokens:\n",
    "        if token in word_to_index:\n",
    "            token_indices.append(word_to_index[token])\n",
    "        else:\n",
    "            token_indices.append(word_to_index['[UNK]'] if '[UNK]' in word_to_index else 1)\n",
    "    \n",
    "    print(f\"  토큰별 인덱스: {dict(zip(tokens, token_indices))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 배치 벡터화 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 배치 벡터화 결과 ===\n",
      "벡터화된 텐서 형태: (3, 20)\n",
      "(배치 크기: 3, 시퀀스 길이: 20)\n",
      "\n",
      "각 텍스트별 벡터:\n",
      "1. '챗봇과 이야기를 합니다'\n",
      "   벡터: [ 2 23  1  1  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "   유효값: [ 2 23  1  1  5]\n",
      "\n",
      "2. '챗봇보다 재미나이가 더 좋아요'\n",
      "   벡터: [ 2  1  1 13 24  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "   유효값: [ 2  1  1 13 24  1  1]\n",
      "\n",
      "3. '자연어 처리는 어려워요'\n",
      "   벡터: [9 7 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "   유효값: [9 7 3 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모든 테스트 텍스트를 한 번에 벡터화\n",
    "standardized_test_texts = [custom_standardization_fn(text) for text in test_texts]\n",
    "batch_vectorized = vectorizer(standardized_test_texts)\n",
    "\n",
    "print(\"=== 배치 벡터화 결과 ===\")\n",
    "print(f\"벡터화된 텐서 형태: {batch_vectorized.shape}\")\n",
    "print(f\"(배치 크기: {batch_vectorized.shape[0]}, 시퀀스 길이: {batch_vectorized.shape[1]})\")\n",
    "print()\n",
    "\n",
    "print(\"각 텍스트별 벡터:\")\n",
    "for i, (original, vector) in enumerate(zip(test_texts, batch_vectorized.numpy())):\n",
    "    print(f\"{i+1}. '{original}'\")\n",
    "    print(f\"   벡터: {vector}\")\n",
    "    # 0이 아닌 값들만 출력 (패딩 제거)\n",
    "    non_zero = vector[vector != 0]\n",
    "    print(f\"   유효값: {non_zero}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 결과 분석 및 시각화\n",
    "\n",
    "### 8.1 어휘 빈도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토큰 빈도 분석 ===\n",
      "총 토큰 수: 26\n",
      "고유 토큰 수: 23\n",
      "\n",
      "빈도별 토큰 순위:\n",
      " 1. '는': 2회 (인덱스: 3)\n",
      " 2. 'ai': 2회 (인덱스: 4)\n",
      " 3. '챗봇': 2회 (인덱스: 2)\n",
      " 4. '나': 1회 (인덱스: 22)\n",
      " 5. '을': 1회 (인덱스: 14)\n",
      " 6. '사용': 1회 (인덱스: 16)\n",
      " 7. '합니다': 1회 (인덱스: 5)\n",
      " 8. '자연어': 1회 (인덱스: 9)\n",
      " 9. '처리': 1회 (인덱스: 7)\n",
      "10. '어렵습니다': 1회 (인덱스: 15)\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터에서 토큰 빈도 계산\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = []\n",
    "for text in sample_texts:\n",
    "    tokens = custom_standardization_fn(text).split()\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "print(\"=== 토큰 빈도 분석 ===\")\n",
    "print(f\"총 토큰 수: {len(all_tokens)}\")\n",
    "print(f\"고유 토큰 수: {len(token_freq)}\")\n",
    "print()\n",
    "\n",
    "print(\"빈도별 토큰 순위:\")\n",
    "for i, (token, freq) in enumerate(token_freq.most_common(10), 1):\n",
    "    index = word_to_index.get(token, 'Unknown')\n",
    "    print(f\"{i:2d}. '{token}': {freq}회 (인덱스: {index})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 벡터화 과정 요약\n",
    "\n",
    "텍스트를 딥러닝 모델에서 사용할 수 있는 숫자 벡터로 변환하는 전체 과정을 정리하면 다음과 같습니다:\n",
    "\n",
    "#### 1단계: 텍스트 전처리 (clean_text)\n",
    "- 소문자 변환\n",
    "- 특수문자 제거 (한글, 영문, 숫자, 공백만 유지)\n",
    "\n",
    "#### 2단계: 형태소 분석 (KoNLPy Okt)\n",
    "- 의미 단위로 토큰 분리\n",
    "- 공백으로 토큰 재결합\n",
    "\n",
    "#### 3단계: 어휘 사전 구축 (TextVectorization.adapt)\n",
    "- 빈도 기반 어휘 사전 생성\n",
    "- 각 토큰에 고유 인덱스 할당\n",
    "\n",
    "#### 4단계: 벡터화 (TextVectorization.__call__)\n",
    "- 토큰을 인덱스로 변환\n",
    "- 시퀀스 길이 통일 (패딩/자르기)\n",
    "\n",
    "**최종 결과**: 텍스트 → 길이 20의 정수 벡터\n",
    "\n",
    "이 과정을 통해 자연어 텍스트를 딥러닝 모델이 이해할 수 있는 수치 데이터로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 실전 활용 예시\n",
    "\n",
    "### 9.1 새로운 텍스트 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 실전 텍스트 처리 예시 ===\n",
      "입력 텍스트: 'AI 기술이 발전하고 있어요'\n",
      "표준화 결과: 'ai 기술 이 발전 하고 있어요'\n",
      "벡터화 결과: [ 4  1 13  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "유효 토큰 수: 6\n",
      "\n",
      "입력 텍스트: '머신러닝은 정말 흥미로운 분야입니다'\n",
      "표준화 결과: '머신 러닝 은 정말 흥미로운 분야 입니다'\n",
      "벡터화 결과: [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "유효 토큰 수: 7\n",
      "\n",
      "입력 텍스트: '코딩하는 것이 즐거워요'\n",
      "표준화 결과: '코딩 하는 것 이 즐거워요'\n",
      "벡터화 결과: [ 1  1  1 13  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "유효 토큰 수: 5\n",
      "\n",
      "최종 입력 배열 형태: (3, 20)\n",
      "이 배열을 딥러닝 모델의 입력으로 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "def process_new_text(text, show_steps=True):\n",
    "    \"\"\"\n",
    "    새로운 텍스트를 처리하여 모델 입력용 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        text (str): 처리할 텍스트\n",
    "        show_steps (bool): 중간 과정 출력 여부\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: 벡터화된 결과\n",
    "    \"\"\"\n",
    "    if show_steps:\n",
    "        print(f\"입력 텍스트: '{text}'\")\n",
    "    \n",
    "    # 표준화\n",
    "    standardized = custom_standardization_fn(text)\n",
    "    if show_steps:\n",
    "        print(f\"표준화 결과: '{standardized}'\")\n",
    "    \n",
    "    # 벡터화\n",
    "    vector = vectorizer([standardized]).numpy()[0]\n",
    "    if show_steps:\n",
    "        print(f\"벡터화 결과: {vector}\")\n",
    "        non_zero_indices = np.nonzero(vector)[0]\n",
    "        if len(non_zero_indices) > 0:\n",
    "            print(f\"유효 토큰 수: {len(non_zero_indices)}\")\n",
    "        print()\n",
    "    \n",
    "    return vector\n",
    "\n",
    "# 실전 예시\n",
    "example_texts = [\n",
    "    \"AI 기술이 발전하고 있어요\",\n",
    "    \"머신러닝은 정말 흥미로운 분야입니다\",\n",
    "    \"코딩하는 것이 즐거워요\"\n",
    "]\n",
    "\n",
    "print(\"=== 실전 텍스트 처리 예시 ===\")\n",
    "processed_vectors = []\n",
    "for text in example_texts:\n",
    "    vector = process_new_text(text)\n",
    "    processed_vectors.append(vector)\n",
    "\n",
    "# 처리된 벡터들을 배열로 변환 (모델 입력용)\n",
    "input_array = np.array(processed_vectors)\n",
    "print(f\"최종 입력 배열 형태: {input_array.shape}\")\n",
    "print(\"이 배열을 딥러닝 모델의 입력으로 사용할 수 있습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
