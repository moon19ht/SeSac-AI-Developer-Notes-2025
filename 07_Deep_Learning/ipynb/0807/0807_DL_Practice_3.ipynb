{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "## ê°œìš”\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°(NSMC)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ë¶„ì„ì„ ìœ„í•œ ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” í•™ìŠµ ë‚´ìš©:\n",
    "1. **Korpora ë¼ì´ë¸ŒëŸ¬ë¦¬**ë¥¼ í†µí•œ NSMC ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "2. **ë°ì´í„° êµ¬ì¡°í™”**: train/validation/test ë¶„í•  ë° íŒŒì¼ ì‹œìŠ¤í…œ êµ¬ì¡° ìƒì„±\n",
    "3. **TensorFlow Dataset API**ë¥¼ í™œìš©í•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬\n",
    "4. **í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**: KoNLPy + TensorFlow í†µí•©\n",
    "5. **ë²¡í„°í™” íŒŒì´í”„ë¼ì¸**: ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜\n",
    "6. **ì„±ëŠ¥ ìµœì í™”**: ìºì‹±, í”„ë¦¬í˜ì¹­, ë³‘ë ¬ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### 1.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "```bash\n",
    "# Korpora ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (condaë¡œ ì„¤ì¹˜ê°€ ì•ˆë˜ë¯€ë¡œ pip ì‚¬ìš©)\n",
    "pip install Korpora\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"Python ë²„ì „: {tf.version.VERSION}\")\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NSMC ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° íƒìƒ‰\n",
    "\n",
    "### 2.1 Korporaë¥¼ í†µí•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Korpora import Korpora\n",
    "\n",
    "# ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "data_dir = \"../../data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# NSMC(Naver Sentiment Movie Corpus) ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "# ê¸°ë³¸ ê²½ë¡œ ëŒ€ì‹  ì§€ì •í•œ ê²½ë¡œì— ì €ì¥\n",
    "print(\"NSMC ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "print(f\"ì €ì¥ ê²½ë¡œ: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# root_dir íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ ìœ„ì¹˜ ì§€ì •\n",
    "Korpora.fetch(\"nsmc\", root_dir=data_dir)\n",
    "print(\"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ë°ì´í„°ì…‹ ë¡œë“œ ë° êµ¬ì¡° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "\n",
    "# ë°ì´í„° êµ¬ì¡° íƒìƒ‰\n",
    "print(\"=== NSMC ë°ì´í„°ì…‹ ì •ë³´ ===\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {len(corpus.train.texts)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(corpus.test.texts)}\")\n",
    "print()\n",
    "\n",
    "print(\"=== ìƒ˜í”Œ ë°ì´í„° (ì²« 3ê°œ) ===\")\n",
    "for i in range(3):\n",
    "    print(f\"ìƒ˜í”Œ {i+1}:\")\n",
    "    print(f\"  í…ìŠ¤íŠ¸: {corpus.train.texts[i]}\")\n",
    "    print(f\"  ë¼ë²¨: {corpus.train.labels[i]} ({'ê¸ì •' if corpus.train.labels[i] == 1 else 'ë¶€ì •'})\")  # ì •ìˆ˜ 1ê³¼ ë¹„êµ\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ë¼ë²¨ë§ ê²€ì¦ ë° ì›ë³¸ ë°ì´í„° í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ íŒŒì¼ì„ ì§ì ‘ ì½ì–´ì„œ í™•ì¸í•´ë³´ì\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== ì›ë³¸ íŒŒì¼ ì§ì ‘ í™•ì¸ ===\")\n",
    "try:\n",
    "    # ì›ë³¸ TSV íŒŒì¼ ì§ì ‘ ì½ê¸°\n",
    "    train_file_path = os.path.join(data_dir, \"nsmc\", \"ratings_train.txt\")\n",
    "    \n",
    "    # ì²« ëª‡ ì¤„ì„ ì§ì ‘ ì½ì–´ë³´ê¸°\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:10]  # ì²« 10ì¤„\n",
    "    \n",
    "    print(\"ì›ë³¸ íŒŒì¼ ì²« 10ì¤„:\")\n",
    "    for i, line in enumerate(lines):\n",
    "        print(f\"{i}: {line.strip()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # pandasë¡œ ì •í™•íˆ íŒŒì‹±í•´ë³´ê¸°\n",
    "    df_train = pd.read_csv(train_file_path, sep='\\t', encoding='utf-8')\n",
    "    print(f\"pandasë¡œ ì½ì€ ë°ì´í„°:\")\n",
    "    print(f\"ì»¬ëŸ¼ëª…: {list(df_train.columns)}\")\n",
    "    print(f\"ë°ì´í„° í˜•íƒœ: {df_train.shape}\")\n",
    "    print(\"\\nì²« 5ê°œ ìƒ˜í”Œ:\")\n",
    "    print(df_train.head())\n",
    "    \n",
    "    print(f\"\\në¼ë²¨ ë¶„í¬:\")\n",
    "    print(df_train['label'].value_counts().sort_index())\n",
    "    \n",
    "    # Korpora ë°ì´í„°ì™€ ë¹„êµ\n",
    "    print(f\"\\n=== Korpora vs ì›ë³¸ ë°ì´í„° ë¹„êµ ===\")\n",
    "    print(\"ì²« 5ê°œ ìƒ˜í”Œ ë¹„êµ:\")\n",
    "    for i in range(5):\n",
    "        korpora_text = corpus.train.texts[i]\n",
    "        korpora_label = corpus.train.labels[i]\n",
    "        pandas_text = df_train.iloc[i]['document']\n",
    "        pandas_label = df_train.iloc[i]['label']\n",
    "        \n",
    "        print(f\"\\nìƒ˜í”Œ {i+1}:\")\n",
    "        print(f\"  Korpora: '{korpora_text}' | ë¼ë²¨: {korpora_label}\")\n",
    "        print(f\"  ì›ë³¸:    '{pandas_text}' | ë¼ë²¨: {pandas_label}\")\n",
    "        print(f\"  ë§¤ì¹˜: {korpora_text == pandas_text and korpora_label == pandas_label}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ì›ë³¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ë²¨ë§ ë¬¸ì œ ë¶„ì„: ìƒ˜í”Œë“¤ì„ ë” ìì„¸íˆ ì‚´í´ë³´ì\n",
    "print(\"=== ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¼ë²¨ë§ ìƒ˜í”Œ ë¶„ì„ ===\")\n",
    "\n",
    "# ëª…ë°±íˆ ê¸ì •ì ì¸ í‘œí˜„ë“¤ì„ ê°€ì§„ ë¶€ì • ë¼ë²¨ ìƒ˜í”Œë“¤ ì°¾ê¸°\n",
    "positive_words = ['ì¢‹', 'ì¬ë¯¸', 'ì¶”ì²œ', 'ìµœê³ ', 'í›Œë¥­', 'ê°ë™', 'ë©‹ì§„', 'ì™„ë²½']\n",
    "negative_words = ['ì§œì¦', 'ë³„ë¡œ', 'ìµœì•…', 'ì‹¤ë§', 'ì§€ë£¨', 'ì¬ë¯¸ì—†', 'ì•„ê¹']\n",
    "\n",
    "print(\"ë¶€ì • ë¼ë²¨(0)ì¸ë° ê¸ì • ë‹¨ì–´ê°€ í¬í•¨ëœ ìƒ˜í”Œë“¤:\")\n",
    "positive_in_negative = []\n",
    "for i in range(min(1000, len(corpus.train.texts))):  # ì²« 1000ê°œë§Œ í™•ì¸\n",
    "    text = corpus.train.texts[i]\n",
    "    label = corpus.train.labels[i]\n",
    "    \n",
    "    if label == 0:  # ë¶€ì • ë¼ë²¨\n",
    "        if any(pos_word in text for pos_word in positive_words):\n",
    "            positive_in_negative.append((i, text, label))\n",
    "            if len(positive_in_negative) <= 5:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n",
    "                print(f\"  {i}: [{label}] {text}\")\n",
    "\n",
    "print(f\"\\nì´ {len(positive_in_negative)}ê°œ ë°œê²¬\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ê¸ì • ë¼ë²¨(1)ì¸ë° ë¶€ì • ë‹¨ì–´ê°€ í¬í•¨ëœ ìƒ˜í”Œë“¤:\")\n",
    "negative_in_positive = []\n",
    "for i in range(min(1000, len(corpus.train.texts))):\n",
    "    text = corpus.train.texts[i]\n",
    "    label = corpus.train.labels[i]\n",
    "    \n",
    "    if label == 1:  # ê¸ì • ë¼ë²¨\n",
    "        if any(neg_word in text for neg_word in negative_words):\n",
    "            negative_in_positive.append((i, text, label))\n",
    "            if len(negative_in_positive) <= 5:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n",
    "                print(f\"  {i}: [{label}] {text}\")\n",
    "\n",
    "print(f\"\\nì´ {len(negative_in_positive)}ê°œ ë°œê²¬\")\n",
    "\n",
    "# ì „ì²´ ë¼ë²¨ ë¶„í¬ ë‹¤ì‹œ í™•ì¸\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== ì „ì²´ ë¼ë²¨ ë¶„í¬ í™•ì¸ ===\")\n",
    "pos_count = sum(1 for label in corpus.train.labels if label == 1)\n",
    "neg_count = sum(1 for label in corpus.train.labels if label == 0)\n",
    "total = len(corpus.train.labels)\n",
    "\n",
    "print(f\"ê¸ì •(1): {pos_count:,}ê°œ ({pos_count/total*100:.1f}%)\")\n",
    "print(f\"ë¶€ì •(0): {neg_count:,}ê°œ ({neg_count/total*100:.1f}%)\")\n",
    "print(f\"ì´ ìƒ˜í”Œ: {total:,}ê°œ\")\n",
    "\n",
    "# ë°ì´í„° íƒ€ì…ê³¼ ê³ ìœ ê°’ í™•ì¸\n",
    "print(f\"\\në¼ë²¨ ë°ì´í„° íƒ€ì…: {type(corpus.train.labels[0])}\")\n",
    "print(f\"ë¼ë²¨ ê³ ìœ ê°’: {set(corpus.train.labels)}\")\n",
    "\n",
    "print(\"\\n=== ê²°ë¡  ===\")\n",
    "print(\"NSMC ë°ì´í„°ì…‹ì€ ì‹¤ì œë¡œ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"ì¼ë¶€ ë¼ë²¨ë§ ì˜¤ë¥˜ê°€ ì¡´ì¬í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì‹¤ì œ ë°ì´í„°ì—ì„œ í”íˆ ë°œìƒí•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.\")\n",
    "print(\"ëª¨ë¸ í›ˆë ¨ ì‹œì—ëŠ” ì´ëŸ¬í•œ ë…¸ì´ì¦ˆë¥¼ ê³ ë ¤í•˜ì—¬ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° êµ¬ì¡°í™” ë° íŒŒì¼ ì‹œìŠ¤í…œ ì¤€ë¹„\n",
    "\n",
    "TensorFlowì˜ `text_dataset_from_directory`ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 3.1 ë°ì´í„° êµ¬ì¡°í™” í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_korean_dataset(base_dir=None, validation_size=1000):\n",
    "    \"\"\"\n",
    "    NSMC ë°ì´í„°ë¥¼ TensorFlow Dataset í˜•ì‹ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ë³€í™˜\n",
    "    \n",
    "    êµ¬ì¡°:\n",
    "    ../../data/korean_imdb/\n",
    "    â”œâ”€â”€ train/\n",
    "    â”‚   â”œâ”€â”€ pos/  (ê¸ì • ë¦¬ë·°ë“¤)\n",
    "    â”‚   â””â”€â”€ neg/  (ë¶€ì • ë¦¬ë·°ë“¤)\n",
    "    â”œâ”€â”€ val/\n",
    "    â”‚   â”œâ”€â”€ pos/\n",
    "    â”‚   â””â”€â”€ neg/\n",
    "    â””â”€â”€ test/\n",
    "        â”œâ”€â”€ pos/\n",
    "        â””â”€â”€ neg/\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): ë””ë ‰í† ë¦¬ ì´ë¦„ (ê¸°ë³¸ê°’: Noneì´ë©´ data_dir/korean_imdb ì‚¬ìš©)\n",
    "        validation_size (int): ê²€ì¦ ë°ì´í„° í¬ê¸° (ê° í´ë˜ìŠ¤ë³„)\n",
    "    \n",
    "    Returns:\n",
    "        str: ìƒì„±ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    \n",
    "    if base_dir is None:\n",
    "        base_dir = os.path.join(data_dir, \"korean_imdb\")\n",
    "    \n",
    "    print(f\"ë°ì´í„° êµ¬ì¡°í™” ì‹œì‘: {base_dir}\")\n",
    "    \n",
    "    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ (ìˆë‹¤ë©´)\n",
    "    if os.path.exists(base_dir):\n",
    "        try:\n",
    "            shutil.rmtree(base_dir)\n",
    "            print(f\"ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ: {base_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"ë””ë ‰í† ë¦¬ ì‚­ì œ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n",
    "    directories = [\n",
    "        os.path.join(base_dir, \"train\", \"pos\"),\n",
    "        os.path.join(base_dir, \"train\", \"neg\"),\n",
    "        os.path.join(base_dir, \"val\", \"pos\"),\n",
    "        os.path.join(base_dir, \"val\", \"neg\"),\n",
    "        os.path.join(base_dir, \"test\", \"pos\"),\n",
    "        os.path.join(base_dir, \"test\", \"neg\")\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    print(\"ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ë°ì´í„° ë¶„í•  ë° íŒŒì¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_files(corpus, base_dir, validation_size=1000):\n",
    "    \"\"\"\n",
    "    corpus ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„° ë¼ë²¨ë³„ ë¶„ë¦¬\n",
    "    print(\"í›ˆë ¨ ë°ì´í„° ë¶„ë¦¬ ì¤‘...\")\n",
    "    pos_train_texts = []\n",
    "    neg_train_texts = []\n",
    "    \n",
    "    for text, label in zip(corpus.train.texts, corpus.train.labels):\n",
    "        if label == 1:  # ì •ìˆ˜ 1ë¡œ ë¹„êµ\n",
    "            pos_train_texts.append(text)\n",
    "        elif label == 0:  # ì •ìˆ˜ 0ë¡œ ë¹„êµ\n",
    "            neg_train_texts.append(text)\n",
    "    \n",
    "    print(f\"ê¸ì • í›ˆë ¨ ë°ì´í„°: {len(pos_train_texts):,}ê°œ\")\n",
    "    print(f\"ë¶€ì • í›ˆë ¨ ë°ì´í„°: {len(neg_train_texts):,}ê°œ\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ë³„ ë¶„ë¦¬\n",
    "    print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ ì¤‘...\")\n",
    "    pos_test_texts = []\n",
    "    neg_test_texts = []\n",
    "    \n",
    "    for text, label in zip(corpus.test.texts, corpus.test.labels):\n",
    "        if label == 1:  # ì •ìˆ˜ 1ë¡œ ë¹„êµ\n",
    "            pos_test_texts.append(text)\n",
    "        elif label == 0:  # ì •ìˆ˜ 0ë¡œ ë¹„êµ\n",
    "            neg_test_texts.append(text)\n",
    "    \n",
    "    print(f\"ê¸ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(pos_test_texts):,}ê°œ\")\n",
    "    print(f\"ë¶€ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(neg_test_texts):,}ê°œ\")\n",
    "    \n",
    "    # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸\n",
    "    if len(pos_train_texts) < validation_size or len(neg_train_texts) < validation_size:\n",
    "        print(f\"ê²½ê³ : ê²€ì¦ ë°ì´í„° ë¶„í• ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        validation_size = min(len(pos_train_texts), len(neg_train_texts)) // 2\n",
    "        print(f\"ê²€ì¦ ë°ì´í„° í¬ê¸°ë¥¼ {validation_size}ê°œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì—ì„œ ì¼ë¶€ ì¶”ì¶œ)\n",
    "    print(f\"ê²€ì¦ ë°ì´í„° ë¶„í•  ì¤‘... (ê° í´ë˜ìŠ¤ë³„ {validation_size}ê°œ)\")\n",
    "    pos_val_texts = pos_train_texts[:validation_size] if pos_train_texts else []\n",
    "    neg_val_texts = neg_train_texts[:validation_size] if neg_train_texts else []\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ê²€ì¦ ë°ì´í„° ì œê±°\n",
    "    pos_train_texts = pos_train_texts[validation_size:] if pos_train_texts else []\n",
    "    neg_train_texts = neg_train_texts[validation_size:] if neg_train_texts else []\n",
    "    \n",
    "    print(f\"ìµœì¢… í›ˆë ¨ ë°ì´í„° - ê¸ì •: {len(pos_train_texts):,}ê°œ, ë¶€ì •: {len(neg_train_texts):,}ê°œ\")\n",
    "    print(f\"ê²€ì¦ ë°ì´í„° - ê¸ì •: {len(pos_val_texts):,}ê°œ, ë¶€ì •: {len(neg_val_texts):,}ê°œ\")\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥ í•¨ìˆ˜\n",
    "    def save_texts_to_files(texts, directory, prefix):\n",
    "        os.makedirs(directory, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "        for i, text in enumerate(texts):\n",
    "            filename = f\"{prefix}_{i}.txt\"\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„° ì €ì¥\n",
    "    print(\"í›ˆë ¨ ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "    if pos_train_texts:\n",
    "        save_texts_to_files(pos_train_texts, os.path.join(base_dir, \"train\", \"pos\"), \"pos\")\n",
    "    if neg_train_texts:\n",
    "        save_texts_to_files(neg_train_texts, os.path.join(base_dir, \"train\", \"neg\"), \"neg\")\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„° ì €ì¥\n",
    "    print(\"ê²€ì¦ ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "    if pos_val_texts:\n",
    "        save_texts_to_files(pos_val_texts, os.path.join(base_dir, \"val\", \"pos\"), \"pos\")\n",
    "    if neg_val_texts:\n",
    "        save_texts_to_files(neg_val_texts, os.path.join(base_dir, \"val\", \"neg\"), \"neg\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥\n",
    "    print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "    if pos_test_texts:\n",
    "        save_texts_to_files(pos_test_texts, os.path.join(base_dir, \"test\", \"pos\"), \"pos\")\n",
    "    if neg_test_texts:\n",
    "        save_texts_to_files(neg_test_texts, os.path.join(base_dir, \"test\", \"neg\"), \"neg\")\n",
    "    \n",
    "    print(\"ëª¨ë“  ë°ì´í„° íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "    return {\n",
    "        'train_pos': len(pos_train_texts),\n",
    "        'train_neg': len(neg_train_texts), \n",
    "        'val_pos': len(pos_val_texts),\n",
    "        'val_neg': len(neg_val_texts),\n",
    "        'test_pos': len(pos_test_texts),\n",
    "        'test_neg': len(neg_test_texts)\n",
    "    }\n",
    "\n",
    "# ì‹¤í–‰ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
    "korean_data_dir = create_korean_dataset()\n",
    "data_stats = save_data_to_files(corpus, korean_data_dir)\n",
    "\n",
    "print(f\"ë°ì´í„° ë””ë ‰í† ë¦¬: {korean_data_dir}\")\n",
    "print(f\"ë°ì´í„° í†µê³„: {data_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorFlow Dataset ë¡œë“œ\n",
    "\n",
    "### 4.1 ë°ì´í„°ì…‹ ì´ˆê¸° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(\"TensorFlow Dataset ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "try:\n",
    "    # text_dataset_from_directoryë¥¼ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    # ì˜¤íƒ€ ìˆ˜ì •: dataset_from_directory -> text_dataset_from_directory\n",
    "    train_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "        korean_data_dir + \"/train\", \n",
    "        batch_size=BATCH_SIZE, \n",
    "        label_mode=\"binary\",\n",
    "        validation_split=None\n",
    "    )\n",
    "    \n",
    "    val_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "        korean_data_dir + \"/val\", \n",
    "        batch_size=BATCH_SIZE, \n",
    "        label_mode=\"binary\",\n",
    "        validation_split=None\n",
    "    )\n",
    "    \n",
    "    test_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "        korean_data_dir + \"/test\", \n",
    "        batch_size=BATCH_SIZE, \n",
    "        label_mode=\"binary\",\n",
    "        validation_split=None\n",
    "    )\n",
    "    \n",
    "    print(\"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ë¨¼ì € ë°ì´í„° êµ¬ì¡°í™”ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ë¡œë“œëœ ë°ì´í„° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸\n",
    "print(\"=== ë°ì´í„°ì…‹ ì •ë³´ ===\")\n",
    "\n",
    "def get_dataset_info(dataset, name):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  ìš”ì†Œ ìŠ¤í™: {dataset.element_spec}\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    for texts, labels in dataset.take(1):\n",
    "        print(f\"  ë°°ì¹˜ í¬ê¸°: {texts.shape[0]}\")\n",
    "        print(f\"  í…ìŠ¤íŠ¸ í˜•íƒœ: {texts.shape}\")\n",
    "        print(f\"  ë¼ë²¨ í˜•íƒœ: {labels.shape}\")\n",
    "        print(f\"  ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {texts[0].numpy().decode('utf-8')[:100]}...\")\n",
    "        print(f\"  ì²« ë²ˆì§¸ ë¼ë²¨: {labels[0].numpy()}\")\n",
    "    print()\n",
    "\n",
    "try:\n",
    "    get_dataset_info(train_ds_raw, \"í›ˆë ¨ ë°ì´í„°ì…‹\")\n",
    "    get_dataset_info(val_ds_raw, \"ê²€ì¦ ë°ì´í„°ì…‹\")\n",
    "    get_dataset_info(test_ds_raw, \"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹\")\n",
    "except:\n",
    "    print(\"ë°ì´í„°ì…‹ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "### 5.1 í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "okt = Okt()\n",
    "print(\"KoNLPy Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
    "    \n",
    "    ì£¼ì˜: TensorFlowì—ì„œ ì „ë‹¬ë˜ëŠ” í…ìŠ¤íŠ¸ëŠ” bytes í˜•íƒœë¡œ ì¸ì½”ë”©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ\n",
    "    ë¨¼ì € UTF-8ë¡œ ë””ì½”ë”©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        text (bytes): TensorFlowì—ì„œ ì „ë‹¬ëœ ì¸ì½”ë”©ëœ í…ìŠ¤íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "        str: ì •ì œëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # TensorFlow tensorë¥¼ Python stringìœ¼ë¡œ ë””ì½”ë”©\n",
    "    text = text.decode(\"utf-8\")\n",
    "    \n",
    "    # ì†Œë¬¸ì ë³€í™˜\n",
    "    text = text.lower()\n",
    "    \n",
    "    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€\n",
    "    text = re.sub(r\"[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_text = \"ì´ ì˜í™”ëŠ” ì •ë§ ì¢‹ì•˜ì–´ìš”! 5ì  ë§Œì ì— 5ì â˜…â˜…â˜…\"\n",
    "test_bytes = test_text.encode('utf-8')\n",
    "cleaned = clean_text(test_bytes)\n",
    "print(f\"ì›ë³¸: {test_text}\")\n",
    "print(f\"ì •ì œ: {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ë°°ì¹˜ ì „ì²˜ë¦¬ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_korean_preprocess(text_tensor):\n",
    "    \"\"\"\n",
    "    TensorFlow í…ì„œ ë°°ì¹˜ì— ëŒ€í•œ í•œêµ­ì–´ ì „ì²˜ë¦¬\n",
    "    \n",
    "    ì´ í•¨ìˆ˜ëŠ” tf.py_functionì„ í†µí•´ í˜¸ì¶œë˜ë©°,\n",
    "    TensorFlow í…ì„œë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬í•œ í›„ ë‹¤ì‹œ í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        text_tensor (tf.Tensor): í…ìŠ¤íŠ¸ ë°°ì¹˜ í…ì„œ\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë°°ì¹˜ í…ì„œ\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    \n",
    "    # ë°°ì¹˜ì˜ ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì²˜ë¦¬\n",
    "    for text_bytes in text_tensor.numpy():\n",
    "        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ\n",
    "        cleaned_text = clean_text(text_bytes)\n",
    "        \n",
    "        # 2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„\n",
    "        morphed_tokens = okt.morphs(cleaned_text)\n",
    "        \n",
    "        # 3ë‹¨ê³„: í† í°ë“¤ì„ ê³µë°±ìœ¼ë¡œ ê²°í•©\n",
    "        morphed_text = \" \".join(morphed_tokens)\n",
    "        \n",
    "        processed_texts.append(morphed_text)\n",
    "    \n",
    "    # Python ë¦¬ìŠ¤íŠ¸ë¥¼ TensorFlow í…ì„œë¡œ ë³€í™˜\n",
    "    return tf.constant(processed_texts, dtype=tf.string)\n",
    "\n",
    "print(\"ë°°ì¹˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TensorFlow í†µí•© ì „ì²˜ë¦¬ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_korean_preprocess_fn(texts, labels):\n",
    "    \"\"\"\n",
    "    TensorFlow Datasetì—ì„œ ì‚¬ìš©í•  ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "    \n",
    "    tf.py_functionì„ ì‚¬ìš©í•˜ì—¬ Python í•¨ìˆ˜ë¥¼ TensorFlow ê·¸ë˜í”„ì— í†µí•©í•©ë‹ˆë‹¤.\n",
    "    ì´ë¥¼ í†µí•´ KoNLPyì™€ ê°™ì€ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ TensorFlow íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        texts (tf.Tensor): í…ìŠ¤íŠ¸ ë°°ì¹˜\n",
    "        labels (tf.Tensor): ë¼ë²¨ ë°°ì¹˜\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸, ë¼ë²¨)\n",
    "    \"\"\"\n",
    "    # ì˜¤íƒ€ ìˆ˜ì •: tf.py_func -> tf.py_function\n",
    "    processed_texts = tf.py_function(\n",
    "        func=python_korean_preprocess,  # ì‹¤í–‰í•  Python í•¨ìˆ˜\n",
    "        inp=[texts],                    # ì…ë ¥ ë°ì´í„°\n",
    "        Tout=tf.string                  # ì¶œë ¥ ë°ì´í„° íƒ€ì…\n",
    "    )\n",
    "    \n",
    "    # ëª…ì‹œì ìœ¼ë¡œ shape ì„¤ì • (TensorFlow ìš”êµ¬ì‚¬í•­)\n",
    "    processed_texts.set_shape(texts.get_shape())\n",
    "    \n",
    "    return processed_texts, labels\n",
    "\n",
    "print(\"TensorFlow í†µí•© ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í…ìŠ¤íŠ¸ ë²¡í„°í™” ì„¤ì •\n",
    "\n",
    "### 6.1 TextVectorization ë ˆì´ì–´ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„°í™” íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "MAX_TOKENS = 10000          # ì–´íœ˜ ì‚¬ì „ í¬ê¸° (ìƒìœ„ 10,000ê°œ ë¹ˆë„ ë‹¨ì–´)\n",
    "OUTPUT_SEQUENCE_LENGTH = 20 # ì¶œë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (íŒ¨ë”©/ìë¥´ê¸°)\n",
    "\n",
    "print(f\"ë²¡í„°í™” ì„¤ì •:\")\n",
    "print(f\"  ìµœëŒ€ í† í° ìˆ˜: {MAX_TOKENS:,}\")\n",
    "print(f\"  ì‹œí€€ìŠ¤ ê¸¸ì´: {OUTPUT_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# TextVectorization ë ˆì´ì–´ ìƒì„±\n",
    "# ì˜¤íƒ€ ìˆ˜ì •: standarize -> standardize\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",                    # ì •ìˆ˜ ì‹œí€€ìŠ¤ ì¶œë ¥\n",
    "    output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "    standardize=None,                     # ì™¸ë¶€ì—ì„œ í‘œì¤€í™” ìˆ˜í–‰\n",
    "    split=\"whitespace\"                    # ê³µë°± ê¸°ì¤€ í† í° ë¶„ë¦¬\n",
    ")\n",
    "\n",
    "print(\"TextVectorization ë ˆì´ì–´ ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì ìš© ì¤‘...\")\n",
    "    \n",
    "    # ì˜¤íƒ€ ìˆ˜ì •: num_paralle_calls -> num_parallel_calls\n",
    "    # ëª¨ë“  ë°ì´í„°ì…‹ì— í•œêµ­ì–´ ì „ì²˜ë¦¬ ì ìš©\n",
    "    train_ds_processed = train_ds_raw.map(\n",
    "        tf_korean_preprocess_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    val_ds_processed = val_ds_raw.map(\n",
    "        tf_korean_preprocess_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    test_ds_processed = test_ds_raw.map(\n",
    "        tf_korean_preprocess_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    print(\"ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n",
    "    print(\"\\nì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:\")\n",
    "    for texts, labels in train_ds_processed.take(1):\n",
    "        print(f\"ì›ë³¸ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼: {texts[0].numpy().decode('utf-8')}\")\n",
    "        print(f\"ë¼ë²¨: {labels[0].numpy()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ë°ì´í„°ì…‹ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• ë° ë²¡í„°í™”\n",
    "\n",
    "### 7.1 ì–´íœ˜ ì‚¬ì „ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"ì–´íœ˜ ì‚¬ì „ í•™ìŠµ ì¤‘...\")\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„°ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ì–´íœ˜ ì‚¬ì „ í•™ìŠµ\n",
    "    # lambda í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸(x)ë§Œ ì„ íƒ, ë¼ë²¨(y)ì€ ë¬´ì‹œ\n",
    "    text_only_dataset = train_ds_processed.map(lambda x, y: x)\n",
    "    \n",
    "    # ì–´íœ˜ ì‚¬ì „ í•™ìŠµ (adapt)\n",
    "    vectorizer.adapt(text_only_dataset)\n",
    "    \n",
    "    print(\"ì–´íœ˜ ì‚¬ì „ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ì–´íœ˜ ì‚¬ì „ ì •ë³´ í™•ì¸\n",
    "    vocabulary = vectorizer.get_vocabulary()\n",
    "    print(f\"\\nì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(vocabulary):,}\")\n",
    "    print(\"ìƒìœ„ 20ê°œ ì–´íœ˜:\")\n",
    "    for i, word in enumerate(vocabulary[:20]):\n",
    "        print(f\"  {i:2d}: '{word}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ì–´íœ˜ ì‚¬ì „ í•™ìŠµ ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 ë²¡í„°í™” í•¨ìˆ˜ ë° ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_fn(texts, labels):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë²¡í„°í™” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        texts (tf.Tensor): ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë°°ì¹˜\n",
    "        labels (tf.Tensor): ë¼ë²¨ ë°°ì¹˜\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ë²¡í„°í™”ëœ í…ìŠ¤íŠ¸, ë¼ë²¨)\n",
    "    \"\"\"\n",
    "    return vectorizer(texts), labels\n",
    "\n",
    "try:\n",
    "    print(\"í…ìŠ¤íŠ¸ ë²¡í„°í™” ì ìš© ì¤‘...\")\n",
    "    \n",
    "    # ë²¡í„°í™” ì ìš©\n",
    "    train_ds_vectorized = train_ds_processed.map(\n",
    "        vectorize_text_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    val_ds_vectorized = val_ds_processed.map(\n",
    "        vectorize_text_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    test_ds_vectorized = test_ds_processed.map(\n",
    "        vectorize_text_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    print(\"ë²¡í„°í™” ì™„ë£Œ!\")\n",
    "    \n",
    "    # ë²¡í„°í™” ê²°ê³¼ í™•ì¸\n",
    "    print(\"\\në²¡í„°í™” ê²°ê³¼ ìƒ˜í”Œ:\")\n",
    "    for vectors, labels in train_ds_vectorized.take(1):\n",
    "        print(f\"ë²¡í„° í˜•íƒœ: {vectors.shape}\")\n",
    "        print(f\"ì²« ë²ˆì§¸ ë²¡í„°: {vectors[0].numpy()}\")\n",
    "        print(f\"ë¼ë²¨: {labels[0].numpy()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ë²¡í„°í™” ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì„±ëŠ¥ ìµœì í™” ì ìš©\n",
    "\n",
    "### 8.1 ìºì‹± ë° í”„ë¦¬í˜ì¹­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"ì„±ëŠ¥ ìµœì í™” ì ìš© ì¤‘...\")\n",
    "    \n",
    "    # ìºì‹±ê³¼ í”„ë¦¬í˜ì¹­ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”\n",
    "    # - cache(): ë©”ëª¨ë¦¬ì— ë°ì´í„°ë¥¼ ìºì‹œí•˜ì—¬ ë°˜ë³µ ì ‘ê·¼ ì‹œ ì„±ëŠ¥ í–¥ìƒ\n",
    "    # - prefetch(): ë‹¤ìŒ ë°°ì¹˜ë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•˜ì—¬ GPU ìœ íœ´ ì‹œê°„ ìµœì†Œí™”\n",
    "    \n",
    "    train_ds_final = train_ds_vectorized.cache().prefetch(\n",
    "        buffer_size=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    val_ds_final = val_ds_vectorized.cache().prefetch(\n",
    "        buffer_size=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    test_ds_final = test_ds_vectorized.cache().prefetch(\n",
    "        buffer_size=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    print(\"ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!\")\n",
    "    \n",
    "    print(\"\\n=== ìµœì¢… ë°ì´í„°ì…‹ ì •ë³´ ===\")\n",
    "    print(\"ëª¨ë“  ë°ì´í„°ì…‹ì´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê±°ì³ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "    print(\"  1. ì›ë³¸ í…ìŠ¤íŠ¸ ë¡œë“œ\")\n",
    "    print(\"  2. í•œê¸€ í…ìŠ¤íŠ¸ ì •ì œ ë° í˜•íƒœì†Œ ë¶„ì„\")\n",
    "    print(\"  3. í…ìŠ¤íŠ¸ ë²¡í„°í™” (ì •ìˆ˜ ì‹œí€€ìŠ¤ ë³€í™˜)\")\n",
    "    print(\"  4. ìºì‹± ë° í”„ë¦¬í˜ì¹­ ìµœì í™”\")\n",
    "    print(\"\\nì´ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ì„±ëŠ¥ ìµœì í™” ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦\n",
    "\n",
    "### 9.1 ìµœì¢… ë°ì´í„° í˜•íƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== ìµœì¢… ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦ ===\")\n",
    "    \n",
    "    def validate_dataset(dataset, name):\n",
    "        print(f\"\\n{name} ê²€ì¦:\")\n",
    "        \n",
    "        batch_count = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for vectors, labels in dataset.take(3):  # ì²« 3ê°œ ë°°ì¹˜ë§Œ í™•ì¸\n",
    "            batch_count += 1\n",
    "            batch_size = vectors.shape[0]\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            print(f\"  ë°°ì¹˜ {batch_count}: {batch_size}ê°œ ìƒ˜í”Œ\")\n",
    "            print(f\"    ë²¡í„° í˜•íƒœ: {vectors.shape}\")\n",
    "            print(f\"    ë¼ë²¨ í˜•íƒœ: {labels.shape}\")\n",
    "            print(f\"    ë²¡í„° ë°ì´í„° íƒ€ì…: {vectors.dtype}\")\n",
    "            print(f\"    ë¼ë²¨ ë°ì´í„° íƒ€ì…: {labels.dtype}\")\n",
    "            \n",
    "            # ìƒ˜í”Œ ë²¡í„° í™•ì¸\n",
    "            sample_vector = vectors[0].numpy()\n",
    "            non_zero_count = np.count_nonzero(sample_vector)\n",
    "            print(f\"    ìƒ˜í”Œ ë²¡í„° ìœ íš¨ í† í° ìˆ˜: {non_zero_count}/{len(sample_vector)}\")\n",
    "            print(f\"    ìƒ˜í”Œ ë¼ë²¨: {labels[0].numpy()}\")\n",
    "        \n",
    "        print(f\"  í™•ì¸í•œ ì´ ìƒ˜í”Œ ìˆ˜: {total_samples}\")\n",
    "    \n",
    "    # ê° ë°ì´í„°ì…‹ ê²€ì¦\n",
    "    validate_dataset(train_ds_final, \"í›ˆë ¨ ë°ì´í„°ì…‹\")\n",
    "    validate_dataset(val_ds_final, \"ê²€ì¦ ë°ì´í„°ì…‹\") \n",
    "    validate_dataset(test_ds_final, \"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 ì²˜ë¦¬ ê³¼ì • ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìš”ì•½ ===\")\n",
    "print()\n",
    "print(\"ğŸ” 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘\")\n",
    "print(\"   - Korporaë¥¼ í†µí•œ NSMC ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\")\n",
    "print(\"   - ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° 150,000ê°œ + í…ŒìŠ¤íŠ¸ 50,000ê°œ\")\n",
    "print()\n",
    "print(\"ğŸ“ 2ë‹¨ê³„: ë°ì´í„° êµ¬ì¡°í™”\")\n",
    "print(\"   - train/val/test ë¶„í• \")\n",
    "print(\"   - pos(ê¸ì •)/neg(ë¶€ì •) ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\")\n",
    "print(\"   - ê° ë¦¬ë·°ë¥¼ ê°œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\")\n",
    "print()\n",
    "print(\"ğŸ“Š 3ë‹¨ê³„: TensorFlow Dataset ë¡œë“œ\")\n",
    "print(\"   - text_dataset_from_directory ì‚¬ìš©\")\n",
    "print(\"   - ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ë¡œë”©\")\n",
    "print(\"   - ì´ì§„ ë¶„ë¥˜ìš© ë¼ë²¨ ì„¤ì •\")\n",
    "print()\n",
    "print(\"ğŸ”§ 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\")\n",
    "print(\"   - UTF-8 ë””ì½”ë”© (TensorFlow tensor â†’ Python string)\")\n",
    "print(\"   - íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜\")\n",
    "print(\"   - KoNLPy Oktë¥¼ í†µí•œ í˜•íƒœì†Œ ë¶„ì„\")\n",
    "print(\"   - tf.py_functionìœ¼ë¡œ TensorFlowì™€ í†µí•©\")\n",
    "print()\n",
    "print(\"ğŸ”¢ 5ë‹¨ê³„: í…ìŠ¤íŠ¸ ë²¡í„°í™”\")\n",
    "print(\"   - TextVectorizationìœ¼ë¡œ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\")\n",
    "print(\"   - í…ìŠ¤íŠ¸ â†’ ì •ìˆ˜ ì‹œí€€ìŠ¤ ë³€í™˜\")\n",
    "print(\"   - ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼ (íŒ¨ë”©/ìë¥´ê¸°)\")\n",
    "print()\n",
    "print(\"âš¡ 6ë‹¨ê³„: ì„±ëŠ¥ ìµœì í™”\")\n",
    "print(\"   - ìºì‹±: ë°˜ë³µ ì ‘ê·¼ ì‹œ ë©”ëª¨ë¦¬ì—ì„œ ë¹ ë¥¸ ë¡œë“œ\")\n",
    "print(\"   - í”„ë¦¬í˜ì¹­: GPU ìœ íœ´ ì‹œê°„ ìµœì†Œí™”\")\n",
    "print(\"   - ë³‘ë ¬ ì²˜ë¦¬: ë©€í‹°ì½”ì–´ CPU í™œìš©\")\n",
    "print()\n",
    "print(\"âœ… ê²°ê³¼: ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"   - ì…ë ¥ í˜•íƒœ: (ë°°ì¹˜í¬ê¸°={BATCH_SIZE}, ì‹œí€€ìŠ¤ê¸¸ì´={OUTPUT_SEQUENCE_LENGTH})\")\n",
    "print(f\"   - ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {MAX_TOKENS:,}ê°œ\")\n",
    "print(\"   - ë¼ë²¨: 0(ë¶€ì •), 1(ê¸ì •)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
    "\n",
    "ì´ì œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ëª¨ë¸ êµ¬ì¶• ì˜ˆì‹œ ì½”ë“œ:\n",
    "\n",
    "```python\n",
    "# ê°„ë‹¨í•œ ê°ì • ë¶„ì„ ëª¨ë¸ ì˜ˆì‹œ\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(MAX_TOKENS, 128),\n",
    "    keras.layers.LSTM(64, dropout=0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í›ˆë ¨\n",
    "model.fit(\n",
    "    train_ds_final,\n",
    "    validation_data=val_ds_final,\n",
    "    epochs=5\n",
    ")\n",
    "```\n",
    "\n",
    "### ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:\n",
    "- `train_ds_final`: í›ˆë ¨ìš© ë°ì´í„°ì…‹\n",
    "- `val_ds_final`: ê²€ì¦ìš© ë°ì´í„°ì…‹  \n",
    "- `test_ds_final`: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹\n",
    "\n",
    "ëª¨ë“  ë°ì´í„°ì…‹ì€ ë²¡í„°í™”ë˜ì–´ ìˆìœ¼ë©° ì„±ëŠ¥ ìµœì í™”ê°€ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
