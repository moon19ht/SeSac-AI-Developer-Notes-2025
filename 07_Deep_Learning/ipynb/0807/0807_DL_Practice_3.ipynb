{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 영화 리뷰 감정 분석을 위한 데이터 파이프라인\n",
    "\n",
    "## 개요\n",
    "이 노트북에서는 네이버 영화 리뷰 데이터(NSMC)를 사용하여 감정 분석을 위한 완전한 데이터 파이프라인을 구축합니다.\n",
    "\n",
    "### 주요 학습 내용:\n",
    "1. **Korpora 라이브러리**를 통한 NSMC 데이터셋 다운로드\n",
    "2. **데이터 구조화**: train/validation/test 분할 및 파일 시스템 구조 생성\n",
    "3. **TensorFlow Dataset API**를 활용한 대용량 데이터 처리\n",
    "4. **한글 텍스트 전처리**: KoNLPy + TensorFlow 통합\n",
    "5. **벡터화 파이프라인**: 실시간 데이터 변환\n",
    "6. **성능 최적화**: 캐싱, 프리페칭, 병렬 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 환경 설정\n",
    "\n",
    "### 1.1 필수 라이브러리 설치\n",
    "```bash\n",
    "# Korpora 라이브러리 설치 (conda로 설치가 안되므로 pip 사용)\n",
    "pip install Korpora\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(f\"Python 버전: {tf.version.VERSION}\")\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "print(f\"GPU 사용 가능: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NSMC 데이터셋 다운로드 및 탐색\n",
    "\n",
    "### 2.1 Korpora를 통한 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Korpora import Korpora\n",
    "\n",
    "# 데이터 저장 경로 설정\n",
    "data_dir = \"../../data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# NSMC(Naver Sentiment Movie Corpus) 데이터셋 다운로드\n",
    "# 기본 경로 대신 지정한 경로에 저장\n",
    "print(\"NSMC 데이터셋 다운로드 중...\")\n",
    "print(f\"저장 경로: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# root_dir 파라미터를 사용하여 저장 위치 지정\n",
    "Korpora.fetch(\"nsmc\", root_dir=data_dir)\n",
    "print(\"다운로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 데이터셋 로드 및 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "\n",
    "# 데이터 구조 탐색\n",
    "print(\"=== NSMC 데이터셋 정보 ===\")\n",
    "print(f\"훈련 데이터 크기: {len(corpus.train.texts)}\")\n",
    "print(f\"테스트 데이터 크기: {len(corpus.test.texts)}\")\n",
    "print()\n",
    "\n",
    "print(\"=== 샘플 데이터 (첫 3개) ===\")\n",
    "for i in range(3):\n",
    "    print(f\"샘플 {i+1}:\")\n",
    "    print(f\"  텍스트: {corpus.train.texts[i]}\")\n",
    "    print(f\"  라벨: {corpus.train.labels[i]} ({'긍정' if corpus.train.labels[i] == 1 else '부정'})\")  # 정수 1과 비교\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 라벨링 검증 및 원본 데이터 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 파일을 직접 읽어서 확인해보자\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 원본 파일 직접 확인 ===\")\n",
    "try:\n",
    "    # 원본 TSV 파일 직접 읽기\n",
    "    train_file_path = os.path.join(data_dir, \"nsmc\", \"ratings_train.txt\")\n",
    "    \n",
    "    # 첫 몇 줄을 직접 읽어보기\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:10]  # 첫 10줄\n",
    "    \n",
    "    print(\"원본 파일 첫 10줄:\")\n",
    "    for i, line in enumerate(lines):\n",
    "        print(f\"{i}: {line.strip()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # pandas로 정확히 파싱해보기\n",
    "    df_train = pd.read_csv(train_file_path, sep='\\t', encoding='utf-8')\n",
    "    print(f\"pandas로 읽은 데이터:\")\n",
    "    print(f\"컬럼명: {list(df_train.columns)}\")\n",
    "    print(f\"데이터 형태: {df_train.shape}\")\n",
    "    print(\"\\n첫 5개 샘플:\")\n",
    "    print(df_train.head())\n",
    "    \n",
    "    print(f\"\\n라벨 분포:\")\n",
    "    print(df_train['label'].value_counts().sort_index())\n",
    "    \n",
    "    # Korpora 데이터와 비교\n",
    "    print(f\"\\n=== Korpora vs 원본 데이터 비교 ===\")\n",
    "    print(\"첫 5개 샘플 비교:\")\n",
    "    for i in range(5):\n",
    "        korpora_text = corpus.train.texts[i]\n",
    "        korpora_label = corpus.train.labels[i]\n",
    "        pandas_text = df_train.iloc[i]['document']\n",
    "        pandas_label = df_train.iloc[i]['label']\n",
    "        \n",
    "        print(f\"\\n샘플 {i+1}:\")\n",
    "        print(f\"  Korpora: '{korpora_text}' | 라벨: {korpora_label}\")\n",
    "        print(f\"  원본:    '{pandas_text}' | 라벨: {pandas_label}\")\n",
    "        print(f\"  매치: {korpora_text == pandas_text and korpora_label == pandas_label}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"원본 파일 읽기 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링 문제 분석: 샘플들을 더 자세히 살펴보자\n",
    "print(\"=== 의심스러운 라벨링 샘플 분석 ===\")\n",
    "\n",
    "# 명백히 긍정적인 표현들을 가진 부정 라벨 샘플들 찾기\n",
    "positive_words = ['좋', '재미', '추천', '최고', '훌륭', '감동', '멋진', '완벽']\n",
    "negative_words = ['짜증', '별로', '최악', '실망', '지루', '재미없', '아깝']\n",
    "\n",
    "print(\"부정 라벨(0)인데 긍정 단어가 포함된 샘플들:\")\n",
    "positive_in_negative = []\n",
    "for i in range(min(1000, len(corpus.train.texts))):  # 첫 1000개만 확인\n",
    "    text = corpus.train.texts[i]\n",
    "    label = corpus.train.labels[i]\n",
    "    \n",
    "    if label == 0:  # 부정 라벨\n",
    "        if any(pos_word in text for pos_word in positive_words):\n",
    "            positive_in_negative.append((i, text, label))\n",
    "            if len(positive_in_negative) <= 5:  # 처음 5개만 출력\n",
    "                print(f\"  {i}: [{label}] {text}\")\n",
    "\n",
    "print(f\"\\n총 {len(positive_in_negative)}개 발견\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"긍정 라벨(1)인데 부정 단어가 포함된 샘플들:\")\n",
    "negative_in_positive = []\n",
    "for i in range(min(1000, len(corpus.train.texts))):\n",
    "    text = corpus.train.texts[i]\n",
    "    label = corpus.train.labels[i]\n",
    "    \n",
    "    if label == 1:  # 긍정 라벨\n",
    "        if any(neg_word in text for neg_word in negative_words):\n",
    "            negative_in_positive.append((i, text, label))\n",
    "            if len(negative_in_positive) <= 5:  # 처음 5개만 출력\n",
    "                print(f\"  {i}: [{label}] {text}\")\n",
    "\n",
    "print(f\"\\n총 {len(negative_in_positive)}개 발견\")\n",
    "\n",
    "# 전체 라벨 분포 다시 확인\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== 전체 라벨 분포 확인 ===\")\n",
    "pos_count = sum(1 for label in corpus.train.labels if label == 1)\n",
    "neg_count = sum(1 for label in corpus.train.labels if label == 0)\n",
    "total = len(corpus.train.labels)\n",
    "\n",
    "print(f\"긍정(1): {pos_count:,}개 ({pos_count/total*100:.1f}%)\")\n",
    "print(f\"부정(0): {neg_count:,}개 ({neg_count/total*100:.1f}%)\")\n",
    "print(f\"총 샘플: {total:,}개\")\n",
    "\n",
    "# 데이터 타입과 고유값 확인\n",
    "print(f\"\\n라벨 데이터 타입: {type(corpus.train.labels[0])}\")\n",
    "print(f\"라벨 고유값: {set(corpus.train.labels)}\")\n",
    "\n",
    "print(\"\\n=== 결론 ===\")\n",
    "print(\"NSMC 데이터셋은 실제로 노이즈가 있는 데이터셋으로 알려져 있습니다.\")\n",
    "print(\"일부 라벨링 오류가 존재할 수 있으며, 이는 실제 데이터에서 흔히 발생하는 문제입니다.\")\n",
    "print(\"모델 훈련 시에는 이러한 노이즈를 고려하여 진행해야 합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 구조화 및 파일 시스템 준비\n",
    "\n",
    "TensorFlow의 `text_dataset_from_directory`를 사용하기 위해 데이터를 디렉토리 구조로 정리합니다.\n",
    "\n",
    "### 3.1 데이터 구조화 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_korean_dataset(base_dir=None, validation_size=1000):\n",
    "    \"\"\"\n",
    "    NSMC 데이터를 TensorFlow Dataset 형식의 디렉토리 구조로 변환\n",
    "    \n",
    "    구조:\n",
    "    ../../data/korean_imdb/\n",
    "    ├── train/\n",
    "    │   ├── pos/  (긍정 리뷰들)\n",
    "    │   └── neg/  (부정 리뷰들)\n",
    "    ├── val/\n",
    "    │   ├── pos/\n",
    "    │   └── neg/\n",
    "    └── test/\n",
    "        ├── pos/\n",
    "        └── neg/\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): 디렉토리 이름 (기본값: None이면 data_dir/korean_imdb 사용)\n",
    "        validation_size (int): 검증 데이터 크기 (각 클래스별)\n",
    "    \n",
    "    Returns:\n",
    "        str: 생성된 데이터 디렉토리 경로\n",
    "    \"\"\"\n",
    "    \n",
    "    if base_dir is None:\n",
    "        base_dir = os.path.join(data_dir, \"korean_imdb\")\n",
    "    \n",
    "    print(f\"데이터 구조화 시작: {base_dir}\")\n",
    "    \n",
    "    # 기존 디렉토리 삭제 (있다면)\n",
    "    if os.path.exists(base_dir):\n",
    "        try:\n",
    "            shutil.rmtree(base_dir)\n",
    "            print(f\"기존 디렉토리 삭제: {base_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"디렉토리 삭제 오류: {e}\")\n",
    "    \n",
    "    # 디렉토리 구조 생성\n",
    "    directories = [\n",
    "        os.path.join(base_dir, \"train\", \"pos\"),\n",
    "        os.path.join(base_dir, \"train\", \"neg\"),\n",
    "        os.path.join(base_dir, \"val\", \"pos\"),\n",
    "        os.path.join(base_dir, \"val\", \"neg\"),\n",
    "        os.path.join(base_dir, \"test\", \"pos\"),\n",
    "        os.path.join(base_dir, \"test\", \"neg\")\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    print(\"디렉토리 구조 생성 완료\")\n",
    "    \n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 데이터 분할 및 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_files(corpus, base_dir, validation_size=1000):\n",
    "    \"\"\"\n",
    "    corpus 데이터를 파일로 저장\n",
    "    \"\"\"\n",
    "    \n",
    "    # 훈련 데이터 라벨별 분리\n",
    "    print(\"훈련 데이터 분리 중...\")\n",
    "    pos_train_texts = []\n",
    "    neg_train_texts = []\n",
    "    \n",
    "    for text, label in zip(corpus.train.texts, corpus.train.labels):\n",
    "        if label == 1:  # 정수 1로 비교\n",
    "            pos_train_texts.append(text)\n",
    "        elif label == 0:  # 정수 0로 비교\n",
    "            neg_train_texts.append(text)\n",
    "    \n",
    "    print(f\"긍정 훈련 데이터: {len(pos_train_texts):,}개\")\n",
    "    print(f\"부정 훈련 데이터: {len(neg_train_texts):,}개\")\n",
    "    \n",
    "    # 테스트 데이터 라벨별 분리\n",
    "    print(\"테스트 데이터 분리 중...\")\n",
    "    pos_test_texts = []\n",
    "    neg_test_texts = []\n",
    "    \n",
    "    for text, label in zip(corpus.test.texts, corpus.test.labels):\n",
    "        if label == 1:  # 정수 1로 비교\n",
    "            pos_test_texts.append(text)\n",
    "        elif label == 0:  # 정수 0로 비교\n",
    "            neg_test_texts.append(text)\n",
    "    \n",
    "    print(f\"긍정 테스트 데이터: {len(pos_test_texts):,}개\")\n",
    "    print(f\"부정 테스트 데이터: {len(neg_test_texts):,}개\")\n",
    "    \n",
    "    # 데이터가 충분한지 확인\n",
    "    if len(pos_train_texts) < validation_size or len(neg_train_texts) < validation_size:\n",
    "        print(f\"경고: 검증 데이터 분할을 위한 충분한 데이터가 없습니다.\")\n",
    "        validation_size = min(len(pos_train_texts), len(neg_train_texts)) // 2\n",
    "        print(f\"검증 데이터 크기를 {validation_size}개로 조정합니다.\")\n",
    "    \n",
    "    # 검증 데이터 분할 (훈련 데이터에서 일부 추출)\n",
    "    print(f\"검증 데이터 분할 중... (각 클래스별 {validation_size}개)\")\n",
    "    pos_val_texts = pos_train_texts[:validation_size] if pos_train_texts else []\n",
    "    neg_val_texts = neg_train_texts[:validation_size] if neg_train_texts else []\n",
    "    \n",
    "    # 훈련 데이터에서 검증 데이터 제거\n",
    "    pos_train_texts = pos_train_texts[validation_size:] if pos_train_texts else []\n",
    "    neg_train_texts = neg_train_texts[validation_size:] if neg_train_texts else []\n",
    "    \n",
    "    print(f\"최종 훈련 데이터 - 긍정: {len(pos_train_texts):,}개, 부정: {len(neg_train_texts):,}개\")\n",
    "    print(f\"검증 데이터 - 긍정: {len(pos_val_texts):,}개, 부정: {len(neg_val_texts):,}개\")\n",
    "    \n",
    "    # 파일 저장 함수\n",
    "    def save_texts_to_files(texts, directory, prefix):\n",
    "        os.makedirs(directory, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "        for i, text in enumerate(texts):\n",
    "            filename = f\"{prefix}_{i}.txt\"\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "    \n",
    "    # 훈련 데이터 저장\n",
    "    print(\"훈련 데이터 파일 저장 중...\")\n",
    "    if pos_train_texts:\n",
    "        save_texts_to_files(pos_train_texts, os.path.join(base_dir, \"train\", \"pos\"), \"pos\")\n",
    "    if neg_train_texts:\n",
    "        save_texts_to_files(neg_train_texts, os.path.join(base_dir, \"train\", \"neg\"), \"neg\")\n",
    "    \n",
    "    # 검증 데이터 저장\n",
    "    print(\"검증 데이터 파일 저장 중...\")\n",
    "    if pos_val_texts:\n",
    "        save_texts_to_files(pos_val_texts, os.path.join(base_dir, \"val\", \"pos\"), \"pos\")\n",
    "    if neg_val_texts:\n",
    "        save_texts_to_files(neg_val_texts, os.path.join(base_dir, \"val\", \"neg\"), \"neg\")\n",
    "    \n",
    "    # 테스트 데이터 저장\n",
    "    print(\"테스트 데이터 파일 저장 중...\")\n",
    "    if pos_test_texts:\n",
    "        save_texts_to_files(pos_test_texts, os.path.join(base_dir, \"test\", \"pos\"), \"pos\")\n",
    "    if neg_test_texts:\n",
    "        save_texts_to_files(neg_test_texts, os.path.join(base_dir, \"test\", \"neg\"), \"neg\")\n",
    "    \n",
    "    print(\"모든 데이터 파일 저장 완료!\")\n",
    "    return {\n",
    "        'train_pos': len(pos_train_texts),\n",
    "        'train_neg': len(neg_train_texts), \n",
    "        'val_pos': len(pos_val_texts),\n",
    "        'val_neg': len(neg_val_texts),\n",
    "        'test_pos': len(pos_test_texts),\n",
    "        'test_neg': len(neg_test_texts)\n",
    "    }\n",
    "\n",
    "# 실행 (시간이 오래 걸릴 수 있습니다)\n",
    "korean_data_dir = create_korean_dataset()\n",
    "data_stats = save_data_to_files(corpus, korean_data_dir)\n",
    "\n",
    "print(f\"데이터 디렉토리: {korean_data_dir}\")\n",
    "print(f\"데이터 통계: {data_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorFlow Dataset 로드\n",
    "\n",
    "### 4.1 데이터셋 초기 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 크기 설정\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"배치 크기: {BATCH_SIZE}\")\n",
    "print(\"TensorFlow Dataset 로드 중...\")\n",
    "\n",
    "try:\n",
    "    # text_dataset_from_directory를 사용한 데이터셋 로드\n",
    "    # 오타 수정: dataset_from_directory -> text_dataset_from_directory\n",
    "    train_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "        korean_data_dir + \"/train\", \n",
    "        batch_size=BATCH_SIZE, \n",
    "        label_mode=\"binary\",\n",
    "        validation_split=None\n",
    "    )\n",
    "    \n",
    "    val_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "        korean_data_dir + \"/val\", \n",
    "        batch_size=BATCH_SIZE, \n",
    "        label_mode=\"binary\",\n",
    "        validation_split=None\n",
    "    )\n",
    "    \n",
    "    test_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "        korean_data_dir + \"/test\", \n",
    "        batch_size=BATCH_SIZE, \n",
    "        label_mode=\"binary\",\n",
    "        validation_split=None\n",
    "    )\n",
    "    \n",
    "    print(\"데이터셋 로드 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"데이터셋 로드 실패: {e}\")\n",
    "    print(\"먼저 데이터 구조화를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 로드된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 구조 확인\n",
    "print(\"=== 데이터셋 정보 ===\")\n",
    "\n",
    "def get_dataset_info(dataset, name):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  요소 스펙: {dataset.element_spec}\")\n",
    "    \n",
    "    # 첫 번째 배치 가져오기\n",
    "    for texts, labels in dataset.take(1):\n",
    "        print(f\"  배치 크기: {texts.shape[0]}\")\n",
    "        print(f\"  텍스트 형태: {texts.shape}\")\n",
    "        print(f\"  라벨 형태: {labels.shape}\")\n",
    "        print(f\"  첫 번째 텍스트 샘플: {texts[0].numpy().decode('utf-8')[:100]}...\")\n",
    "        print(f\"  첫 번째 라벨: {labels[0].numpy()}\")\n",
    "    print()\n",
    "\n",
    "try:\n",
    "    get_dataset_info(train_ds_raw, \"훈련 데이터셋\")\n",
    "    get_dataset_info(val_ds_raw, \"검증 데이터셋\")\n",
    "    get_dataset_info(test_ds_raw, \"테스트 데이터셋\")\n",
    "except:\n",
    "    print(\"데이터셋이 로드되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 한국어 텍스트 전처리 파이프라인\n",
    "\n",
    "### 5.1 텍스트 정제 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "print(\"KoNLPy Okt 형태소 분석기 초기화 완료\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    텍스트 정제 함수\n",
    "    \n",
    "    주의: TensorFlow에서 전달되는 텍스트는 bytes 형태로 인코딩되어 있으므로\n",
    "    먼저 UTF-8로 디코딩해야 합니다.\n",
    "    \n",
    "    Args:\n",
    "        text (bytes): TensorFlow에서 전달된 인코딩된 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        str: 정제된 텍스트\n",
    "    \"\"\"\n",
    "    # TensorFlow tensor를 Python string으로 디코딩\n",
    "    text = text.decode(\"utf-8\")\n",
    "    \n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 한글, 영문, 숫자, 공백만 유지\n",
    "    text = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 테스트\n",
    "test_text = \"이 영화는 정말 좋았어요! 5점 만점에 5점★★★\"\n",
    "test_bytes = test_text.encode('utf-8')\n",
    "cleaned = clean_text(test_bytes)\n",
    "print(f\"원본: {test_text}\")\n",
    "print(f\"정제: {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 배치 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_korean_preprocess(text_tensor):\n",
    "    \"\"\"\n",
    "    TensorFlow 텐서 배치에 대한 한국어 전처리\n",
    "    \n",
    "    이 함수는 tf.py_function을 통해 호출되며,\n",
    "    TensorFlow 텐서를 받아서 처리한 후 다시 텐서로 반환합니다.\n",
    "    \n",
    "    Args:\n",
    "        text_tensor (tf.Tensor): 텍스트 배치 텐서\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: 전처리된 텍스트 배치 텐서\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    \n",
    "    # 배치의 각 텍스트에 대해 처리\n",
    "    for text_bytes in text_tensor.numpy():\n",
    "        # 1단계: 텍스트 정제\n",
    "        cleaned_text = clean_text(text_bytes)\n",
    "        \n",
    "        # 2단계: 형태소 분석\n",
    "        morphed_tokens = okt.morphs(cleaned_text)\n",
    "        \n",
    "        # 3단계: 토큰들을 공백으로 결합\n",
    "        morphed_text = \" \".join(morphed_tokens)\n",
    "        \n",
    "        processed_texts.append(morphed_text)\n",
    "    \n",
    "    # Python 리스트를 TensorFlow 텐서로 변환\n",
    "    return tf.constant(processed_texts, dtype=tf.string)\n",
    "\n",
    "print(\"배치 전처리 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TensorFlow 통합 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_korean_preprocess_fn(texts, labels):\n",
    "    \"\"\"\n",
    "    TensorFlow Dataset에서 사용할 전처리 함수\n",
    "    \n",
    "    tf.py_function을 사용하여 Python 함수를 TensorFlow 그래프에 통합합니다.\n",
    "    이를 통해 KoNLPy와 같은 Python 라이브러리를 TensorFlow 파이프라인에서 사용할 수 있습니다.\n",
    "    \n",
    "    Args:\n",
    "        texts (tf.Tensor): 텍스트 배치\n",
    "        labels (tf.Tensor): 라벨 배치\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (전처리된 텍스트, 라벨)\n",
    "    \"\"\"\n",
    "    # 오타 수정: tf.py_func -> tf.py_function\n",
    "    processed_texts = tf.py_function(\n",
    "        func=python_korean_preprocess,  # 실행할 Python 함수\n",
    "        inp=[texts],                    # 입력 데이터\n",
    "        Tout=tf.string                  # 출력 데이터 타입\n",
    "    )\n",
    "    \n",
    "    # 명시적으로 shape 설정 (TensorFlow 요구사항)\n",
    "    processed_texts.set_shape(texts.get_shape())\n",
    "    \n",
    "    return processed_texts, labels\n",
    "\n",
    "print(\"TensorFlow 통합 전처리 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 텍스트 벡터화 설정\n",
    "\n",
    "### 6.1 TextVectorization 레이어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화 파라미터 설정\n",
    "MAX_TOKENS = 10000          # 어휘 사전 크기 (상위 10,000개 빈도 단어)\n",
    "OUTPUT_SEQUENCE_LENGTH = 20 # 출력 시퀀스 길이 (패딩/자르기)\n",
    "\n",
    "print(f\"벡터화 설정:\")\n",
    "print(f\"  최대 토큰 수: {MAX_TOKENS:,}\")\n",
    "print(f\"  시퀀스 길이: {OUTPUT_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# TextVectorization 레이어 생성\n",
    "# 오타 수정: standarize -> standardize\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",                    # 정수 시퀀스 출력\n",
    "    output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "    standardize=None,                     # 외부에서 표준화 수행\n",
    "    split=\"whitespace\"                    # 공백 기준 토큰 분리\n",
    ")\n",
    "\n",
    "print(\"TextVectorization 레이어 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 데이터 전처리 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"데이터 전처리 적용 중...\")\n",
    "    \n",
    "    # 오타 수정: num_paralle_calls -> num_parallel_calls\n",
    "    # 모든 데이터셋에 한국어 전처리 적용\n",
    "    train_ds_processed = train_ds_raw.map(\n",
    "        tf_korean_preprocess_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    val_ds_processed = val_ds_raw.map(\n",
    "        tf_korean_preprocess_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    test_ds_processed = test_ds_raw.map(\n",
    "        tf_korean_preprocess_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    print(\"전처리 완료!\")\n",
    "    \n",
    "    # 전처리 결과 확인\n",
    "    print(\"\\n전처리 결과 샘플:\")\n",
    "    for texts, labels in train_ds_processed.take(1):\n",
    "        print(f\"원본 형태소 분석 결과: {texts[0].numpy().decode('utf-8')}\")\n",
    "        print(f\"라벨: {labels[0].numpy()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"전처리 실패: {e}\")\n",
    "    print(\"데이터셋이 로드되지 않았거나 오류가 발생했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 어휘 사전 구축 및 벡터화\n",
    "\n",
    "### 7.1 어휘 사전 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"어휘 사전 학습 중...\")\n",
    "    \n",
    "    # 훈련 데이터의 텍스트만 추출하여 어휘 사전 학습\n",
    "    # lambda 함수를 사용해 텍스트(x)만 선택, 라벨(y)은 무시\n",
    "    text_only_dataset = train_ds_processed.map(lambda x, y: x)\n",
    "    \n",
    "    # 어휘 사전 학습 (adapt)\n",
    "    vectorizer.adapt(text_only_dataset)\n",
    "    \n",
    "    print(\"어휘 사전 학습 완료!\")\n",
    "    \n",
    "    # 어휘 사전 정보 확인\n",
    "    vocabulary = vectorizer.get_vocabulary()\n",
    "    print(f\"\\n어휘 사전 크기: {len(vocabulary):,}\")\n",
    "    print(\"상위 20개 어휘:\")\n",
    "    for i, word in enumerate(vocabulary[:20]):\n",
    "        print(f\"  {i:2d}: '{word}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"어휘 사전 학습 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 벡터화 함수 및 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_fn(texts, labels):\n",
    "    \"\"\"\n",
    "    텍스트 벡터화 함수\n",
    "    \n",
    "    Args:\n",
    "        texts (tf.Tensor): 전처리된 텍스트 배치\n",
    "        labels (tf.Tensor): 라벨 배치\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (벡터화된 텍스트, 라벨)\n",
    "    \"\"\"\n",
    "    return vectorizer(texts), labels\n",
    "\n",
    "try:\n",
    "    print(\"텍스트 벡터화 적용 중...\")\n",
    "    \n",
    "    # 벡터화 적용\n",
    "    train_ds_vectorized = train_ds_processed.map(\n",
    "        vectorize_text_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    val_ds_vectorized = val_ds_processed.map(\n",
    "        vectorize_text_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    test_ds_vectorized = test_ds_processed.map(\n",
    "        vectorize_text_fn, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    print(\"벡터화 완료!\")\n",
    "    \n",
    "    # 벡터화 결과 확인\n",
    "    print(\"\\n벡터화 결과 샘플:\")\n",
    "    for vectors, labels in train_ds_vectorized.take(1):\n",
    "        print(f\"벡터 형태: {vectors.shape}\")\n",
    "        print(f\"첫 번째 벡터: {vectors[0].numpy()}\")\n",
    "        print(f\"라벨: {labels[0].numpy()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"벡터화 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 성능 최적화 적용\n",
    "\n",
    "### 8.1 캐싱 및 프리페칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"성능 최적화 적용 중...\")\n",
    "    \n",
    "    # 캐싱과 프리페칭을 통한 성능 최적화\n",
    "    # - cache(): 메모리에 데이터를 캐시하여 반복 접근 시 성능 향상\n",
    "    # - prefetch(): 다음 배치를 미리 준비하여 GPU 유휴 시간 최소화\n",
    "    \n",
    "    train_ds_final = train_ds_vectorized.cache().prefetch(\n",
    "        buffer_size=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    val_ds_final = val_ds_vectorized.cache().prefetch(\n",
    "        buffer_size=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    test_ds_final = test_ds_vectorized.cache().prefetch(\n",
    "        buffer_size=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    print(\"성능 최적화 완료!\")\n",
    "    \n",
    "    print(\"\\n=== 최종 데이터셋 정보 ===\")\n",
    "    print(\"모든 데이터셋이 다음 단계를 거쳐 처리되었습니다:\")\n",
    "    print(\"  1. 원본 텍스트 로드\")\n",
    "    print(\"  2. 한글 텍스트 정제 및 형태소 분석\")\n",
    "    print(\"  3. 텍스트 벡터화 (정수 시퀀스 변환)\")\n",
    "    print(\"  4. 캐싱 및 프리페칭 최적화\")\n",
    "    print(\"\\n이제 딥러닝 모델 훈련에 사용할 준비가 완료되었습니다!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"성능 최적화 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 데이터 파이프라인 검증\n",
    "\n",
    "### 9.1 최종 데이터 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== 최종 데이터 파이프라인 검증 ===\")\n",
    "    \n",
    "    def validate_dataset(dataset, name):\n",
    "        print(f\"\\n{name} 검증:\")\n",
    "        \n",
    "        batch_count = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for vectors, labels in dataset.take(3):  # 첫 3개 배치만 확인\n",
    "            batch_count += 1\n",
    "            batch_size = vectors.shape[0]\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            print(f\"  배치 {batch_count}: {batch_size}개 샘플\")\n",
    "            print(f\"    벡터 형태: {vectors.shape}\")\n",
    "            print(f\"    라벨 형태: {labels.shape}\")\n",
    "            print(f\"    벡터 데이터 타입: {vectors.dtype}\")\n",
    "            print(f\"    라벨 데이터 타입: {labels.dtype}\")\n",
    "            \n",
    "            # 샘플 벡터 확인\n",
    "            sample_vector = vectors[0].numpy()\n",
    "            non_zero_count = np.count_nonzero(sample_vector)\n",
    "            print(f\"    샘플 벡터 유효 토큰 수: {non_zero_count}/{len(sample_vector)}\")\n",
    "            print(f\"    샘플 라벨: {labels[0].numpy()}\")\n",
    "        \n",
    "        print(f\"  확인한 총 샘플 수: {total_samples}\")\n",
    "    \n",
    "    # 각 데이터셋 검증\n",
    "    validate_dataset(train_ds_final, \"훈련 데이터셋\")\n",
    "    validate_dataset(val_ds_final, \"검증 데이터셋\") \n",
    "    validate_dataset(test_ds_final, \"테스트 데이터셋\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"데이터셋 검증 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 처리 과정 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 한국어 텍스트 처리 파이프라인 요약 ===\")\n",
    "print()\n",
    "print(\"🔍 1단계: 데이터 수집\")\n",
    "print(\"   - Korpora를 통한 NSMC 데이터셋 다운로드\")\n",
    "print(\"   - 네이버 영화 리뷰 150,000개 + 테스트 50,000개\")\n",
    "print()\n",
    "print(\"📁 2단계: 데이터 구조화\")\n",
    "print(\"   - train/val/test 분할\")\n",
    "print(\"   - pos(긍정)/neg(부정) 디렉토리 구조 생성\")\n",
    "print(\"   - 각 리뷰를 개별 텍스트 파일로 저장\")\n",
    "print()\n",
    "print(\"📊 3단계: TensorFlow Dataset 로드\")\n",
    "print(\"   - text_dataset_from_directory 사용\")\n",
    "print(\"   - 배치 단위 데이터 로딩\")\n",
    "print(\"   - 이진 분류용 라벨 설정\")\n",
    "print()\n",
    "print(\"🔧 4단계: 텍스트 전처리\")\n",
    "print(\"   - UTF-8 디코딩 (TensorFlow tensor → Python string)\")\n",
    "print(\"   - 특수문자 제거, 소문자 변환\")\n",
    "print(\"   - KoNLPy Okt를 통한 형태소 분석\")\n",
    "print(\"   - tf.py_function으로 TensorFlow와 통합\")\n",
    "print()\n",
    "print(\"🔢 5단계: 텍스트 벡터화\")\n",
    "print(\"   - TextVectorization으로 어휘 사전 구축\")\n",
    "print(\"   - 텍스트 → 정수 시퀀스 변환\")\n",
    "print(\"   - 시퀀스 길이 통일 (패딩/자르기)\")\n",
    "print()\n",
    "print(\"⚡ 6단계: 성능 최적화\")\n",
    "print(\"   - 캐싱: 반복 접근 시 메모리에서 빠른 로드\")\n",
    "print(\"   - 프리페칭: GPU 유휴 시간 최소화\")\n",
    "print(\"   - 병렬 처리: 멀티코어 CPU 활용\")\n",
    "print()\n",
    "print(\"✅ 결과: 딥러닝 모델 훈련 준비 완료!\")\n",
    "print(f\"   - 입력 형태: (배치크기={BATCH_SIZE}, 시퀀스길이={OUTPUT_SEQUENCE_LENGTH})\")\n",
    "print(f\"   - 어휘 사전 크기: {MAX_TOKENS:,}개\")\n",
    "print(\"   - 라벨: 0(부정), 1(긍정)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 다음 단계 안내\n",
    "\n",
    "이제 전처리된 데이터셋을 사용하여 딥러닝 모델을 구축할 수 있습니다.\n",
    "\n",
    "### 모델 구축 예시 코드:\n",
    "\n",
    "```python\n",
    "# 간단한 감정 분석 모델 예시\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(MAX_TOKENS, 128),\n",
    "    keras.layers.LSTM(64, dropout=0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(\n",
    "    train_ds_final,\n",
    "    validation_data=val_ds_final,\n",
    "    epochs=5\n",
    ")\n",
    "```\n",
    "\n",
    "### 사용 가능한 데이터셋:\n",
    "- `train_ds_final`: 훈련용 데이터셋\n",
    "- `val_ds_final`: 검증용 데이터셋  \n",
    "- `test_ds_final`: 테스트용 데이터셋\n",
    "\n",
    "모든 데이터셋은 벡터화되어 있으며 성능 최적화가 적용되어 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
