{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "# IMDB 영화 리뷰 감정 분석 (사전 훈련된 GloVe 임베딩 + 양방향 LSTM)\n",
        "\n",
        "이 노트북은 사전 훈련된 GloVe 임베딩을 사용하여 IMDB 영화 리뷰 감정 분석을 수행합니다.\n",
        "\n",
        "## 주요 특징\n",
        "- **데이터**: IMDB 영화 리뷰 (긍정/부정 이진 분류)\n",
        "- **임베딩**: 사전 훈련된 GloVe 6B.100d 임베딩 사용\n",
        "- **모델**: 양방향 LSTM (Bidirectional LSTM)\n",
        "- **가중치 고정**: trainable=False로 사전 훈련된 가중치 보존\n",
        "- **평가**: 훈련/검증/테스트 데이터셋으로 성능 평가\n",
        "\n",
        "## 워크플로우\n",
        "1. 데이터 다운로드 및 전처리\n",
        "2. 데이터셋 로드 및 텍스트 벡터화\n",
        "3. GloVe 사전 훈련된 임베딩 로드\n",
        "4. 임베딩 매트릭스 생성 및 매핑\n",
        "5. 사전 훈련된 임베딩으로 모델 구축\n",
        "6. 모델 훈련 및 평가\n",
        "\n",
        "## GloVe (Global Vectors for Word Representation)\n",
        "- **Stanford에서 개발**: 전역 단어 벡터 표현 방법\n",
        "- **장점**: 대규모 코퍼스에서 학습된 풍부한 의미 정보 활용\n",
        "- **6B.100d**: 60억 개 토큰으로 학습된 100차원 벡터\n",
        "- **전이 학습**: 도메인 특화 작업에 일반적 언어 지식 적용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 1. 라이브러리 임포트\n",
        "\n",
        "사전 훈련된 임베딩을 사용하기 위해 필요한 라이브러리들을 임포트합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 사전 훈련된 임베딩 사용을 위한 GloVe 관련 라이브러리\n",
        "import requests\n",
        "import subprocess\n",
        "import re\n",
        "import string\n",
        "import numpy as np  # GloVe 임베딩 처리용\n",
        "\n",
        "# TensorFlow 및 Keras 라이브러리\n",
        "import tensorflow as tf\n",
        "from keras.layers import TextVectorization\n",
        "from keras import models, layers\n",
        "import keras\n",
        "import os, pathlib, shutil, random\n",
        "\n",
        "print(\"TensorFlow 버전:\", tf.__version__)\n",
        "print(\"Keras 버전:\", keras.__version__)\n",
        "print(\"NumPy 버전:\", np.__version__)\n",
        "print(\"GloVe 임베딩 처리를 위한 라이브러리 임포트 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 2. 데이터 다운로드 및 전처리 함수\n",
        "\n",
        "IMDB 데이터셋을 다운로드하고 전처리하는 함수들을 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download():\n",
        "    \"\"\"IMDB 데이터셋을 다운로드하는 함수\"\"\"\n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    file_name = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "    print(\"IMDB 데이터셋 다운로드 시작...\")\n",
        "    response = requests.get(url, stream=True)  # 스트리밍 방식으로 다운로드\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):  # 8KB씩 다운로드\n",
        "            file.write(chunk)\n",
        "\n",
        "    print(\"Download complete!\")\n",
        "\n",
        "def release():\n",
        "    \"\"\"압축을 해제하는 함수\"\"\"\n",
        "    print(\"압축 해제 시작...\")\n",
        "    subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"], shell=True)\n",
        "    # tar.gz => linux에서는 파일을 여러개를 한번에 압축을 못함 \n",
        "    # tar라는 형식으로 압축할 모든 파일을 하나로 묶어서 패키지로 만든다음에 압축을 한다.  \n",
        "    # tar, gz가동 그래서 압축풀고 다시 패키지도 풀어야 한다. \n",
        "    # tar -xvzf 파일명 형태임         \n",
        "    print(\"압축풀기 완료\")\n",
        "\n",
        "def labeling(): \n",
        "    \"\"\"Train 데이터를 Train과 Validation으로 분할하는 함수\"\"\"\n",
        "    print(\"데이터 라벨링 및 분할 시작...\")\n",
        "    base_dir = pathlib.Path(\"aclImdb\") \n",
        "    val_dir = base_dir/\"val\"   # pathlib 객체에 / \"디렉토리\" => 결과가 문자열이 아니다 \n",
        "    train_dir = base_dir/\"train\"\n",
        "\n",
        "    # validation 디렉토리 생성 및 데이터 분할\n",
        "    # train => train과 validation으로 나눠야 한다. 라벨이 2개 여야 한다.\n",
        "    for category in (\"neg\", \"pos\"):\n",
        "        os.makedirs(val_dir/category, exist_ok=True)  # 디렉토리를 만들고 \n",
        "        files = os.listdir(train_dir/category)  # 해당 카테고리의 파일 목록을 모두 가져온다 \n",
        "        random.Random(1337).shuffle(files)  # 파일을 랜덤하게 섞어서 복사하려고 파일 목록을 모두 섞는다 \n",
        "        num_val_samples = int(0.2 * len(files))  # 20%를 validation으로 사용\n",
        "        val_files = files[-num_val_samples:]  # 20%만 val폴더로 이동한다 \n",
        "        for fname in val_files:\n",
        "            shutil.move(train_dir/category/fname, val_dir/category/fname)\n",
        "    \n",
        "    print(\"데이터 라벨링 및 분할 완료\")\n",
        "\n",
        "# 주석 처리: 이미 데이터가 있다면 다시 다운로드할 필요 없음\n",
        "# download()  # 파일 다운받기 = 용량이 너무 커서 8192만큼씩 잘라서 저장하는 코드임 \n",
        "# release()   # 압축 해제\n",
        "# labeling()  # 데이터 분할\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 3. 데이터셋 로드\n",
        "\n",
        "디렉토리 구조를 활용하여 IMDB 데이터셋을 로드하고 구조를 확인합니다.\n",
        "\n",
        "- **0**: 부정 리뷰 (neg)\n",
        "- **1**: 긍정 리뷰 (pos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터셋을 활용해서 디렉토리로부터 파일을 불러와서 벡터화를 진행한다 \n",
        "batch_size = 32  # 한번에 읽어올 양 \n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",  # 디렉토리명 \n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\",  # 디렉토리명 \n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\",  # 디렉토리명 \n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\"데이터셋 로드 완료!\")\n",
        "print(f\"훈련 데이터: {len(train_ds)} 배치\")\n",
        "print(f\"검증 데이터: {len(val_ds)} 배치\") \n",
        "print(f\"테스트 데이터: {len(test_ds)} 배치\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### 3.1 데이터 구조 확인\n",
        "\n",
        "로드된 데이터의 구조와 내용을 확인해봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터셋은 알아서 inputs, targets을 반복해서 갖고 온다. 우리한테 필요한거는 inputs만이다\n",
        "for inputs, targets in train_ds:  # 실제 읽어오는 데이터 확인 \n",
        "    print(\"inputs.shape\", inputs.shape)\n",
        "    print(\"inputs.dtype\", inputs.dtype)\n",
        "    print(\"targets.shape\", targets.shape)\n",
        "    print(\"targets.dtype\", targets.dtype)\n",
        "    print(\"\\n=== 샘플 데이터 ===\")\n",
        "    print(\"inputs 샘플 (처음 3개):\")\n",
        "    for i, text in enumerate(inputs[:3]):\n",
        "        print(f\"  {i+1}: {text.numpy().decode('utf-8')[:100]}...\")  # 처음 100자만 출력\n",
        "    print(f\"\\ntargets 샘플 (처음 3개): {targets[:3]}\")\n",
        "    break  # 하나만 출력해보자 \n",
        "\n",
        "print(\"\\n=== 라벨 정보 ===\")\n",
        "print(\"0: 부정 리뷰 (neg)\")  \n",
        "print(\"1: 긍정 리뷰 (pos)\")\n",
        "print(\"폴더명을 정렬해서 0,1,2 이런식으로 라벨링을 한다 (neg -> 0, pos -> 1)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 4. 텍스트 벡터화\n",
        "\n",
        "텍스트 데이터를 정수 시퀀스로 변환하여 GloVe 임베딩과 매핑할 준비를 합니다.\n",
        "\n",
        "### 하이퍼파라미터 설정\n",
        "- **max_length**: 한 리뷰에서 사용하는 최대 단어 수 (600)\n",
        "- **max_tokens**: 자주 사용하는 단어 개수 (20000) - 어휘 사전 크기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 시퀀스를 만들어야 한다 \n",
        "max_length = 600   # 한 평론에서 사용하는 단어는 최대 길이를 600개라고 보자  \n",
        "max_tokens = 20000  # 자주 사용하는 단어 20000개만 쓰겠다 \n",
        "\n",
        "text_vectorization = TextVectorization( \n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",  # 임베딩 층을 사용하려면 반드시 int여야 한다\n",
        "    output_sequence_length=max_length  \n",
        ")\n",
        "\n",
        "print(f\"최대 시퀀스 길이: {max_length}\")\n",
        "print(f\"어휘 사전 크기: {max_tokens}\")\n",
        "print(\"출력 모드: 정수 (int)\")\n",
        "print(\"TextVectorization 레이어 생성 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### 4.1 어휘사전 생성 및 벡터화 적용\n",
        "\n",
        "훈련 데이터를 사용하여 어휘사전을 생성하고, 모든 데이터셋에 벡터화를 적용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트만 추출 (라벨 제거)\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# 어휘사전 생성 (훈련 데이터 기반)\n",
        "print(\"어휘사전 생성 중...\")\n",
        "text_vectorization.adapt(text_only_train_ds)  # 어휘사전을 만들어야 한다 \n",
        "print(\"어휘사전 생성 완료!\")\n",
        "\n",
        "# 모든 데이터셋에 벡터화 적용\n",
        "print(\"데이터셋 벡터화 중...\")\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=1)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=1)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=1)\n",
        "\n",
        "print(\"데이터셋 벡터화 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### 4.2 벡터화된 데이터 확인\n",
        "\n",
        "벡터화 후 데이터의 내부 구조를 확인해봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 내부구조 살짝 보기 \n",
        "print(\"=== 벡터화된 데이터 내부구조 확인 ===\")\n",
        "for item in int_train_ds:\n",
        "    vectorized_texts, labels = item\n",
        "    print(f\"벡터화된 텍스트 형태: {vectorized_texts.shape}\")\n",
        "    print(f\"벡터화된 텍스트 타입: {vectorized_texts.dtype}\")\n",
        "    print(f\"라벨 형태: {labels.shape}\")\n",
        "    print(f\"라벨 타입: {labels.dtype}\")\n",
        "    \n",
        "    print(\"\\n=== 샘플 벡터화 결과 ===\")\n",
        "    print(\"첫 번째 리뷰의 벡터화 결과 (처음 20개 토큰):\")\n",
        "    print(vectorized_texts[0][:20])\n",
        "    print(f\"해당 라벨: {labels[0]}\")\n",
        "    break  # 하나만 출력해보자\n",
        "\n",
        "print(\"\\n다음 단계: GloVe 임베딩과 매핑하여 사전 훈련된 단어 표현 활용\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 5. GloVe 사전 훈련된 임베딩 로드\n",
        "\n",
        "Stanford에서 제공하는 GloVe 6B.100d 임베딩을 로드하여 사전 훈련된 단어 벡터를 활용합니다.\n",
        "\n",
        "### GloVe 임베딩 특징\n",
        "- **Global Vectors**: 전역 단어-단어 동시 출현 통계 활용\n",
        "- **6B**: 60억 개 토큰으로 훈련 (Wikipedia 2014 + Gigaword 5)\n",
        "- **100d**: 100차원 벡터 표현\n",
        "- **파일 형식**: `단어 벡터1 벡터2 ... 벡터100` 형태\n",
        "\n",
        "### 임베딩층과 원핫인코딩 비교\n",
        "- **임베딩층**: 내부적으로 연산을 해서 단어와 단어사이의 관계를 계산해서 밀집벡터를 만든다\n",
        "- **원핫인코딩**: 메모리를 너무 많이 차지함. 최대한 한문장을 표현하는데 만일 최대 20000 단어까지 처리한다면 한문장당 20000개가 필요. 희소행렬 요소가 거의다 0인데 그중 몇개가 값이 있을때, 학습시 속도가 엄청 느리다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 사전학습된 임베딩 데이터를 불러온다\n",
        "path_to_glove_file = \"./data/glove.6B.100d.txt\"\n",
        "# 임베딩 데이터, 단어별 각 단어와의 거리가 벡터로 저장되어 있음  \n",
        "# 파일명의 100이 출력 벡터의 크기이다 \n",
        "\n",
        "print(f\"GloVe 임베딩 파일 로드 중: {path_to_glove_file}\")\n",
        "print(\"파일 형식: 단어 벡터1 벡터2 ... 벡터100\")\n",
        "\n",
        "embeddings_index = {}\n",
        "try:\n",
        "    with open(path_to_glove_file, encoding=\"utf-8\") as f:\n",
        "        for line_num, line in enumerate(f, 1):  # 한 라인씩 읽는다 \n",
        "            # 단어, 단어들간의 벡터 구조로 되어 있다  예) the 0.0012 000172 ...... \n",
        "            word, coefs = line.split(maxsplit=1)  \n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")  # 나머지 벡터들을 numpy배열로 전환\n",
        "            embeddings_index[word] = coefs\n",
        "            \n",
        "            # 진행 상황 출력 (1만 라인마다)\n",
        "            if line_num % 10000 == 0:\n",
        "                print(f\"  처리된 라인: {line_num:,}\")\n",
        "                \n",
        "    print(f\"\\nGloVe 임베딩 로드 완료!\")\n",
        "    print(f\"총 단어 개수: {len(embeddings_index):,}개\")\n",
        "    print(f\"벡터 차원: {len(next(iter(embeddings_index.values())))}차원\")\n",
        "    \n",
        "    # 샘플 단어들의 벡터 확인\n",
        "    sample_words = [\"the\", \"good\", \"bad\", \"movie\", \"film\"]\n",
        "    print(f\"\\n=== 샘플 단어들의 벡터 (처음 5개 값만) ===\")\n",
        "    for word in sample_words:\n",
        "        if word in embeddings_index:\n",
        "            print(f\"{word}: {embeddings_index[word][:5]}\")\n",
        "        else:\n",
        "            print(f\"{word}: (없음)\")\n",
        "            \n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ 오류: {path_to_glove_file} 파일을 찾을 수 없습니다.\")\n",
        "    print(\"GloVe 파일을 다운로드하고 ./data/ 폴더에 저장해주세요.\")\n",
        "    print(\"다운로드 링크: https://nlp.stanford.edu/projects/glove/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 6. 임베딩 매트릭스 생성\n",
        "\n",
        "우리 데이터셋의 어휘사전과 GloVe 임베딩을 매핑하여 모델에서 사용할 임베딩 매트릭스를 생성합니다.\n",
        "\n",
        "### 매핑 과정\n",
        "1. **어휘사전 추출**: TextVectorization에서 생성된 어휘사전 가져오기\n",
        "2. **인덱스 매핑**: {단어: 인덱스} 딕셔너리 생성\n",
        "3. **매트릭스 초기화**: (20000, 100) 크기의 0 매트릭스 생성\n",
        "4. **벡터 매핑**: GloVe에 있는 단어들의 벡터를 해당 인덱스에 할당\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 우리데이터와 연동을 해야 한다 \n",
        "vocabulary = text_vectorization.get_vocabulary()  # 우리 어휘사전 가져오기\n",
        "print(f\"우리 데이터의 어휘사전 크기: {len(vocabulary)}\")\n",
        "print(f\"어휘사전 샘플 (처음 10개): {vocabulary[:10]}\")\n",
        "\n",
        "# {단어:인덱스} 형태의 딕셔너리를 만들어야 한다 \n",
        "# {\"\", \"[UNK]\", \"write\", \"love\", \"make\",...........} vocabulary\n",
        "# {0,1,2,3,4,5,6,..................}\n",
        "# zip (\"\",0) (\"[UNK]\",1) (\"write\", 2 )............\n",
        "# {\"\":0, \"[UNK]\":1, \"write\":2, ,,,,,,,}\n",
        "\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary)))) \n",
        "print(f\"단어-인덱스 매핑 딕셔너리 생성 완료\")\n",
        "print(f\"샘플 매핑: {dict(list(word_index.items())[:5])}\")\n",
        "\n",
        "embedding_dim = 100  # 미리 학습한 임베딩층의 출력값이 100개임 \n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))  # 20000 * 100 배열을 잡고 0으로 채운다 \n",
        "print(f\"\\n임베딩 매트릭스 초기화: {embedding_matrix.shape}\")\n",
        "print(\"케라스 Embedding 레이어에 초기값으로 사용될 예정\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### 6.1 GloVe 벡터를 임베딩 매트릭스에 매핑\n",
        "\n",
        "우리 어휘사전의 각 단어에 대해 GloVe에서 해당 벡터를 찾아 임베딩 매트릭스에 할당합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embedding_matrix를 우리가 embedding_index 정보로 채워야 한다. \n",
        "# word_index는 {단어:인덱스} 형태임 \n",
        "\n",
        "hits = 0  # GloVe에서 찾은 단어 개수\n",
        "misses = 0  # GloVe에서 찾지 못한 단어 개수\n",
        "\n",
        "print(\"GloVe 벡터를 임베딩 매트릭스에 매핑 중...\")\n",
        "\n",
        "for word, i in word_index.items(): \n",
        "    # 단어와 인덱스를 가져온다 \n",
        "    if i < max_tokens:  # 혹시나 20000개를 넘어가는 토큰이 있을까봐 오류처리\n",
        "        embedding_vector = embeddings_index.get(word)  # 단어에 해당하는 벡터들 이동 \n",
        "        if embedding_vector is not None:  # embedding_vector값이 None인 경우를 제외하고 \n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            misses += 1\n",
        "\n",
        "print(f\"\\n=== 매핑 결과 ===\")\n",
        "print(f\"GloVe에서 찾은 단어: {hits:,}개\")\n",
        "print(f\"GloVe에서 찾지 못한 단어: {misses:,}개\")\n",
        "print(f\"매핑 성공률: {hits/(hits+misses)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n=== 임베딩 매트릭스 확인 ===\")\n",
        "print(f\"매트릭스 크기: {embedding_matrix.shape}\")\n",
        "print(f\"0이 아닌 행의 개수: {np.count_nonzero(np.any(embedding_matrix != 0, axis=1))}\")\n",
        "print(\"임베딩 매트릭스 생성 완료!\")\n",
        "\n",
        "# 매트릭스 샘플 확인 (처음 10개 단어의 처음 5개 벡터 값)\n",
        "print(f\"\\n=== 임베딩 매트릭스 샘플 (처음 10개 단어, 처음 5개 벡터 값) ===\")\n",
        "for i in range(min(10, len(vocabulary))):\n",
        "    word = vocabulary[i]\n",
        "    vector_sample = embedding_matrix[i][:5]\n",
        "    print(f\"인덱스 {i:2d} | 단어 '{word:8s}' | 벡터: {vector_sample}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 7. 사전 훈련된 임베딩으로 모델 구축\n",
        "\n",
        "GloVe 임베딩 매트릭스를 초기값으로 사용하는 Embedding 레이어로 모델을 구성합니다.\n",
        "\n",
        "### 핵심 설정\n",
        "- **embeddings_initializer**: GloVe 임베딩 매트릭스로 초기화\n",
        "- **trainable=False**: 사전 훈련된 가중치를 고정하여 보존\n",
        "- **mask_zero=True**: 패딩 토큰(0)을 마스킹하여 연산에서 제외\n",
        "\n",
        "### 모델 아키텍처\n",
        "1. **입력**: 정수 시퀀스 (배치_크기, 시퀀스_길이)\n",
        "2. **사전 훈련된 임베딩**: GloVe 100차원 벡터로 변환\n",
        "3. **양방향 LSTM**: 양방향으로 시퀀스 처리\n",
        "4. **드롭아웃**: 과적합 방지\n",
        "5. **출력**: 이진 분류를 위한 시그모이드 활성화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 구축 시작\n",
        "print(\"=== 사전 훈련된 GloVe 임베딩으로 모델 구축 ===\")\n",
        "\n",
        "# 입력 레이어\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "print(f\"입력 형태: {inputs.shape}\")\n",
        "\n",
        "# 사전 훈련된 임베딩 레이어\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, \n",
        "    output_dim=embedding_dim, \n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),  # 반드시 GloVe 매트릭스로 초기화\n",
        "    # 사전학습된 층에 의해 바꿔치기가 이뤄져야 한다 \n",
        "    trainable=False,  # 임베딩가중치를 훈련중에 업데이트할거냐? 사전학습된 임베딩층을 사용할때는 False로 지정해야 한다 \n",
        "    mask_zero=True  # 패딩 토큰(0)을 마스킹\n",
        ")(inputs)\n",
        " \n",
        "print(f\"임베딩 후 형태: {embedded.shape}\")\n",
        "print(\"🔒 trainable=False: 사전 훈련된 GloVe 가중치 고정\")\n",
        "print(\"🎭 mask_zero=True: 패딩 토큰 마스킹 적용\")\n",
        "\n",
        "# 미리 학습된 임베딩층으로 바꿀 수가 있다\n",
        "# 입력벡터크기는 20000, 출력벡터는 100(GloVe 차원)의 크기를 갖는다  \n",
        "print(f\"GloVe 임베딩: 입력 {max_tokens}차원 → 출력 {embedding_dim}차원\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### 7.1 양방향 LSTM 및 출력 레이어\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 양방향 RNN을 가동시킴 \n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) \n",
        "print(\"양방향 LSTM 추가 (32 유닛)\")\n",
        "print(\"- 순방향 LSTM: 문장의 앞에서 뒤로 정보 처리\")\n",
        "print(\"- 역방향 LSTM: 문장의 뒤에서 앞으로 정보 처리\")\n",
        "print(\"- 결과: 양방향 정보를 결합한 풍부한 표현\")\n",
        "\n",
        "# 드롭아웃으로 과적합 방지\n",
        "x = layers.Dropout(0.5)(x) \n",
        "print(\"\\n드롭아웃 추가 (0.5)\")\n",
        "print(\"- 훈련 시 50% 뉴런을 무작위로 비활성화\")\n",
        "print(\"- 과적합 방지 및 일반화 성능 향상\")\n",
        "\n",
        "# 출력 레이어 (이진 분류)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "print(\"\\n출력 레이어 추가 (시그모이드 활성화)\")\n",
        "print(\"- 1개 뉴런: 이진 분류 (긍정/부정)\")\n",
        "print(\"- 시그모이드: 0~1 사이 확률값 출력\")\n",
        "\n",
        "# 모델 생성\n",
        "model = keras.Model(inputs, outputs) \n",
        "print(f\"\\n🎯 모델 생성 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### 7.2 모델 컴파일 및 구조 확인\n",
        "\n",
        "모델을 컴파일하고 전체 구조를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 컴파일\n",
        "model.compile(\n",
        "    optimizer='rmsprop',           # RMSprop 옵티마이저\n",
        "    loss='binary_crossentropy',    # 이진 분류용 손실 함수\n",
        "    metrics=['accuracy']           # 정확도 메트릭\n",
        ")\n",
        "\n",
        "print(\"=== 모델 컴파일 완료 ===\")\n",
        "print(\"옵티마이저: RMSprop\")\n",
        "print(\"- 학습률을 적응적으로 조정하는 옵티마이저\")\n",
        "print(\"- RNN/LSTM에서 안정적인 성능\")\n",
        "print(\"\\n손실 함수: binary_crossentropy\") \n",
        "print(\"- 이진 분류 문제에 최적화된 손실 함수\")\n",
        "print(\"- 예측 확률과 실제 라벨 간의 교차 엔트로피\")\n",
        "print(\"\\n메트릭: accuracy\")\n",
        "print(\"- 정확히 예측한 샘플의 비율\")\n",
        "\n",
        "print(\"\\n=== 모델 구조 요약 ===\")\n",
        "model.summary()\n",
        "\n",
        "print(f\"\\n=== 사전 훈련된 임베딩 활용 확인 ===\")\n",
        "print(f\"임베딩 레이어 trainable: {model.layers[1].trainable}\")\n",
        "print(f\"임베딩 가중치 크기: {model.layers[1].get_weights()[0].shape}\")\n",
        "print(\"✅ GloVe 사전 훈련된 가중치가 고정되어 학습에 활용됩니다!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 8. 모델 훈련 및 평가\n",
        "\n",
        "사전 훈련된 GloVe 임베딩을 활용한 모델을 훈련하고 성능을 평가합니다.\n",
        "\n",
        "### 훈련 설정\n",
        "- **에포크**: 15회 (원본 코드 기준)\n",
        "- **검증 데이터**: val 데이터셋 사용\n",
        "- **고정된 임베딩**: GloVe 가중치는 학습되지 않음\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 훈련 시작\n",
        "print(\"=== 사전 훈련된 GloVe 임베딩으로 모델 훈련 시작 ===\")\n",
        "print(\"에포크: 15\")\n",
        "print(\"검증 데이터: val_ds\")\n",
        "print(\"🔒 GloVe 임베딩 가중치: 고정 (trainable=False)\")\n",
        "print()\n",
        "\n",
        "# 훈련 실행\n",
        "history = model.fit(\n",
        "    int_train_ds, \n",
        "    validation_data=int_val_ds, \n",
        "    epochs=15,\n",
        "    verbose=1  # 훈련 과정 출력\n",
        ")\n",
        "\n",
        "print(\"=== 모델 훈련 완료 ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### 8.1 모델 성능 평가\n",
        "\n",
        "훈련이 완료된 모델을 테스트 데이터셋으로 평가하여 최종 성능을 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 데이터셋으로 모델 평가\n",
        "print(\"=== 테스트 데이터셋 평가 ===\")\n",
        "test_results = model.evaluate(int_test_ds, verbose=1)\n",
        "test_loss, test_accuracy = test_results\n",
        "\n",
        "print(f\"\\n=== GloVe 임베딩 모델 최종 결과 ===\")\n",
        "print(f\"테스트 손실: {test_loss:.4f}\")\n",
        "print(f\"테스트 정확도: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "# 훈련 히스토리 요약\n",
        "if 'history' in locals():\n",
        "    final_train_acc = history.history['accuracy'][-1]\n",
        "    final_val_acc = history.history['val_accuracy'][-1]\n",
        "    \n",
        "    print(f\"\\n=== 훈련 과정 요약 ===\")\n",
        "    print(f\"최종 훈련 정확도: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
        "    print(f\"최종 검증 정확도: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
        "    print(f\"테스트 정확도: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "    \n",
        "    # 과적합 여부 확인\n",
        "    if final_train_acc - test_accuracy > 0.1:\n",
        "        print(\"\\n⚠️  과적합 가능성이 있습니다.\")\n",
        "        print(\"- 에포크 수 감소 고려\")\n",
        "        print(\"- 드롭아웃 비율 증가 고려\")\n",
        "        print(\"- 조기 종료(Early Stopping) 적용 고려\")\n",
        "    else:\n",
        "        print(\"\\n✅ 적절한 일반화 성능을 보입니다.\")\n",
        "\n",
        "# 원본 코드 스타일 출력\n",
        "print(f\"\\n=== 간단 출력 (원본 스타일) ===\")\n",
        "print(\"테스트셋\", test_results)\n",
        "\n",
        "print(f\"\\n=== 사전 훈련된 임베딩의 효과 ===\")\n",
        "print(\"🎯 GloVe 임베딩을 통해 풍부한 언어 지식을 활용했습니다!\")\n",
        "print(\"📈 사전 훈련된 단어 표현으로 더 나은 성능을 기대할 수 있습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 9. 결론 및 비교 분석\n",
        "\n",
        "### 사전 훈련된 임베딩의 장점\n",
        "- **풍부한 언어 지식**: 60억 개 토큰으로 학습된 GloVe의 방대한 언어 정보 활용\n",
        "- **빠른 수렴**: 좋은 초기값으로 시작하여 더 빠른 학습\n",
        "- **일반화 성능**: 대규모 코퍼스에서 학습된 일반적 언어 표현\n",
        "- **도메인 적응**: 특정 도메인에 특화되지 않은 범용적 단어 표현\n",
        "\n",
        "### 모델 특징 요약\n",
        "- **GloVe 6B.100d**: Stanford의 사전 훈련된 100차원 단어 벡터\n",
        "- **고정된 가중치**: trainable=False로 사전 훈련된 지식 보존\n",
        "- **양방향 LSTM**: 문맥을 양방향으로 고려하여 성능 향상\n",
        "- **마스킹**: mask_zero=True로 패딩 토큰 처리\n",
        "\n",
        "### 임베딩 방법 비교\n",
        "\n",
        "| 특성 | 원핫 인코딩 | 학습 가능한 임베딩 | 사전 훈련된 임베딩 (GloVe) |\n",
        "|------|-------------|-------------------|---------------------------|\n",
        "| **메모리 사용량** | 매우 높음 | 보통 | 보통 |\n",
        "| **학습 속도** | 느림 | 보통 | 빠름 |\n",
        "| **초기 성능** | 낮음 | 낮음 | 높음 |\n",
        "| **언어 지식** | 없음 | 학습으로 획득 | 사전 훈련된 지식 |\n",
        "| **도메인 적응** | 제한적 | 우수 | 보통 |\n",
        "| **메모리 효율성** | ❌ | ✅ | ✅ |\n",
        "| **전이 학습** | ❌ | ❌ | ✅ |\n",
        "| **권장 사용** | 실험용 | 일반적 | 고성능 필요시 |\n",
        "\n",
        "### 언제 사전 훈련된 임베딩을 사용할까?\n",
        "\n",
        "#### ✅ 사용 권장 상황\n",
        "- **소규모 데이터셋**: 충분한 데이터가 없어 임베딩 학습이 어려운 경우\n",
        "- **빠른 프로토타이핑**: 빠르게 좋은 성능을 얻고 싶은 경우\n",
        "- **일반적 언어 작업**: 감정 분석, 문서 분류 등 범용 언어 이해 필요\n",
        "- **계산 자원 제한**: 학습 시간과 자원을 절약하고 싶은 경우\n",
        "\n",
        "#### ❌ 사용 비권장 상황\n",
        "- **특수 도메인**: 의료, 법률 등 전문 용어가 많은 도메인\n",
        "- **대규모 데이터**: 충분한 데이터로 도메인 특화 임베딩 학습 가능\n",
        "- **특수 어휘**: 신조어, 슬랭 등이 많은 소셜 미디어 데이터\n",
        "\n",
        "### 가능한 개선사항\n",
        "1. **다른 사전 훈련된 임베딩**: Word2Vec, FastText, 한국어 특화 임베딩\n",
        "2. **미세 조정**: trainable=True로 설정하여 도메인 적응\n",
        "3. **Transformer 모델**: BERT, RoBERTa 등 최신 언어 모델 활용\n",
        "4. **앙상블**: 여러 임베딩 방법의 결합\n",
        "5. **하이퍼파라미터 튜닝**: \n",
        "   - LSTM 유닛 수 조정\n",
        "   - 드롭아웃 비율 최적화\n",
        "   - 학습률 스케줄링\n",
        "\n",
        "### 실제 적용 고려사항\n",
        "- **임베딩 파일 크기**: GloVe 6B.100d는 약 350MB\n",
        "- **로딩 시간**: 임베딩 매트릭스 생성에 시간 소요\n",
        "- **메모리 사용량**: 임베딩 매트릭스가 모델 크기에 영향\n",
        "- **업데이트**: 새로운 단어에 대한 처리 방법 고려\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
