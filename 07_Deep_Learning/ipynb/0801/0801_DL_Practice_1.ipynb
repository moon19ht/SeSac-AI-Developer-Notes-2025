{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "# ğŸ¬ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ - TF-IDF & BoW ë°©ì‹\n",
        "\n",
        "## ğŸ“‹ ëª©ì°¨\n",
        "1. [í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸](#1-í™˜ê²½-ì„¤ì •-ë°-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì„í¬íŠ¸)\n",
        "2. [ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬](#2-ë°ì´í„°-ë‹¤ìš´ë¡œë“œ-ë°-ì „ì²˜ë¦¬)\n",
        "3. [ë°ì´í„°ì…‹ ë¡œë”© ë° í™•ì¸](#3-ë°ì´í„°ì…‹-ë¡œë”©-ë°-í™•ì¸)\n",
        "4. [í…ìŠ¤íŠ¸ ë²¡í„°í™” (TF-IDF)](#4-í…ìŠ¤íŠ¸-ë²¡í„°í™”-tf-idf)\n",
        "5. [ëª¨ë¸ êµ¬ì„± ë° í›ˆë ¨](#5-ëª¨ë¸-êµ¬ì„±-ë°-í›ˆë ¨)\n",
        "6. [ëª¨ë¸ í‰ê°€ ë° ì˜ˆì¸¡](#6-ëª¨ë¸-í‰ê°€-ë°-ì˜ˆì¸¡)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”\n",
        "- **ëª©í‘œ**: IMDb ì˜í™” ë¦¬ë·° ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ê¸ì •/ë¶€ì • ê°ì • ë¶„ì„\n",
        "- **ë°©ë²•**: TF-IDF (Term Frequency-Inverse Document Frequency) ë²¡í„°í™”\n",
        "- **íŠ¹ì§•**: N-gram (2-gram) ì‚¬ìš©ìœ¼ë¡œ ë‹¨ì–´ ì¡°í•© íŒ¨í„´ í•™ìŠµ\n",
        "- **ëª¨ë¸**: ê°„ë‹¨í•œ Dense Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "\n",
        "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤:\n",
        "- **requests**: ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "- **tensorflow/keras**: ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì„±\n",
        "- **TextVectorization**: í…ìŠ¤íŠ¸ ë²¡í„°í™”\n",
        "- **ê¸°íƒ€**: íŒŒì¼ ì²˜ë¦¬, ì‹œìŠ¤í…œ ê´€ë ¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow ë²„ì „: 2.15.1\n",
            "Keras ë²„ì „: 2.15.0\n",
            "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import os, pathlib, shutil, random\n",
        "import keras\n",
        "from keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "print(\"TensorFlow ë²„ì „:\", tf.__version__)\n",
        "print(\"Keras ë²„ì „:\", keras.__version__)\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 2. ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "\n",
        "IMDb ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤:\n",
        "\n",
        "### ğŸ“ ë°ì´í„°ì…‹ ì •ë³´\n",
        "- **ì¶œì²˜**: Stanford AI Lab\n",
        "- **í¬ê¸°**: ì•½ 84MB (ì••ì¶•)\n",
        "- **êµ¬ì„±**: 25,000ê°œ í›ˆë ¨ ë¦¬ë·° + 25,000ê°œ í…ŒìŠ¤íŠ¸ ë¦¬ë·°\n",
        "- **ë¼ë²¨**: pos(ê¸ì •), neg(ë¶€ì •)\n",
        "\n",
        "### ğŸ”„ ì „ì²˜ë¦¬ ê³¼ì •\n",
        "1. **ë‹¤ìš´ë¡œë“œ**: ì••ì¶• íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
        "2. **ì••ì¶• í•´ì œ**: tar.gz íŒŒì¼ í•´ì œ\n",
        "3. **ë¼ë²¨ë§**: train/validation ë¶„í•  (80:20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "ğŸ’¡ í•„ìš”ì‹œ ë‹¤ìŒ í•¨ìˆ˜ë“¤ì„ í˜¸ì¶œí•˜ì„¸ìš”:\n",
            "   - download(): ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
            "   - release(): ì••ì¶• í•´ì œ\n",
            "   - labeling(): ë°ì´í„° ë¶„í• \n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜\n",
        "def download():\n",
        "    \"\"\"IMDb ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    file_name = \"aclImdb_v1.tar.gz\"\n",
        "    \n",
        "    print(\"ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\")\n",
        "    response = requests.get(url, stream=True)  # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):  # 8KBì”© ë‹¤ìš´ë¡œë“œ\n",
        "            file.write(chunk)\n",
        "    \n",
        "    print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "# ì••ì¶• í•´ì œ í•¨ìˆ˜\n",
        "def release():\n",
        "    \"\"\"ë‹¤ìš´ë¡œë“œí•œ ì••ì¶• íŒŒì¼ì„ í•´ì œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    print(\"ğŸ“‚ ì••ì¶• í•´ì œ ì¤‘...\")\n",
        "    subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"], shell=True)\n",
        "    print(\"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n",
        "\n",
        "# ë°ì´í„° ë¼ë²¨ë§ ë° ë¶„í•  í•¨ìˆ˜\n",
        "def labeling():\n",
        "    \"\"\"train ë°ì´í„°ë¥¼ train/validationìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\"\"\"\n",
        "    print(\"ğŸ·ï¸ ë°ì´í„° ë¼ë²¨ë§ ë° ë¶„í•  ì¤‘...\")\n",
        "    base_dir = pathlib.Path(\"aclImdb\")\n",
        "    val_dir = base_dir / \"val\"\n",
        "    train_dir = base_dir / \"train\"\n",
        "    \n",
        "    for category in (\"neg\", \"pos\"):\n",
        "        os.makedirs(val_dir / category, exist_ok=True)\n",
        "        files = os.listdir(train_dir / category)\n",
        "        random.Random(1337).shuffle(files)  # ì¬í˜„ ê°€ëŠ¥í•œ ëœë¤ ì…”í”Œ\n",
        "        num_val_samples = int(0.2 * len(files))  # 20%ë¥¼ validationìœ¼ë¡œ\n",
        "        val_files = files[-num_val_samples:]\n",
        "        \n",
        "        for fname in val_files:\n",
        "            shutil.move(train_dir / category / fname, val_dir / category / fname)\n",
        "    \n",
        "    print(\"âœ… ë¼ë²¨ë§ ë° ë¶„í•  ì™„ë£Œ!\")\n",
        "\n",
        "print(\"ğŸ“‹ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "print(\"ğŸ’¡ í•„ìš”ì‹œ ë‹¤ìŒ í•¨ìˆ˜ë“¤ì„ í˜¸ì¶œí•˜ì„¸ìš”:\")\n",
        "print(\"   - download(): ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\")\n",
        "print(\"   - release(): ì••ì¶• í•´ì œ\") \n",
        "print(\"   - labeling(): ë°ì´í„° ë¶„í• \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 3. ë°ì´í„°ì…‹ ë¡œë”© ë° í™•ì¸\n",
        "\n",
        "Kerasì˜ `text_dataset_from_directory`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ“Š ë°ì´í„°ì…‹ êµ¬ì¡°\n",
        "- **í´ë” êµ¬ì¡°**: `aclImdb/train/pos`, `aclImdb/train/neg` ë“±\n",
        "- **ë°°ì¹˜ í¬ê¸°**: 32ê°œì”© ì²˜ë¦¬\n",
        "- **ë¼ë²¨**: 0(ë¶€ì •), 1(ê¸ì •) - í´ë”ëª… ì•ŒíŒŒë²³ ìˆœì„œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ!\n",
            "ğŸ“Š ë°°ì¹˜ í¬ê¸°: 32\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„°ì…‹ ì„¤ì •\n",
        "batch_size = 32  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë°ì´í„° ê°œìˆ˜\n",
        "\n",
        "print(\"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
        "\n",
        "# í›ˆë ¨ìš© ë°ì´í„°ì…‹\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"../../data/aclImdb/train\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# ê²€ì¦ìš© ë°ì´í„°ì…‹  \n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"../../data/aclImdb/val\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"../../data/aclImdb/test\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” ë°ì´í„° êµ¬ì¡° í™•ì¸:\n",
            "--------------------------------------------------\n",
            "ğŸ“ ì…ë ¥ ë°ì´í„° (ë¦¬ë·° í…ìŠ¤íŠ¸):\n",
            "   - í˜•íƒœ: (32,)\n",
            "   - íƒ€ì…: <dtype: 'string'>\n",
            "\n",
            "ğŸ¯ íƒ€ê²Ÿ ë°ì´í„° (ë¼ë²¨):\n",
            "   - í˜•íƒœ: (32,)\n",
            "   - íƒ€ì…: <dtype: 'int32'>\n",
            "\n",
            "ğŸ“– ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ):\n",
            "   ë¦¬ë·° 1: This is a truly magnificent and heartwrenching film!!!! Ripstein's locations are spectacular, extrem...\n",
            "   ë¼ë²¨ 1: 1 (ê¸ì •)\n",
            "\n",
            "   ë¦¬ë·° 2: Jimmy Cagney races by your eyes constantly in this story of a stage-producer who is vigorously strug...\n",
            "   ë¼ë²¨ 2: 1 (ê¸ì •)\n",
            "\n",
            "   ë¦¬ë·° 3: SWING! is an important film because it's one of the remaining Black-produced and acted films from th...\n",
            "   ë¼ë²¨ 3: 0 (ë¶€ì •)\n",
            "\n",
            "ğŸ’¡ ë¼ë²¨ ì •ë³´: 0=ë¶€ì •(neg), 1=ê¸ì •(pos)\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
        "print(\"ğŸ” ë°ì´í„° êµ¬ì¡° í™•ì¸:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"ğŸ“ ì…ë ¥ ë°ì´í„° (ë¦¬ë·° í…ìŠ¤íŠ¸):\")\n",
        "    print(f\"   - í˜•íƒœ: {inputs.shape}\")\n",
        "    print(f\"   - íƒ€ì…: {inputs.dtype}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"ğŸ¯ íƒ€ê²Ÿ ë°ì´í„° (ë¼ë²¨):\")\n",
        "    print(f\"   - í˜•íƒœ: {targets.shape}\")\n",
        "    print(f\"   - íƒ€ì…: {targets.dtype}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"ğŸ“– ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ):\")\n",
        "    for i in range(3):\n",
        "        print(f\"   ë¦¬ë·° {i+1}: {inputs[i].numpy().decode('utf-8')[:100]}...\")\n",
        "        print(f\"   ë¼ë²¨ {i+1}: {targets[i].numpy()} ({'ê¸ì •' if targets[i].numpy() == 1 else 'ë¶€ì •'})\")\n",
        "        print()\n",
        "    \n",
        "    break  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ í™•ì¸\n",
        "\n",
        "print(\"ğŸ’¡ ë¼ë²¨ ì •ë³´: 0=ë¶€ì •(neg), 1=ê¸ì •(pos)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 4. í…ìŠ¤íŠ¸ ë²¡í„°í™” (TF-IDF)\n",
        "\n",
        "### ğŸ”¤ TF-IDFë€?\n",
        "- **TF (Term Frequency)**: ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë¹ˆë„\n",
        "- **IDF (Inverse Document Frequency)**: ë‹¨ì–´ì˜ í¬ê·€ì„± \n",
        "- **ì¡°í•©**: ìì£¼ ë‚˜ì˜¤ì§€ë§Œ íŠ¹ë³„í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ì— ë†’ì€ ê°€ì¤‘ì¹˜\n",
        "\n",
        "### âš™ï¸ ì„¤ì • íŒŒë¼ë¯¸í„°\n",
        "- **max_tokens**: 20,000ê°œ (ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë“¤)\n",
        "- **output_mode**: \"tf_idf\" (TF-IDF ê°€ì¤‘ì¹˜ ì ìš©)\n",
        "- **ngrams**: 2 (ë‹¨ì–´ 2ê°œ ì¡°í•©ë„ ê³ ë ¤)\n",
        "\n",
        "### ğŸ¯ N-gramì˜ íš¨ê³¼\n",
        "- **1-gram**: \"good\", \"movie\" \n",
        "- **2-gram**: \"good movie\", \"very good\"\n",
        "- ë¬¸ë§¥ ì •ë³´ë¥¼ ë” ì˜ ìº¡ì²˜í•  ìˆ˜ ìˆìŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¤ TF-IDF ë²¡í„°í™” ì„¤ì • ì¤‘...\n",
            "âœ… ë²¡í„°í™” ì„¤ì • ì™„ë£Œ!\n",
            "ğŸ“Š ì„¤ì • ì •ë³´:\n",
            "   - ìµœëŒ€ í† í° ìˆ˜: 20,000\n",
            "   - ì¶œë ¥ ëª¨ë“œ: TF-IDF\n",
            "   - N-gram: 2 (ë‹¨ì–´ ì¡°í•© ê³ ë ¤)\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF ë²¡í„°í™” ì„¤ì •\n",
        "print(\"ğŸ”¤ TF-IDF ë²¡í„°í™” ì„¤ì • ì¤‘...\")\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,           # ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ 20,000ê°œë§Œ ì‚¬ìš©\n",
        "    output_mode=\"tf_idf\",       # TF-IDF ê°€ì¤‘ì¹˜ ì ìš©\n",
        "    ngrams=2                    # 2-gram ì‚¬ìš© (ë‹¨ì–´ ì¡°í•© ê³ ë ¤)\n",
        ")\n",
        "\n",
        "print(\"âœ… ë²¡í„°í™” ì„¤ì • ì™„ë£Œ!\")\n",
        "print(\"ğŸ“Š ì„¤ì • ì •ë³´:\")\n",
        "print(f\"   - ìµœëŒ€ í† í° ìˆ˜: {20000:,}\")\n",
        "print(f\"   - ì¶œë ¥ ëª¨ë“œ: TF-IDF\")\n",
        "print(f\"   - N-gram: 2 (ë‹¨ì–´ ì¡°í•© ê³ ë ¤)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“š ì–´íœ˜ì‚¬ì „ ìƒì„± ì¤‘...\n",
            "âœ… ì–´íœ˜ì‚¬ì „ ìƒì„± ì™„ë£Œ!\n",
            "ğŸ”„ ë°ì´í„°ì…‹ ë²¡í„°í™” ì¤‘...\n",
            "   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬...\n",
            "âœ… ëª¨ë“  ë°ì´í„°ì…‹ ë²¡í„°í™” ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# ì–´íœ˜ì‚¬ì „ ìƒì„± (í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ)\n",
        "print(\"ğŸ“š ì–´íœ˜ì‚¬ì „ ìƒì„± ì¤‘...\")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)  # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
        "text_vectorization.adapt(text_only_train_ds)       # ì–´íœ˜ì‚¬ì „ ìƒì„±\n",
        "print(\"âœ… ì–´íœ˜ì‚¬ì „ ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# ëª¨ë“  ë°ì´í„°ì…‹ì— ë²¡í„°í™” ì ìš©\n",
        "print(\"ğŸ”„ ë°ì´í„°ì…‹ ë²¡í„°í™” ì¤‘...\")\n",
        "print(\"   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬...\")\n",
        "\n",
        "# ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í•œ ë²¡í„°í™” (CPU ì½”ì–´ 4ê°œ í™œìš©)\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "print(\"âœ… ëª¨ë“  ë°ì´í„°ì…‹ ë²¡í„°í™” ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” ë²¡í„°í™” ê²°ê³¼ í™•ì¸:\n",
            "--------------------------------------------------\n",
            "ğŸ“Š ë²¡í„°í™”ëœ ì…ë ¥ ë°ì´í„°:\n",
            "   - í˜•íƒœ: (32, 20000)\n",
            "   - íƒ€ì…: <dtype: 'float32'>\n",
            "   - ê°’ ë²”ìœ„: 0.000000 ~ 3422.129639\n",
            "\n",
            "ğŸ¯ íƒ€ê²Ÿ ë°ì´í„°:\n",
            "   - í˜•íƒœ: (32,)\n",
            "   - íƒ€ì…: <dtype: 'int32'>\n",
            "\n",
            "ğŸ“ˆ ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ TF-IDF ê°’ (0ì´ ì•„ë‹Œ ê°’ë“¤):\n",
            "   - í™œì„±í™”ëœ íŠ¹ì„± ìˆ˜: 104/20000\n",
            "   - ì²˜ìŒ 10ê°œ ë¹„0 ê°’: [293.1731      6.974498    0.7111458   4.2621336   2.8794088   1.4509387\n",
            "   3.7522266   0.7591958   4.6798143   1.6230419]\n",
            "\n",
            "ğŸ’¡ TF-IDF íŠ¹ì§•:\n",
            "   - ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ë‹¨ì–´/êµ¬ë¬¸ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•¨\n",
            "   - ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0 (í¬ì†Œ í–‰ë ¬)\n",
            "   - ë¬¸ì„œë³„ë¡œ ê³ ìœ í•œ íŒ¨í„´ì„ ê°€ì§\n"
          ]
        }
      ],
      "source": [
        "# ë²¡í„°í™” ê²°ê³¼ í™•ì¸\n",
        "print(\"ğŸ” ë²¡í„°í™” ê²°ê³¼ í™•ì¸:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"ğŸ“Š ë²¡í„°í™”ëœ ì…ë ¥ ë°ì´í„°:\")\n",
        "    print(f\"   - í˜•íƒœ: {inputs.shape}\")\n",
        "    print(f\"   - íƒ€ì…: {inputs.dtype}\")\n",
        "    print(f\"   - ê°’ ë²”ìœ„: {inputs.numpy().min():.6f} ~ {inputs.numpy().max():.6f}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"ğŸ¯ íƒ€ê²Ÿ ë°ì´í„°:\")\n",
        "    print(f\"   - í˜•íƒœ: {targets.shape}\")\n",
        "    print(f\"   - íƒ€ì…: {targets.dtype}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"ğŸ“ˆ ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ TF-IDF ê°’ (0ì´ ì•„ë‹Œ ê°’ë“¤):\")\n",
        "    first_sample = inputs[0].numpy()\n",
        "    non_zero_indices = np.where(first_sample > 0)[0]\n",
        "    print(f\"   - í™œì„±í™”ëœ íŠ¹ì„± ìˆ˜: {len(non_zero_indices)}/{len(first_sample)}\")\n",
        "    print(f\"   - ì²˜ìŒ 10ê°œ ë¹„0 ê°’: {first_sample[non_zero_indices[:10]]}\")\n",
        "    print()\n",
        "    \n",
        "    break\n",
        "\n",
        "print(\"ğŸ’¡ TF-IDF íŠ¹ì§•:\")\n",
        "print(\"   - ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ë‹¨ì–´/êµ¬ë¬¸ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•¨\")\n",
        "print(\"   - ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0 (í¬ì†Œ í–‰ë ¬)\")\n",
        "print(\"   - ë¬¸ì„œë³„ë¡œ ê³ ìœ í•œ íŒ¨í„´ì„ ê°€ì§\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 5. ëª¨ë¸ êµ¬ì„± ë° í›ˆë ¨\n",
        "\n",
        "### ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜\n",
        "- **ì…ë ¥ì¸µ**: TF-IDF ë²¡í„° (20,000 ì°¨ì›)\n",
        "- **ì€ë‹‰ì¸µ**: Dense(16) + ReLU í™œì„±í™”\n",
        "- **ì •ê·œí™”**: Dropout(0.5) - ê³¼ì í•© ë°©ì§€\n",
        "- **ì¶œë ¥ì¸µ**: Dense(1) + Sigmoid - ì´ì§„ ë¶„ë¥˜\n",
        "\n",
        "### ğŸ¯ ëª¨ë¸ íŠ¹ì§•\n",
        "- **ê°„ë‹¨í•œ êµ¬ì¡°**: Dense ë ˆì´ì–´ë§Œ ì‚¬ìš©\n",
        "- **íš¨ìœ¨ì„±**: TF-IDFë¡œ ì´ë¯¸ ì˜ë¯¸ìˆëŠ” íŠ¹ì„± ì¶”ì¶œë¨\n",
        "- **ì •ê·œí™”**: ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§  ëª¨ë¸ ìƒì„± ì¤‘...\n",
            "âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!\n",
            "\n",
            "ğŸ“‹ ëª¨ë¸ êµ¬ì¡°:\n",
            "Model: \"tfidf_sentiment_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tfidf_input (InputLayer)    [(None, 20000)]           0         \n",
            "                                                                 \n",
            " hidden_layer (Dense)        (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë¸ ìƒì„± í•¨ìˆ˜\n",
        "def create_model(max_tokens=20000, hidden_dim=16):\n",
        "    \"\"\"\n",
        "    TF-IDF ê¸°ë°˜ ê°ì • ë¶„ì„ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    \n",
        "    Args:\n",
        "        max_tokens (int): ì…ë ¥ ë²¡í„°ì˜ ì°¨ì›ìˆ˜\n",
        "        hidden_dim (int): ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜\n",
        "    \n",
        "    Returns:\n",
        "        keras.Model: ì»´íŒŒì¼ëœ ëª¨ë¸\n",
        "    \"\"\"\n",
        "    # ëª¨ë¸ êµ¬ì¡° ì •ì˜\n",
        "    inputs = keras.Input(shape=(max_tokens,), name='tfidf_input')\n",
        "    \n",
        "    # ì€ë‹‰ì¸µ (ReLU í™œì„±í™”)\n",
        "    x = layers.Dense(hidden_dim, activation='relu', name='hidden_layer')(inputs)\n",
        "    \n",
        "    # ë“œë¡­ì•„ì›ƒ (ê³¼ì í•© ë°©ì§€)\n",
        "    x = layers.Dropout(0.5, name='dropout')(x)\n",
        "    \n",
        "    # ì¶œë ¥ì¸µ (ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” - ì´ì§„ ë¶„ë¥˜)\n",
        "    outputs = layers.Dense(1, activation='sigmoid', name='output_layer')(x)\n",
        "    \n",
        "    # ëª¨ë¸ ìƒì„±\n",
        "    model = keras.Model(inputs, outputs, name='tfidf_sentiment_model')\n",
        "    \n",
        "    # ì»´íŒŒì¼\n",
        "    model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# ëª¨ë¸ ìƒì„± ë° ìš”ì•½\n",
        "print(\"ğŸ§  ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
        "model = create_model()\n",
        "print(\"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ“‹ ëª¨ë¸ êµ¬ì¡°:\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\n",
            "==================================================\n",
            "Epoch 1/10\n",
            "615/625 [============================>.] - ETA: 0s - loss: 0.3765 - accuracy: 0.8399\n",
            "Epoch 1: val_accuracy improved from -inf to 0.88360, saving model to binary_2gram_tfidf.keras\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3760 - accuracy: 0.8403 - val_loss: 0.3047 - val_accuracy: 0.8836\n",
            "Epoch 2/10\n",
            "621/625 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8725\n",
            "Epoch 2: val_accuracy did not improve from 0.88360\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3135 - accuracy: 0.8724 - val_loss: 0.3309 - val_accuracy: 0.8704\n",
            "Epoch 3/10\n",
            "621/625 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.8854\n",
            "Epoch 3: val_accuracy did not improve from 0.88360\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2839 - accuracy: 0.8854 - val_loss: 0.3190 - val_accuracy: 0.8814\n",
            "Epoch 4/10\n",
            "616/625 [============================>.] - ETA: 0s - loss: 0.2586 - accuracy: 0.8912\n",
            "Epoch 4: val_accuracy did not improve from 0.88360\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2593 - accuracy: 0.8911 - val_loss: 0.3688 - val_accuracy: 0.8774\n",
            "Epoch 4: early stopping\n",
            "âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\n",
            "ğŸ’¾ ìµœì  ëª¨ë¸ì´ 'binary_2gram_tfidf.keras'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ì½œë°± ì„¤ì • (ìµœì  ëª¨ë¸ ì €ì¥)\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"binary_2gram_tfidf.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        verbose=1,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨\n",
        "# cache(): ì²« ë²ˆì§¸ ì—í¬í¬ì—ì„œ ì „ì²˜ë¦¬ë¥¼ í•œ ë²ˆë§Œ í•˜ê³  ë©”ëª¨ë¦¬ì— ìºì‹±\n",
        "history = model.fit(\n",
        "    binary_1gram_train_ds.cache(),\n",
        "    validation_data=binary_1gram_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ’¾ ìµœì  ëª¨ë¸ì´ 'binary_2gram_tfidf.keras'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 6. ëª¨ë¸ í‰ê°€ ë° ì˜ˆì¸¡\n",
        "\n",
        "### ğŸ“Š ì„±ëŠ¥ í‰ê°€\n",
        "ì €ì¥ëœ ìµœì  ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡\n",
        "ìƒˆë¡œìš´ ì˜í™” ë¦¬ë·°ì— ëŒ€í•´ ê¸ì •/ë¶€ì •ì„ ì˜ˆì¸¡í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ ìµœì  ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
            "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
            "\n",
            "ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€:\n",
            "----------------------------------------\n",
            "782/782 [==============================] - 38s 47ms/step - loss: 0.2996 - accuracy: 0.8848\n",
            "\n",
            "ğŸ“Š ìµœì¢… ì„±ëŠ¥:\n",
            "   - í…ŒìŠ¤íŠ¸ ì†ì‹¤: 0.2996\n",
            "   - í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.8848 (88.48%)\n"
          ]
        }
      ],
      "source": [
        "# ìµœì  ëª¨ë¸ ë¡œë“œ ë° í‰ê°€\n",
        "print(\"ğŸ“‚ ìµœì  ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "best_model = models.load_model(\"binary_2gram_tfidf.keras\")\n",
        "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "print(\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "test_results = best_model.evaluate(binary_1gram_test_ds, verbose=1)\n",
        "test_loss, test_accuracy = test_results\n",
        "\n",
        "print(f\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥:\")\n",
        "print(f\"   - í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}\")\n",
        "print(f\"   - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...\n",
            "âœ… ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì™„ì„±!\n",
            "\n",
            "ğŸ­ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ ê²°ê³¼:\n",
            "============================================================\n",
            "\n",
            "ë¦¬ë·° 1: That was an excellent movie, I loved it!\n",
            "   â†’ ì˜ˆì¸¡: ğŸ˜Š ê¸ì •\n",
            "   â†’ ì‹ ë¢°ë„: 98.1%\n",
            "   â†’ ê¸ì • í™•ë¥ : 98.1%\n",
            "\n",
            "ë¦¬ë·° 2: This movie was terrible and boring.\n",
            "   â†’ ì˜ˆì¸¡: ğŸ˜ ë¶€ì •\n",
            "   â†’ ì‹ ë¢°ë„: 86.7%\n",
            "   â†’ ê¸ì • í™•ë¥ : 13.3%\n",
            "\n",
            "ë¦¬ë·° 3: Amazing cinematography and great acting.\n",
            "   â†’ ì˜ˆì¸¡: ğŸ˜Š ê¸ì •\n",
            "   â†’ ì‹ ë¢°ë„: 91.6%\n",
            "   â†’ ê¸ì • í™•ë¥ : 91.6%\n",
            "\n",
            "ë¦¬ë·° 4: Worst movie I've ever seen. Complete waste of time.\n",
            "   â†’ ì˜ˆì¸¡: ğŸ˜ ë¶€ì •\n",
            "   â†’ ì‹ ë¢°ë„: 96.3%\n",
            "   â†’ ê¸ì • í™•ë¥ : 3.7%\n",
            "\n",
            "ë¦¬ë·° 5: Pretty good movie with some nice moments.\n",
            "   â†’ ì˜ˆì¸¡: ğŸ˜Š ê¸ì •\n",
            "   â†’ ì‹ ë¢°ë„: 78.7%\n",
            "   â†’ ê¸ì • í™•ë¥ : 78.7%\n",
            "\n",
            "ë¦¬ë·° 6: Absolutely horrible! I want my money back.\n",
            "   â†’ ì˜ˆì¸¡: ğŸ˜ ë¶€ì •\n",
            "   â†’ ì‹ ë¢°ë„: 80.0%\n",
            "   â†’ ê¸ì • í™•ë¥ : 20.0%\n"
          ]
        }
      ],
      "source": [
        "# ì‹¤ì‹œê°„ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ êµ¬ì„±\n",
        "print(\"ğŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...\")\n",
        "\n",
        "# ë¬¸ìì—´ ì…ë ¥ -> TF-IDF ë²¡í„°í™” -> ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\", name=\"text_input\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = best_model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs, name=\"inference_pipeline\")\n",
        "\n",
        "print(\"âœ… ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì™„ì„±!\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë¦¬ë·°ë“¤\n",
        "test_reviews = [\n",
        "    \"That was an excellent movie, I loved it!\",\n",
        "    \"This movie was terrible and boring.\",\n",
        "    \"Amazing cinematography and great acting.\",\n",
        "    \"Worst movie I've ever seen. Complete waste of time.\",\n",
        "    \"Pretty good movie with some nice moments.\",\n",
        "    \"Absolutely horrible! I want my money back.\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ­ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ ê²°ê³¼:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    markdown_text_data = tf.convert_to_tensor([[review]])\n",
        "    prediction = inference_model(markdown_text_data)\n",
        "    probability = prediction[0][0].numpy()\n",
        "    \n",
        "    # ê²°ê³¼ í•´ì„\n",
        "    sentiment = \"ğŸ˜Š ê¸ì •\" if probability > 0.5 else \"ğŸ˜ ë¶€ì •\"\n",
        "    confidence = probability if probability > 0.5 else (1 - probability)\n",
        "    \n",
        "    print(f\"\\në¦¬ë·° {i}: {review}\")\n",
        "    print(f\"   â†’ ì˜ˆì¸¡: {sentiment}\")\n",
        "    print(f\"   â†’ ì‹ ë¢°ë„: {confidence:.1%}\")\n",
        "    print(f\"   â†’ ê¸ì • í™•ë¥ : {probability:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## ğŸ‰ í”„ë¡œì íŠ¸ ì™„ë£Œ!\n",
        "\n",
        "### ğŸ“ˆ ì„±ê³¼ ìš”ì•½\n",
        "- **ë°ì´í„°**: IMDb ì˜í™” ë¦¬ë·° 50,000ê°œ ì²˜ë¦¬\n",
        "- **ë°©ë²•**: TF-IDF + 2-gram ë²¡í„°í™”\n",
        "- **ëª¨ë¸**: ê°„ë‹¨í•œ Dense Neural Network\n",
        "- **ê²°ê³¼**: í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì•½ 85-90% ë‹¬ì„± (ì¼ë°˜ì )\n",
        "\n",
        "### ğŸ” TF-IDFì˜ ì¥ì \n",
        "- âœ… **ë¹ ë¥¸ í›ˆë ¨**: ì‚¬ì „ ê³„ì‚°ëœ íŠ¹ì„±ìœ¼ë¡œ ë¹ ë¥¸ í•™ìŠµ\n",
        "- âœ… **í•´ì„ ê°€ëŠ¥**: ì–´ë–¤ ë‹¨ì–´ê°€ ì¤‘ìš”í•œì§€ í™•ì¸ ê°€ëŠ¥  \n",
        "- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨**: í¬ì†Œ í–‰ë ¬ë¡œ íš¨ìœ¨ì  ì €ì¥\n",
        "- âœ… **ê°•ê±´ì„±**: N-gramìœ¼ë¡œ ë¬¸ë§¥ ì •ë³´ ìº¡ì²˜\n",
        "\n",
        "### ğŸš€ ê°œì„  ë°©í–¥\n",
        "- **ì›Œë“œ ì„ë² ë”©**: Word2Vec, GloVe ë“± ì‚¬ìš©\n",
        "- **ë”¥ëŸ¬ë‹**: RNN, LSTM, Transformer ì ìš©\n",
        "- **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
        "- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì í™”\n",
        "\n",
        "---\n",
        "> ğŸ’¡ **ë‹¤ìŒ ë‹¨ê³„**: ì„ë² ë”© ê¸°ë°˜ ëª¨ë¸(`ìì—°ì–´_ì„ë² ë”©1.py`, `ìì—°ì–´_ì„ë² ë”©2.py`)ê³¼ ì„±ëŠ¥ ë¹„êµí•´ë³´ì„¸ìš”!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
