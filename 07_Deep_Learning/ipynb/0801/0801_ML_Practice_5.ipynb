{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습된 임베딩 계층 활용 (Word2Vec)\n",
    "\n",
    "이 노트북은 사전학습된 Word2Vec 임베딩을 활용하여 IMDB 영화 리뷰 감정 분석을 수행하는 방법을 다룹니다.\n",
    "\n",
    "## 목차\n",
    "1. [환경 설정 및 라이브러리 임포트](#1-환경-설정-및-라이브러리-임포트)\n",
    "2. [데이터 다운로드 및 전처리](#2-데이터-다운로드-및-전처리)\n",
    "3. [데이터셋 로딩 및 벡터화](#3-데이터셋-로딩-및-벡터화)\n",
    "4. [Word2Vec 사전학습된 임베딩 로딩](#4-word2vec-사전학습된-임베딩-로딩)\n",
    "5. [임베딩 매트릭스 생성](#5-임베딩-매트릭스-생성)\n",
    "6. [모델 구성 및 훈련](#6-모델-구성-및-훈련)\n",
    "7. [결과 평가](#7-결과-평가)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트\n",
    "\n",
    "### 필요한 라이브러리 설치\n",
    "```bash\n",
    "conda install gensim\n",
    "```\n",
    "\n",
    "### Word2Vec 모델 다운로드 링크\n",
    "- **Direct Link (S3 AWS)**: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "- **Hugging Face**: NathaNn1111/word2vec-google-news-negative-300-bin\n",
    "- **Kaggle**: https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "import os, pathlib, shutil, random\n",
    "import numpy as np\n",
    "import keras\n",
    "from gensim.models import KeyedVectors\n",
    "from keras import models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 다운로드 및 전처리\n",
    "\n",
    "### IMDB 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download():\n",
    "    \"\"\"IMDB 데이터셋을 다운로드합니다.\"\"\"\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    file_name = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "\n",
    "    print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 압축 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release():\n",
    "    \"\"\"다운로드한 tar.gz 파일을 압축 해제합니다.\"\"\"\n",
    "    subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"], shell=True)\n",
    "    print(\"압축풀기 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 라벨링 및 검증 세트 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling():\n",
    "    \"\"\"훈련 데이터를 훈련/검증 세트로 분리합니다.\"\"\"\n",
    "    base_dir = pathlib.Path(\"aclImdb\")\n",
    "    val_dir = base_dir/\"val\"\n",
    "    train_dir = base_dir/\"train\"\n",
    "\n",
    "    for category in (\"neg\", \"pos\"):\n",
    "        os.makedirs(val_dir/category)\n",
    "        files = os.listdir(train_dir/category)\n",
    "        random.Random(1337).shuffle(files)\n",
    "        num_val_samples = int(0.2 * len(files))\n",
    "        val_files = files[-num_val_samples:]\n",
    "        for fname in val_files:\n",
    "            shutil.move(train_dir/category/fname, val_dir/category/fname)\n",
    "\n",
    "# 데이터 준비 (필요시 주석 해제)\n",
    "# download()\n",
    "# release()\n",
    "# labeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 로딩 및 벡터화\n",
    "\n",
    "### 데이터셋 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\",\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs 샘플:\", inputs[:3])\n",
    "    print(\"targets 샘플:\", targets[:3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 벡터화 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600    # 한 리뷰당 최대 단어 수\n",
    "max_tokens = 20000  # 사용할 최대 단어 수\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",  # 임베딩 층을 위해 정수 출력\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "\n",
    "# 어휘사전 구축\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# 데이터셋 벡터화\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=1)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=1)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화된 데이터 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in int_train_ds:\n",
    "    print(\"벡터화된 데이터:\", item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vec 사전학습된 임베딩 로딩\n",
    "\n",
    "### Word2Vec 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "try:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    print(f\"Word2Vec 모델 로딩 성공 - 임베딩 차원: {embedding_dim}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"{filename}을 찾을 수 없습니다\")\n",
    "    print(\"Word2Vec 파일을 다운로드 후 ./data/ 폴더에 저장해주세요.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"에러 발생: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 임베딩 매트릭스 생성\n",
    "\n",
    "### 어휘사전과 임베딩 매트릭스 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘사전 가져오기\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "print(f\"어휘 사전 크기: {len(vocabulary)}\")\n",
    "print(f\"처음 10개 단어: {vocabulary[:10]}\")\n",
    "\n",
    "# 단어-인덱스 매핑 딕셔너리 생성\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "# 임베딩 매트릭스 초기화 (영벡터로)\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "print(f\"임베딩 매트릭스 shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 임베딩으로 매트릭스 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_words = 0\n",
    "total_words = 0\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        total_words += 1\n",
    "        try:\n",
    "            embedding_vector = word2vec_model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            matched_words += 1\n",
    "        except KeyError:\n",
    "            pass  # Word2Vec에 없는 단어는 영벡터로 유지\n",
    "\n",
    "print(f\"총 단어 수: {total_words}\")\n",
    "print(f\"Word2Vec에서 매칭된 단어 수: {matched_words}\")\n",
    "print(f\"매칭률: {matched_words/total_words*100:.2f}%\")\n",
    "\n",
    "# 임베딩 매트릭스 샘플 확인\n",
    "print(f\"\\n임베딩 매트릭스 처음 3개 단어의 벡터 (처음 10차원만):\")\n",
    "print(embedding_matrix[:3, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 구성 및 훈련\n",
    "\n",
    "### 임베딩과 임베딩 계층의 차이점\n",
    "\n",
    "- **원핫 인코딩**: 메모리를 많이 차지하고 희소행렬로 인해 학습 속도가 느림\n",
    "- **임베딩 층**: 단어 간 관계를 고려한 밀집 벡터 표현, 효율적인 메모리 사용\n",
    "- **사전학습된 임베딩**: 대규모 데이터로 미리 학습된 단어 표현 활용\n",
    "\n",
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# 사전학습된 임베딩 층\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=max_tokens,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),  # 사전학습된 가중치 사용\n",
    "    trainable=False,  # 임베딩 가중치 고정 (사전학습된 임베딩 보존)\n",
    "    mask_zero=True    # 패딩 토큰(0) 마스킹\n",
    ")(inputs)\n",
    "\n",
    "print(f\"임베딩 층 출력 shape: {embedded.shape}\")\n",
    "\n",
    "# 양방향 LSTM 층\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# 출력 층\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 모델 생성 및 컴파일\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=15,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 평가\n",
    "\n",
    "### 테스트 세트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(int_test_ds, verbose=0)\n",
    "print(f\"테스트 세트 결과:\")\n",
    "print(f\"  - 손실: {test_loss:.4f}\")\n",
    "print(f\"  - 정확도: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 훈련 과정 그래프\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 노트북에서는 다음을 학습했습니다:\n",
    "\n",
    "1. **사전학습된 Word2Vec 임베딩 활용**: Google News 데이터로 학습된 Word2Vec 모델 사용\n",
    "2. **임베딩 매트릭스 생성**: 어휘사전과 사전학습된 임베딩 매핑\n",
    "3. **고정된 임베딩 층**: `trainable=False`로 사전학습된 가중치 보존\n",
    "4. **양방향 LSTM**: 텍스트의 양방향 컨텍스트 활용\n",
    "5. **성능 개선**: 사전학습된 임베딩을 통한 일반화 성능 향상\n",
    "\n",
    "### 주요 장점\n",
    "- 적은 데이터로도 좋은 성능 달성 가능\n",
    "- 사전학습된 단어 표현으로 의미적 유사성 활용\n",
    "- 훈련 시간 단축 (임베딩 층 고정)\n",
    "\n",
    "### 다음 단계\n",
    "- 다른 사전학습된 임베딩 (GloVe, FastText) 비교\n",
    "- Fine-tuning (`trainable=True`) 실험\n",
    "- Transformer 기반 모델 (BERT, GPT) 활용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
