{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "# 🎬 영화 리뷰 감정 분석 - TF-IDF & BoW 방식\n",
        "\n",
        "## 📋 목차\n",
        "1. [환경 설정 및 라이브러리 임포트](#1-환경-설정-및-라이브러리-임포트)\n",
        "2. [데이터 다운로드 및 전처리](#2-데이터-다운로드-및-전처리)\n",
        "3. [데이터셋 로딩 및 확인](#3-데이터셋-로딩-및-확인)\n",
        "4. [텍스트 벡터화 (TF-IDF)](#4-텍스트-벡터화-tf-idf)\n",
        "5. [모델 구성 및 훈련](#5-모델-구성-및-훈련)\n",
        "6. [모델 평가 및 예측](#6-모델-평가-및-예측)\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 프로젝트 개요\n",
        "- **목표**: IMDb 영화 리뷰 데이터를 사용한 긍정/부정 감정 분석\n",
        "- **방법**: TF-IDF (Term Frequency-Inverse Document Frequency) 벡터화\n",
        "- **특징**: N-gram (2-gram) 사용으로 단어 조합 패턴 학습\n",
        "- **모델**: 간단한 Dense Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 1. 환경 설정 및 라이브러리 임포트\n",
        "\n",
        "필요한 라이브러리들을 임포트합니다:\n",
        "- **requests**: 데이터 다운로드\n",
        "- **tensorflow/keras**: 딥러닝 모델 구성\n",
        "- **TextVectorization**: 텍스트 벡터화\n",
        "- **기타**: 파일 처리, 시스템 관련\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 버전: 2.15.1\n",
            "Keras 버전: 2.15.0\n",
            "✅ 라이브러리 임포트 완료!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import os, pathlib, shutil, random\n",
        "import keras\n",
        "from keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "print(\"TensorFlow 버전:\", tf.__version__)\n",
        "print(\"Keras 버전:\", keras.__version__)\n",
        "print(\"✅ 라이브러리 임포트 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 2. 데이터 다운로드 및 전처리\n",
        "\n",
        "IMDb 영화 리뷰 데이터셋을 다운로드하고 전처리합니다:\n",
        "\n",
        "### 📁 데이터셋 정보\n",
        "- **출처**: Stanford AI Lab\n",
        "- **크기**: 약 84MB (압축)\n",
        "- **구성**: 25,000개 훈련 리뷰 + 25,000개 테스트 리뷰\n",
        "- **라벨**: pos(긍정), neg(부정)\n",
        "\n",
        "### 🔄 전처리 과정\n",
        "1. **다운로드**: 압축 파일 다운로드\n",
        "2. **압축 해제**: tar.gz 파일 해제\n",
        "3. **라벨링**: train/validation 분할 (80:20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 데이터 전처리 함수들이 정의되었습니다.\n",
            "💡 필요시 다음 함수들을 호출하세요:\n",
            "   - download(): 데이터셋 다운로드\n",
            "   - release(): 압축 해제\n",
            "   - labeling(): 데이터 분할\n"
          ]
        }
      ],
      "source": [
        "# 데이터 다운로드 함수\n",
        "def download():\n",
        "    \"\"\"IMDb 데이터셋을 다운로드합니다.\"\"\"\n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    file_name = \"aclImdb_v1.tar.gz\"\n",
        "    \n",
        "    print(\"📥 데이터셋 다운로드 시작...\")\n",
        "    response = requests.get(url, stream=True)  # 스트리밍 방식으로 다운로드\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):  # 8KB씩 다운로드\n",
        "            file.write(chunk)\n",
        "    \n",
        "    print(\"✅ 다운로드 완료!\")\n",
        "\n",
        "# 압축 해제 함수\n",
        "def release():\n",
        "    \"\"\"다운로드한 압축 파일을 해제합니다.\"\"\"\n",
        "    print(\"📂 압축 해제 중...\")\n",
        "    subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"], shell=True)\n",
        "    print(\"✅ 압축 해제 완료!\")\n",
        "\n",
        "# 데이터 라벨링 및 분할 함수\n",
        "def labeling():\n",
        "    \"\"\"train 데이터를 train/validation으로 분할합니다.\"\"\"\n",
        "    print(\"🏷️ 데이터 라벨링 및 분할 중...\")\n",
        "    base_dir = pathlib.Path(\"aclImdb\")\n",
        "    val_dir = base_dir / \"val\"\n",
        "    train_dir = base_dir / \"train\"\n",
        "    \n",
        "    for category in (\"neg\", \"pos\"):\n",
        "        os.makedirs(val_dir / category, exist_ok=True)\n",
        "        files = os.listdir(train_dir / category)\n",
        "        random.Random(1337).shuffle(files)  # 재현 가능한 랜덤 셔플\n",
        "        num_val_samples = int(0.2 * len(files))  # 20%를 validation으로\n",
        "        val_files = files[-num_val_samples:]\n",
        "        \n",
        "        for fname in val_files:\n",
        "            shutil.move(train_dir / category / fname, val_dir / category / fname)\n",
        "    \n",
        "    print(\"✅ 라벨링 및 분할 완료!\")\n",
        "\n",
        "print(\"📋 데이터 전처리 함수들이 정의되었습니다.\")\n",
        "print(\"💡 필요시 다음 함수들을 호출하세요:\")\n",
        "print(\"   - download(): 데이터셋 다운로드\")\n",
        "print(\"   - release(): 압축 해제\") \n",
        "print(\"   - labeling(): 데이터 분할\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 3. 데이터셋 로딩 및 확인\n",
        "\n",
        "Keras의 `text_dataset_from_directory`를 사용하여 텍스트 데이터를 로딩합니다.\n",
        "\n",
        "### 📊 데이터셋 구조\n",
        "- **폴더 구조**: `aclImdb/train/pos`, `aclImdb/train/neg` 등\n",
        "- **배치 크기**: 32개씩 처리\n",
        "- **라벨**: 0(부정), 1(긍정) - 폴더명 알파벳 순서\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 데이터셋 로딩 중...\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "✅ 데이터셋 로딩 완료!\n",
            "📊 배치 크기: 32\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 설정\n",
        "batch_size = 32  # 한 번에 처리할 데이터 개수\n",
        "\n",
        "print(\"📂 데이터셋 로딩 중...\")\n",
        "\n",
        "# 훈련용 데이터셋\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"../../data/aclImdb/train\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# 검증용 데이터셋  \n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"../../data/aclImdb/val\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# 테스트용 데이터셋\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"../../data/aclImdb/test\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\"✅ 데이터셋 로딩 완료!\")\n",
        "print(f\"📊 배치 크기: {batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 데이터 구조 확인:\n",
            "--------------------------------------------------\n",
            "📝 입력 데이터 (리뷰 텍스트):\n",
            "   - 형태: (32,)\n",
            "   - 타입: <dtype: 'string'>\n",
            "\n",
            "🎯 타겟 데이터 (라벨):\n",
            "   - 형태: (32,)\n",
            "   - 타입: <dtype: 'int32'>\n",
            "\n",
            "📖 샘플 데이터 (처음 3개):\n",
            "   리뷰 1: This is a truly magnificent and heartwrenching film!!!! Ripstein's locations are spectacular, extrem...\n",
            "   라벨 1: 1 (긍정)\n",
            "\n",
            "   리뷰 2: Jimmy Cagney races by your eyes constantly in this story of a stage-producer who is vigorously strug...\n",
            "   라벨 2: 1 (긍정)\n",
            "\n",
            "   리뷰 3: SWING! is an important film because it's one of the remaining Black-produced and acted films from th...\n",
            "   라벨 3: 0 (부정)\n",
            "\n",
            "💡 라벨 정보: 0=부정(neg), 1=긍정(pos)\n"
          ]
        }
      ],
      "source": [
        "# 데이터 구조 확인\n",
        "print(\"🔍 데이터 구조 확인:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"📝 입력 데이터 (리뷰 텍스트):\")\n",
        "    print(f\"   - 형태: {inputs.shape}\")\n",
        "    print(f\"   - 타입: {inputs.dtype}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"🎯 타겟 데이터 (라벨):\")\n",
        "    print(f\"   - 형태: {targets.shape}\")\n",
        "    print(f\"   - 타입: {targets.dtype}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"📖 샘플 데이터 (처음 3개):\")\n",
        "    for i in range(3):\n",
        "        print(f\"   리뷰 {i+1}: {inputs[i].numpy().decode('utf-8')[:100]}...\")\n",
        "        print(f\"   라벨 {i+1}: {targets[i].numpy()} ({'긍정' if targets[i].numpy() == 1 else '부정'})\")\n",
        "        print()\n",
        "    \n",
        "    break  # 첫 번째 배치만 확인\n",
        "\n",
        "print(\"💡 라벨 정보: 0=부정(neg), 1=긍정(pos)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 4. 텍스트 벡터화 (TF-IDF)\n",
        "\n",
        "### 🔤 TF-IDF란?\n",
        "- **TF (Term Frequency)**: 문서 내 단어 빈도\n",
        "- **IDF (Inverse Document Frequency)**: 단어의 희귀성 \n",
        "- **조합**: 자주 나오지만 특별한 의미를 가진 단어에 높은 가중치\n",
        "\n",
        "### ⚙️ 설정 파라미터\n",
        "- **max_tokens**: 20,000개 (가장 빈번한 단어들)\n",
        "- **output_mode**: \"tf_idf\" (TF-IDF 가중치 적용)\n",
        "- **ngrams**: 2 (단어 2개 조합도 고려)\n",
        "\n",
        "### 🎯 N-gram의 효과\n",
        "- **1-gram**: \"good\", \"movie\" \n",
        "- **2-gram**: \"good movie\", \"very good\"\n",
        "- 문맥 정보를 더 잘 캡처할 수 있음\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔤 TF-IDF 벡터화 설정 중...\n",
            "✅ 벡터화 설정 완료!\n",
            "📊 설정 정보:\n",
            "   - 최대 토큰 수: 20,000\n",
            "   - 출력 모드: TF-IDF\n",
            "   - N-gram: 2 (단어 조합 고려)\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF 벡터화 설정\n",
        "print(\"🔤 TF-IDF 벡터화 설정 중...\")\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,           # 자주 사용하는 단어 20,000개만 사용\n",
        "    output_mode=\"tf_idf\",       # TF-IDF 가중치 적용\n",
        "    ngrams=2                    # 2-gram 사용 (단어 조합 고려)\n",
        ")\n",
        "\n",
        "print(\"✅ 벡터화 설정 완료!\")\n",
        "print(\"📊 설정 정보:\")\n",
        "print(f\"   - 최대 토큰 수: {20000:,}\")\n",
        "print(f\"   - 출력 모드: TF-IDF\")\n",
        "print(f\"   - N-gram: 2 (단어 조합 고려)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📚 어휘사전 생성 중...\n",
            "✅ 어휘사전 생성 완료!\n",
            "🔄 데이터셋 벡터화 중...\n",
            "   - 멀티프로세싱으로 빠른 처리...\n",
            "✅ 모든 데이터셋 벡터화 완료!\n"
          ]
        }
      ],
      "source": [
        "# 어휘사전 생성 (텍스트만 추출하여 학습)\n",
        "print(\"📚 어휘사전 생성 중...\")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)  # 텍스트만 추출\n",
        "text_vectorization.adapt(text_only_train_ds)       # 어휘사전 생성\n",
        "print(\"✅ 어휘사전 생성 완료!\")\n",
        "\n",
        "# 모든 데이터셋에 벡터화 적용\n",
        "print(\"🔄 데이터셋 벡터화 중...\")\n",
        "print(\"   - 멀티프로세싱으로 빠른 처리...\")\n",
        "\n",
        "# 멀티프로세싱을 사용한 벡터화 (CPU 코어 4개 활용)\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "print(\"✅ 모든 데이터셋 벡터화 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 벡터화 결과 확인:\n",
            "--------------------------------------------------\n",
            "📊 벡터화된 입력 데이터:\n",
            "   - 형태: (32, 20000)\n",
            "   - 타입: <dtype: 'float32'>\n",
            "   - 값 범위: 0.000000 ~ 3422.129639\n",
            "\n",
            "🎯 타겟 데이터:\n",
            "   - 형태: (32,)\n",
            "   - 타입: <dtype: 'int32'>\n",
            "\n",
            "📈 첫 번째 샘플의 TF-IDF 값 (0이 아닌 값들):\n",
            "   - 활성화된 특성 수: 104/20000\n",
            "   - 처음 10개 비0 값: [293.1731      6.974498    0.7111458   4.2621336   2.8794088   1.4509387\n",
            "   3.7522266   0.7591958   4.6798143   1.6230419]\n",
            "\n",
            "💡 TF-IDF 특징:\n",
            "   - 값이 클수록 해당 단어/구문이 문서에서 중요함\n",
            "   - 대부분의 값이 0 (희소 행렬)\n",
            "   - 문서별로 고유한 패턴을 가짐\n"
          ]
        }
      ],
      "source": [
        "# 벡터화 결과 확인\n",
        "print(\"🔍 벡터화 결과 확인:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"📊 벡터화된 입력 데이터:\")\n",
        "    print(f\"   - 형태: {inputs.shape}\")\n",
        "    print(f\"   - 타입: {inputs.dtype}\")\n",
        "    print(f\"   - 값 범위: {inputs.numpy().min():.6f} ~ {inputs.numpy().max():.6f}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"🎯 타겟 데이터:\")\n",
        "    print(f\"   - 형태: {targets.shape}\")\n",
        "    print(f\"   - 타입: {targets.dtype}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"📈 첫 번째 샘플의 TF-IDF 값 (0이 아닌 값들):\")\n",
        "    first_sample = inputs[0].numpy()\n",
        "    non_zero_indices = np.where(first_sample > 0)[0]\n",
        "    print(f\"   - 활성화된 특성 수: {len(non_zero_indices)}/{len(first_sample)}\")\n",
        "    print(f\"   - 처음 10개 비0 값: {first_sample[non_zero_indices[:10]]}\")\n",
        "    print()\n",
        "    \n",
        "    break\n",
        "\n",
        "print(\"💡 TF-IDF 특징:\")\n",
        "print(\"   - 값이 클수록 해당 단어/구문이 문서에서 중요함\")\n",
        "print(\"   - 대부분의 값이 0 (희소 행렬)\")\n",
        "print(\"   - 문서별로 고유한 패턴을 가짐\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 5. 모델 구성 및 훈련\n",
        "\n",
        "### 🧠 모델 아키텍처\n",
        "- **입력층**: TF-IDF 벡터 (20,000 차원)\n",
        "- **은닉층**: Dense(16) + ReLU 활성화\n",
        "- **정규화**: Dropout(0.5) - 과적합 방지\n",
        "- **출력층**: Dense(1) + Sigmoid - 이진 분류\n",
        "\n",
        "### 🎯 모델 특징\n",
        "- **간단한 구조**: Dense 레이어만 사용\n",
        "- **효율성**: TF-IDF로 이미 의미있는 특성 추출됨\n",
        "- **정규화**: 드롭아웃으로 일반화 성능 향상\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 모델 생성 중...\n",
            "✅ 모델 생성 완료!\n",
            "\n",
            "📋 모델 구조:\n",
            "Model: \"tfidf_sentiment_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tfidf_input (InputLayer)    [(None, 20000)]           0         \n",
            "                                                                 \n",
            " hidden_layer (Dense)        (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 모델 생성 함수\n",
        "def create_model(max_tokens=20000, hidden_dim=16):\n",
        "    \"\"\"\n",
        "    TF-IDF 기반 감정 분석 모델을 생성합니다.\n",
        "    \n",
        "    Args:\n",
        "        max_tokens (int): 입력 벡터의 차원수\n",
        "        hidden_dim (int): 은닉층의 뉴런 수\n",
        "    \n",
        "    Returns:\n",
        "        keras.Model: 컴파일된 모델\n",
        "    \"\"\"\n",
        "    # 모델 구조 정의\n",
        "    inputs = keras.Input(shape=(max_tokens,), name='tfidf_input')\n",
        "    \n",
        "    # 은닉층 (ReLU 활성화)\n",
        "    x = layers.Dense(hidden_dim, activation='relu', name='hidden_layer')(inputs)\n",
        "    \n",
        "    # 드롭아웃 (과적합 방지)\n",
        "    x = layers.Dropout(0.5, name='dropout')(x)\n",
        "    \n",
        "    # 출력층 (시그모이드 활성화 - 이진 분류)\n",
        "    outputs = layers.Dense(1, activation='sigmoid', name='output_layer')(x)\n",
        "    \n",
        "    # 모델 생성\n",
        "    model = keras.Model(inputs, outputs, name='tfidf_sentiment_model')\n",
        "    \n",
        "    # 컴파일\n",
        "    model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# 모델 생성 및 요약\n",
        "print(\"🧠 모델 생성 중...\")\n",
        "model = create_model()\n",
        "print(\"✅ 모델 생성 완료!\")\n",
        "print()\n",
        "\n",
        "print(\"📋 모델 구조:\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 모델 훈련 시작!\n",
            "==================================================\n",
            "Epoch 1/10\n",
            "615/625 [============================>.] - ETA: 0s - loss: 0.3765 - accuracy: 0.8399\n",
            "Epoch 1: val_accuracy improved from -inf to 0.88360, saving model to binary_2gram_tfidf.keras\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3760 - accuracy: 0.8403 - val_loss: 0.3047 - val_accuracy: 0.8836\n",
            "Epoch 2/10\n",
            "621/625 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8725\n",
            "Epoch 2: val_accuracy did not improve from 0.88360\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3135 - accuracy: 0.8724 - val_loss: 0.3309 - val_accuracy: 0.8704\n",
            "Epoch 3/10\n",
            "621/625 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.8854\n",
            "Epoch 3: val_accuracy did not improve from 0.88360\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2839 - accuracy: 0.8854 - val_loss: 0.3190 - val_accuracy: 0.8814\n",
            "Epoch 4/10\n",
            "616/625 [============================>.] - ETA: 0s - loss: 0.2586 - accuracy: 0.8912\n",
            "Epoch 4: val_accuracy did not improve from 0.88360\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2593 - accuracy: 0.8911 - val_loss: 0.3688 - val_accuracy: 0.8774\n",
            "Epoch 4: early stopping\n",
            "✅ 모델 훈련 완료!\n",
            "💾 최적 모델이 'binary_2gram_tfidf.keras'로 저장되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# 콜백 설정 (최적 모델 저장)\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"binary_2gram_tfidf.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        verbose=1,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"🚀 모델 훈련 시작!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 모델 훈련\n",
        "# cache(): 첫 번째 에포크에서 전처리를 한 번만 하고 메모리에 캐싱\n",
        "history = model.fit(\n",
        "    binary_1gram_train_ds.cache(),\n",
        "    validation_data=binary_1gram_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"✅ 모델 훈련 완료!\")\n",
        "print(f\"💾 최적 모델이 'binary_2gram_tfidf.keras'로 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 6. 모델 평가 및 예측\n",
        "\n",
        "### 📊 성능 평가\n",
        "저장된 최적 모델을 로드하여 테스트 데이터로 성능을 평가합니다.\n",
        "\n",
        "### 🔮 실시간 예측\n",
        "새로운 영화 리뷰에 대해 긍정/부정을 예측하는 파이프라인을 구성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 최적 모델 로드 중...\n",
            "✅ 모델 로드 완료!\n",
            "\n",
            "🧪 테스트 데이터셋 평가:\n",
            "----------------------------------------\n",
            "782/782 [==============================] - 38s 47ms/step - loss: 0.2996 - accuracy: 0.8848\n",
            "\n",
            "📊 최종 성능:\n",
            "   - 테스트 손실: 0.2996\n",
            "   - 테스트 정확도: 0.8848 (88.48%)\n"
          ]
        }
      ],
      "source": [
        "# 최적 모델 로드 및 평가\n",
        "print(\"📂 최적 모델 로드 중...\")\n",
        "best_model = models.load_model(\"binary_2gram_tfidf.keras\")\n",
        "print(\"✅ 모델 로드 완료!\")\n",
        "\n",
        "print(\"\\n🧪 테스트 데이터셋 평가:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "test_results = best_model.evaluate(binary_1gram_test_ds, verbose=1)\n",
        "test_loss, test_accuracy = test_results\n",
        "\n",
        "print(f\"\\n📊 최종 성능:\")\n",
        "print(f\"   - 테스트 손실: {test_loss:.4f}\")\n",
        "print(f\"   - 테스트 정확도: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔮 실시간 예측 파이프라인 구성 중...\n",
            "✅ 예측 파이프라인 완성!\n",
            "\n",
            "🎭 영화 리뷰 감정 분석 결과:\n",
            "============================================================\n",
            "\n",
            "리뷰 1: That was an excellent movie, I loved it!\n",
            "   → 예측: 😊 긍정\n",
            "   → 신뢰도: 98.1%\n",
            "   → 긍정 확률: 98.1%\n",
            "\n",
            "리뷰 2: This movie was terrible and boring.\n",
            "   → 예측: 😞 부정\n",
            "   → 신뢰도: 86.7%\n",
            "   → 긍정 확률: 13.3%\n",
            "\n",
            "리뷰 3: Amazing cinematography and great acting.\n",
            "   → 예측: 😊 긍정\n",
            "   → 신뢰도: 91.6%\n",
            "   → 긍정 확률: 91.6%\n",
            "\n",
            "리뷰 4: Worst movie I've ever seen. Complete waste of time.\n",
            "   → 예측: 😞 부정\n",
            "   → 신뢰도: 96.3%\n",
            "   → 긍정 확률: 3.7%\n",
            "\n",
            "리뷰 5: Pretty good movie with some nice moments.\n",
            "   → 예측: 😊 긍정\n",
            "   → 신뢰도: 78.7%\n",
            "   → 긍정 확률: 78.7%\n",
            "\n",
            "리뷰 6: Absolutely horrible! I want my money back.\n",
            "   → 예측: 😞 부정\n",
            "   → 신뢰도: 80.0%\n",
            "   → 긍정 확률: 20.0%\n"
          ]
        }
      ],
      "source": [
        "# 실시간 예측 파이프라인 구성\n",
        "print(\"🔮 실시간 예측 파이프라인 구성 중...\")\n",
        "\n",
        "# 문자열 입력 -> TF-IDF 벡터화 -> 예측 파이프라인\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\", name=\"text_input\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = best_model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs, name=\"inference_pipeline\")\n",
        "\n",
        "print(\"✅ 예측 파이프라인 완성!\")\n",
        "\n",
        "# 테스트 리뷰들\n",
        "test_reviews = [\n",
        "    \"That was an excellent movie, I loved it!\",\n",
        "    \"This movie was terrible and boring.\",\n",
        "    \"Amazing cinematography and great acting.\",\n",
        "    \"Worst movie I've ever seen. Complete waste of time.\",\n",
        "    \"Pretty good movie with some nice moments.\",\n",
        "    \"Absolutely horrible! I want my money back.\"\n",
        "]\n",
        "\n",
        "print(\"\\n🎭 영화 리뷰 감정 분석 결과:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    # 예측 수행\n",
        "    markdown_text_data = tf.convert_to_tensor([[review]])\n",
        "    prediction = inference_model(markdown_text_data)\n",
        "    probability = prediction[0][0].numpy()\n",
        "    \n",
        "    # 결과 해석\n",
        "    sentiment = \"😊 긍정\" if probability > 0.5 else \"😞 부정\"\n",
        "    confidence = probability if probability > 0.5 else (1 - probability)\n",
        "    \n",
        "    print(f\"\\n리뷰 {i}: {review}\")\n",
        "    print(f\"   → 예측: {sentiment}\")\n",
        "    print(f\"   → 신뢰도: {confidence:.1%}\")\n",
        "    print(f\"   → 긍정 확률: {probability:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## 🎉 프로젝트 완료!\n",
        "\n",
        "### 📈 성과 요약\n",
        "- **데이터**: IMDb 영화 리뷰 50,000개 처리\n",
        "- **방법**: TF-IDF + 2-gram 벡터화\n",
        "- **모델**: 간단한 Dense Neural Network\n",
        "- **결과**: 테스트 정확도 약 85-90% 달성 (일반적)\n",
        "\n",
        "### 🔍 TF-IDF의 장점\n",
        "- ✅ **빠른 훈련**: 사전 계산된 특성으로 빠른 학습\n",
        "- ✅ **해석 가능**: 어떤 단어가 중요한지 확인 가능  \n",
        "- ✅ **메모리 효율**: 희소 행렬로 효율적 저장\n",
        "- ✅ **강건성**: N-gram으로 문맥 정보 캡처\n",
        "\n",
        "### 🚀 개선 방향\n",
        "- **워드 임베딩**: Word2Vec, GloVe 등 사용\n",
        "- **딥러닝**: RNN, LSTM, Transformer 적용\n",
        "- **앙상블**: 여러 모델 조합으로 성능 향상\n",
        "- **하이퍼파라미터**: 그리드 서치로 최적화\n",
        "\n",
        "---\n",
        "> 💡 **다음 단계**: 임베딩 기반 모델(`자연어_임베딩1.py`, `자연어_임베딩2.py`)과 성능 비교해보세요!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
