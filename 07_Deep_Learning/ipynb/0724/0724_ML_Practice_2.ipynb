{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import load_model \n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random \n",
    "import PIL.Image as pilimg \n",
    "import imghdr\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import keras \n",
    "import os\n",
    "import shutil # ë””ë ‰í† ë¦¬ ë§Œë“¤ê¸°, \n",
    "\n",
    "base_path = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "\n",
    "print(\"daisy\", len(os.listdir(base_path+\"/daisy\")))\n",
    "print(\"dandelion\", len(os.listdir(base_path+\"/dandelion\")))\n",
    "print(\"sunflower\", len(os.listdir(base_path+\"/sunflower\")))\n",
    "print(\"rose\", len(os.listdir(base_path+\"/rose\")))\n",
    "print(\"tulip\", len(os.listdir(base_path+\"/tulip\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_images_in_class_folder(src_class_dir, class_name, dest_dir):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    files.sort()\n",
    "\n",
    "    for idx, fname in enumerate(files):\n",
    "        src_path = os.path.join(src_class_dir, fname)\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "        new_name = f\"{class_name}.{idx}{ext}\"\n",
    "        dst_path = os.path.join(dest_dir, new_name)\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "    print(f\"âœ… {class_name} ë¦¬ë„¤ì„ ì™„ë£Œ: {len(files)}ê°œ ì²˜ë¦¬ë¨.\")\n",
    "\n",
    "def rename_all_classes(original_root_dir, renamed_root_dir):\n",
    "    classes = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "\n",
    "    if os.path.exists(renamed_root_dir):\n",
    "        shutil.rmtree(renamed_root_dir)\n",
    "    os.makedirs(renamed_root_dir, exist_ok=True)\n",
    "\n",
    "    for class_name in classes:\n",
    "        src_class_dir = os.path.join(original_root_dir, class_name)\n",
    "        rename_images_in_class_folder(src_class_dir, class_name, renamed_root_dir)\n",
    "\n",
    "def copy_images_by_class(class_name, original_dataset_dir, dest_dirs, split_ratio=(0.5, 0.25, 0.25)):\n",
    "    image_files = [\n",
    "        f for f in os.listdir(original_dataset_dir)\n",
    "        if f.startswith(f\"{class_name}.\") and f.lower().endswith(('.jpg', '.jpeg', '.png')) and os.path.isfile(os.path.join(original_dataset_dir, f))\n",
    "    ]\n",
    "    image_files.sort()\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    total = len(image_files)\n",
    "    train_end = int(total * split_ratio[0])\n",
    "    val_end = train_end + int(total * split_ratio[1])\n",
    "\n",
    "    splits = [image_files[:train_end], image_files[train_end:val_end], image_files[val_end:]]\n",
    "\n",
    "    for split, dst_dir in zip(splits, dest_dirs):\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        for fname in split:\n",
    "            src = os.path.join(original_dataset_dir, fname)\n",
    "            dst = os.path.join(dst_dir, fname)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "def ImageCopy(renamed_dataset_dir, base_dir):\n",
    "    categories = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            os.makedirs(os.path.join(base_dir, set_name, category), exist_ok=True)\n",
    "\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    val_dir = os.path.join(base_dir, \"validation\")\n",
    "    test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"ğŸ”„ {category} ë¶„í•  ì¤‘...\")\n",
    "        copy_images_by_class(\n",
    "            class_name=category,\n",
    "            original_dataset_dir=renamed_dataset_dir,\n",
    "            dest_dirs=[\n",
    "                os.path.join(train_dir, category),\n",
    "                os.path.join(val_dir, category),\n",
    "                os.path.join(test_dir, category)\n",
    "            ],\n",
    "            split_ratio=(0.5, 0.25, 0.25)\n",
    "        )\n",
    "\n",
    "    print(\"\\nâœ… ì´ë¯¸ì§€ ë¶„í•  ë³µì‚¬ ì™„ë£Œ!\\n\")\n",
    "\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            dir_path = os.path.join(base_dir, set_name, category)\n",
    "            count = len(os.listdir(dir_path))\n",
    "            print(f\"ğŸ“ {set_name}/{category}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ed73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ ë°ì´í„°ì…‹ í´ë” (í´ë˜ìŠ¤ë³„ í•˜ìœ„ í´ë” ìˆìŒ)\n",
    "original_dataset_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "\n",
    "# ë¦¬ë„¤ì„ëœ ì´ë¯¸ì§€ë“¤ì´ ì €ì¥ë  ìœ„ì¹˜\n",
    "renamed_root = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_renamed\"\n",
    "\n",
    "# ìµœì¢… ë¶„í• ëœ train/val/test í´ë” ìƒì„± ìœ„ì¹˜\n",
    "base_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/0724/flowers_small\"\n",
    "\n",
    "# 1ë‹¨ê³„: í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ë“¤ì„ daisy.0.jpg í˜•ì‹ìœ¼ë¡œ ë¦¬ë„¤ì„ + í†µí•©\n",
    "rename_all_classes(original_dataset_dir, renamed_root)\n",
    "\n",
    "# 2ë‹¨ê³„: ë¦¬ë„¤ì„ëœ ì´ë¯¸ì§€ë¥¼ 2:1:1 ë¹„ìœ¨ë¡œ train/validation/test ë¶„í•  ë³µì‚¬\n",
    "ImageCopy(renamed_root, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4568df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "from pathlib import Path\n",
    "\n",
    "# batch_sizeì— ì§€ì •ëœ ë§Œí¼ í´ë”ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ì½ì–´ì˜¨ë‹¤. í¬ê¸°ëŠ” image_sizeì— ì§€ì •í•œ ê°’ìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤.\n",
    "# í›ˆë ¨ì…‹ì„ ìª¼ê°œì„œ 8:2 ì •ë„ë¡œ ê²€ì¦ì…‹ì„ ë”°ë¡œ ë§Œë“œëŠ” ë°©ë²•ë„ ìˆê³ , subset ì†ì„±, seed ë¥¼ ì´ìš©í•´ ë‚˜ëˆ ì•¼ í•œë‹¤.\n",
    "base_dir = Path(base_dir)\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    base_dir/\"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16\n",
    ")\n",
    "validation_ds = image_dataset_from_directory(\n",
    "    base_dir/\"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16\n",
    ")\n",
    "test_ds = image_dataset_from_directory(\n",
    "    base_dir/\"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7040c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 ì´ë¯¸ì§€ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "def deeplearning():\n",
    "    conv_base = keras.applications.vgg19.VGG19(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False, # CNNë§Œ ê°€ì ¸ì™€ë¼, CNNì´ í•˜ë‹¨ì— ìˆìŒ, ìƒë‹¨ - ì™„ì „ì—°ê²°ë§(ë¶„ë¥˜)\n",
    "        input_shape=(180, 180, 3) # ì…ë ¥í•  ë°ì´í„° í¬ê¸°ë¥¼ ì£¼ì–´ì•¼ í•œë‹¤.\n",
    "        # ë°ì´í„°ì…‹ì—ì„œ ì§€ì •í•œ í¬ê¸°ì™€ ì¼ì¹˜í•´ì•¼ í•œë‹¤, 3-ìƒ‰ì •ë³´\n",
    "    )\n",
    "\n",
    "    conv_base.summary() # CNN ìš”ì•½ í™•ì¸í•˜ê¸°\n",
    "    # block5_pool(MaxPooling2D) (None, 5, 5, 512)\n",
    "\n",
    "    conv_base.trainable = True\n",
    "    print(\"í•©ì„±ê³± ê¸°ë°˜ ì¸µì„ ë™ê²°í•˜ê¸° ì „ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ê°œìˆ˜\", len(conv_base.trainable_weights))\n",
    "    conv_base.trainable = False # ë™ê²°\n",
    "    print(\"í•©ì„±ê³± ê¸°ë°˜ ì¸µì„ ë™ê²° í›„ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ê°œìˆ˜\", len(conv_base.trainable_weights))\n",
    "\n",
    "    data_argumentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.4)\n",
    "    ])\n",
    "\n",
    "    # ëª¨ë¸ ë§Œë“¤ê¸°\n",
    "    inputs = keras.Input(shape=(180, 180, 3)) # ëª¨ë¸ì˜ ì…ë ¥ë ˆì´ì–´ ì •ì˜ \n",
    "    x = data_argumentation(inputs) # ì…ë ¥ì´ë¯¸ì§€ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•œë‹¤.\n",
    "    x = keras.applications.vgg19.preprocess_input(x) # vgg19 ëª¨ë¸ì— ë§ëŠ” ì „ì²˜ë¦¬ì‘ì—…(í”½ì…€ê°’ë²”ìœ„ì¡°ì • ë“±)\n",
    "\n",
    "    # ì¸ë¼ì¸ë°©ì‹ìœ¼ë¡œ cnn ì—°ê²°í•˜ê¸°\n",
    "    x = conv_base(x) # íŠ¹ì„±ì¶”ì¶œì´ ì´ë¤„ì§„ë‹¤. ì˜¤ë˜ ê±¸ë¦°ë‹¤\n",
    "    #################################################\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Dense(64)(x)\n",
    "    outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    # ì‹œìŠ¤í…œì´ ë‚´ë¶€ì ìœ¼ë¡œ ì¼ ì²˜ë¦¬í•˜ê³  ì¼ ëë‚˜ë©´ ìš°ë¦¬ê°€ ì „ë‹¬í•´ì¤€ ì½œë°±í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•œë‹¤.\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"ê½ƒ.keras\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\" # ê²€ì¦ë°ì´í„° ì…‹ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ê² ë‹¤ ê°€ì¥ ì ì ˆí•œ ì‹œì ì— í˜¸ì¶œí• ê±°ë‹¤.\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(train_ds,\n",
    "                        epochs=10,\n",
    "                        validation_data=validation_ds,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    with open(\"ê½ƒ.bin\", \"wb\") as file:\n",
    "              pickle.dump(history.history, file)\n",
    "\n",
    "deeplearning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
