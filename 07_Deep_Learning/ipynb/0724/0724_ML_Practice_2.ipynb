{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import load_model \n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random \n",
    "import PIL.Image as pilimg \n",
    "import imghdr\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import keras \n",
    "import os\n",
    "import shutil # 디렉토리 만들기, \n",
    "\n",
    "base_path = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "\n",
    "print(\"daisy\", len(os.listdir(base_path+\"/daisy\")))\n",
    "print(\"dandelion\", len(os.listdir(base_path+\"/dandelion\")))\n",
    "print(\"sunflower\", len(os.listdir(base_path+\"/sunflower\")))\n",
    "print(\"rose\", len(os.listdir(base_path+\"/rose\")))\n",
    "print(\"tulip\", len(os.listdir(base_path+\"/tulip\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_images_in_class_folder(src_class_dir, class_name, dest_dir):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    files.sort()\n",
    "\n",
    "    for idx, fname in enumerate(files):\n",
    "        src_path = os.path.join(src_class_dir, fname)\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "        new_name = f\"{class_name}.{idx}{ext}\"\n",
    "        dst_path = os.path.join(dest_dir, new_name)\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "    print(f\"✅ {class_name} 리네임 완료: {len(files)}개 처리됨.\")\n",
    "\n",
    "def rename_all_classes(original_root_dir, renamed_root_dir):\n",
    "    classes = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "\n",
    "    if os.path.exists(renamed_root_dir):\n",
    "        shutil.rmtree(renamed_root_dir)\n",
    "    os.makedirs(renamed_root_dir, exist_ok=True)\n",
    "\n",
    "    for class_name in classes:\n",
    "        src_class_dir = os.path.join(original_root_dir, class_name)\n",
    "        rename_images_in_class_folder(src_class_dir, class_name, renamed_root_dir)\n",
    "\n",
    "def copy_images_by_class(class_name, original_dataset_dir, dest_dirs, split_ratio=(0.5, 0.25, 0.25)):\n",
    "    image_files = [\n",
    "        f for f in os.listdir(original_dataset_dir)\n",
    "        if f.startswith(f\"{class_name}.\") and f.lower().endswith(('.jpg', '.jpeg', '.png')) and os.path.isfile(os.path.join(original_dataset_dir, f))\n",
    "    ]\n",
    "    image_files.sort()\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    total = len(image_files)\n",
    "    train_end = int(total * split_ratio[0])\n",
    "    val_end = train_end + int(total * split_ratio[1])\n",
    "\n",
    "    splits = [image_files[:train_end], image_files[train_end:val_end], image_files[val_end:]]\n",
    "\n",
    "    for split, dst_dir in zip(splits, dest_dirs):\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        for fname in split:\n",
    "            src = os.path.join(original_dataset_dir, fname)\n",
    "            dst = os.path.join(dst_dir, fname)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "def ImageCopy(renamed_dataset_dir, base_dir):\n",
    "    categories = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            os.makedirs(os.path.join(base_dir, set_name, category), exist_ok=True)\n",
    "\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    val_dir = os.path.join(base_dir, \"validation\")\n",
    "    test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"🔄 {category} 분할 중...\")\n",
    "        copy_images_by_class(\n",
    "            class_name=category,\n",
    "            original_dataset_dir=renamed_dataset_dir,\n",
    "            dest_dirs=[\n",
    "                os.path.join(train_dir, category),\n",
    "                os.path.join(val_dir, category),\n",
    "                os.path.join(test_dir, category)\n",
    "            ],\n",
    "            split_ratio=(0.5, 0.25, 0.25)\n",
    "        )\n",
    "\n",
    "    print(\"\\n✅ 이미지 분할 복사 완료!\\n\")\n",
    "\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            dir_path = os.path.join(base_dir, set_name, category)\n",
    "            count = len(os.listdir(dir_path))\n",
    "            print(f\"📁 {set_name}/{category}: {count}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ed73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터셋 폴더 (클래스별 하위 폴더 있음)\n",
    "original_dataset_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "\n",
    "# 리네임된 이미지들이 저장될 위치\n",
    "renamed_root = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_renamed\"\n",
    "\n",
    "# 최종 분할된 train/val/test 폴더 생성 위치\n",
    "base_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/0724/flowers_small\"\n",
    "\n",
    "# 1단계: 클래스별 이미지들을 daisy.0.jpg 형식으로 리네임 + 통합\n",
    "rename_all_classes(original_dataset_dir, renamed_root)\n",
    "\n",
    "# 2단계: 리네임된 이미지를 2:1:1 비율로 train/validation/test 분할 복사\n",
    "ImageCopy(renamed_root, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4568df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "from pathlib import Path\n",
    "\n",
    "# batch_size에 지정된 만큼 폴더로부터 이미지를 읽어온다. 크기는 image_size에 지정한 값으로 가져온다.\n",
    "# 훈련셋을 쪼개서 8:2 정도로 검증셋을 따로 만드는 방법도 있고, subset 속성, seed 를 이용해 나눠야 한다.\n",
    "base_dir = Path(base_dir)\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    base_dir/\"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16\n",
    ")\n",
    "validation_ds = image_dataset_from_directory(\n",
    "    base_dir/\"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16\n",
    ")\n",
    "test_ds = image_dataset_from_directory(\n",
    "    base_dir/\"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7040c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 이미지 모델 가져오기\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "def deeplearning():\n",
    "    conv_base = keras.applications.vgg19.VGG19(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False, # CNN만 가져와라, CNN이 하단에 있음, 상단 - 완전연결망(분류)\n",
    "        input_shape=(180, 180, 3) # 입력할 데이터 크기를 주어야 한다.\n",
    "        # 데이터셋에서 지정한 크기와 일치해야 한다, 3-색정보\n",
    "    )\n",
    "\n",
    "    conv_base.summary() # CNN 요약 확인하기\n",
    "    # block5_pool(MaxPooling2D) (None, 5, 5, 512)\n",
    "\n",
    "    conv_base.trainable = True\n",
    "    print(\"합성곱 기반 층을 동결하기 전의 훈련 가능한 가중치 개수\", len(conv_base.trainable_weights))\n",
    "    conv_base.trainable = False # 동결\n",
    "    print(\"합성곱 기반 층을 동결 후의 훈련 가능한 가중치 개수\", len(conv_base.trainable_weights))\n",
    "\n",
    "    data_argumentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.4)\n",
    "    ])\n",
    "\n",
    "    # 모델 만들기\n",
    "    inputs = keras.Input(shape=(180, 180, 3)) # 모델의 입력레이어 정의 \n",
    "    x = data_argumentation(inputs) # 입력이미지에 데이터 증강을 적용한다.\n",
    "    x = keras.applications.vgg19.preprocess_input(x) # vgg19 모델에 맞는 전처리작업(픽셀값범위조정 등)\n",
    "\n",
    "    # 인라인방식으로 cnn 연결하기\n",
    "    x = conv_base(x) # 특성추출이 이뤄진다. 오래 걸린다\n",
    "    #################################################\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Dense(64)(x)\n",
    "    outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    # 시스템이 내부적으로 일 처리하고 일 끝나면 우리가 전달해준 콜백함수를 호출한다.\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"꽃.keras\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\" # 검증데이터 셋을 기준으로 하겠다 가장 적절한 시점에 호출할거다.\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(train_ds,\n",
    "                        epochs=10,\n",
    "                        validation_data=validation_ds,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    with open(\"꽃.bin\", \"wb\") as file:\n",
    "              pickle.dump(history.history, file)\n",
    "\n",
    "deeplearning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
