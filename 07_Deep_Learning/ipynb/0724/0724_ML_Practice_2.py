from keras.preprocessing import image
from keras.models import load_model 
from keras import models, layers
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np 
import random 
import PIL.Image as pilimg 
import imghdr
import pandas as pd 
import pickle 
import keras 
import os
import shutil # ë””ë ‰í† ë¦¬ ë§Œë“¤ê¸°, 

base_path = "../../data/flowers"

print("daisy", len(os.listdir(base_path+"/daisy")))
print("dandelion", len(os.listdir(base_path+"/dandelion")))
print("sunflower", len(os.listdir(base_path+"/sunflower")))
print("rose", len(os.listdir(base_path+"/rose")))
print("tulip", len(os.listdir(base_path+"/tulip")))


def rename_images_in_class_folder(src_class_dir, class_name, dest_dir):
    os.makedirs(dest_dir, exist_ok=True)

    files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    files.sort()

    for idx, fname in enumerate(files):
        src_path = os.path.join(src_class_dir, fname)
        ext = os.path.splitext(fname)[1].lower()
        new_name = f"{class_name}.{idx}{ext}"
        dst_path = os.path.join(dest_dir, new_name)
        shutil.copyfile(src_path, dst_path)

    print(f"âœ… {class_name} ë¦¬ë„¤ì„ ì™„ë£Œ: {len(files)}ê°œ ì²˜ë¦¬ë¨.")

def rename_all_classes(original_root_dir, renamed_root_dir):
    classes = ["daisy", "dandelion", "rose", "sunflower", "tulip"]

    if os.path.exists(renamed_root_dir):
        shutil.rmtree(renamed_root_dir)
    os.makedirs(renamed_root_dir, exist_ok=True)

    for class_name in classes:
        src_class_dir = os.path.join(original_root_dir, class_name)
        rename_images_in_class_folder(src_class_dir, class_name, renamed_root_dir)

def copy_images_by_class(class_name, original_dataset_dir, dest_dirs, split_ratio=(0.5, 0.25, 0.25)):
    image_files = [
        f for f in os.listdir(original_dataset_dir)
        if f.startswith(f"{class_name}.") and f.lower().endswith(('.jpg', '.jpeg', '.png')) and os.path.isfile(os.path.join(original_dataset_dir, f))
    ]
    image_files.sort()
    random.shuffle(image_files)

    total = len(image_files)
    train_end = int(total * split_ratio[0])
    val_end = train_end + int(total * split_ratio[1])

    splits = [image_files[:train_end], image_files[train_end:val_end], image_files[val_end:]]

    for split, dst_dir in zip(splits, dest_dirs):
        os.makedirs(dst_dir, exist_ok=True)
        for fname in split:
            src = os.path.join(original_dataset_dir, fname)
            dst = os.path.join(dst_dir, fname)
            shutil.copyfile(src, dst)

def ImageCopy(renamed_dataset_dir, base_dir):
    categories = ["daisy", "dandelion", "rose", "sunflower", "tulip"]
    sets = ["train", "validation", "test"]

    if os.path.exists(base_dir):
        shutil.rmtree(base_dir)
    for set_name in sets:
        for category in categories:
            os.makedirs(os.path.join(base_dir, set_name, category), exist_ok=True)

    train_dir = os.path.join(base_dir, "train")
    val_dir = os.path.join(base_dir, "validation")
    test_dir = os.path.join(base_dir, "test")

    for category in categories:
        print(f"ğŸ”„ {category} ë¶„í•  ì¤‘...")
        copy_images_by_class(
            class_name=category,
            original_dataset_dir=renamed_dataset_dir,
            dest_dirs=[
                os.path.join(train_dir, category),
                os.path.join(val_dir, category),
                os.path.join(test_dir, category)
            ],
            split_ratio=(0.5, 0.25, 0.25)
        )

    print("\nâœ… ì´ë¯¸ì§€ ë¶„í•  ë³µì‚¬ ì™„ë£Œ!\n")

    for set_name in sets:
        for category in categories:
            dir_path = os.path.join(base_dir, set_name, category)
            count = len(os.listdir(dir_path))
            print(f"ğŸ“ {set_name}/{category}: {count}ê°œ")
            
            
# ì›ë³¸ ë°ì´í„°ì…‹ í´ë” (í´ë˜ìŠ¤ë³„ í•˜ìœ„ í´ë” ìˆìŒ)
original_dataset_dir = "../../data/flowers"

# ë¦¬ë„¤ì„ëœ ì´ë¯¸ì§€ë“¤ì´ ì €ì¥ë  ìœ„ì¹˜
renamed_root = "../../data/flowers_renamed"

# ìµœì¢… ë¶„í• ëœ train/val/test í´ë” ìƒì„± ìœ„ì¹˜
base_dir = "../../data/flowers_small"

# 1ë‹¨ê³„: í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ë“¤ì„ daisy.0.jpg í˜•ì‹ìœ¼ë¡œ ë¦¬ë„¤ì„ + í†µí•©
rename_all_classes(original_dataset_dir, renamed_root)

# 2ë‹¨ê³„: ë¦¬ë„¤ì„ëœ ì´ë¯¸ì§€ë¥¼ 2:1:1 ë¹„ìœ¨ë¡œ train/validation/test ë¶„í•  ë³µì‚¬
ImageCopy(renamed_root, base_dir)


from keras.utils import image_dataset_from_directory
from pathlib import Path

# batch_sizeì— ì§€ì •ëœ ë§Œí¼ í´ë”ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ì½ì–´ì˜¨ë‹¤. í¬ê¸°ëŠ” image_sizeì— ì§€ì •í•œ ê°’ìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤.
# í›ˆë ¨ì…‹ì„ ìª¼ê°œì„œ 8:2 ì •ë„ë¡œ ê²€ì¦ì…‹ì„ ë”°ë¡œ ë§Œë“œëŠ” ë°©ë²•ë„ ìˆê³ , subset ì†ì„±, seed ë¥¼ ì´ìš©í•´ ë‚˜ëˆ ì•¼ í•œë‹¤.
base_dir = Path(base_dir)

train_ds = image_dataset_from_directory(
    base_dir/"train",
    image_size=(180, 180),
    batch_size=16
)
validation_ds = image_dataset_from_directory(
    base_dir/"validation",
    image_size=(180, 180),
    batch_size=16
)
test_ds = image_dataset_from_directory(
    base_dir/"test",
    image_size=(180, 180),
    batch_size=16
)

# VGG19 ì´ë¯¸ì§€ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
from keras.applications.vgg19 import VGG19

def deeplearning():
    conv_base = keras.applications.vgg19.VGG19(
        weights="imagenet",
        include_top=False, # CNNë§Œ ê°€ì ¸ì™€ë¼, CNNì´ í•˜ë‹¨ì— ìˆìŒ, ìƒë‹¨ - ì™„ì „ì—°ê²°ë§(ë¶„ë¥˜)
        input_shape=(180, 180, 3) # ì…ë ¥í•  ë°ì´í„° í¬ê¸°ë¥¼ ì£¼ì–´ì•¼ í•œë‹¤.
        # ë°ì´í„°ì…‹ì—ì„œ ì§€ì •í•œ í¬ê¸°ì™€ ì¼ì¹˜í•´ì•¼ í•œë‹¤, 3-ìƒ‰ì •ë³´
    )

    conv_base.summary() # CNN ìš”ì•½ í™•ì¸í•˜ê¸°
    # block5_pool(MaxPooling2D) (None, 5, 5, 512)

    conv_base.trainable = True
    print("í•©ì„±ê³± ê¸°ë°˜ ì¸µì„ ë™ê²°í•˜ê¸° ì „ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ê°œìˆ˜", len(conv_base.trainable_weights))
    conv_base.trainable = False # ë™ê²°
    print("í•©ì„±ê³± ê¸°ë°˜ ì¸µì„ ë™ê²° í›„ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ê°œìˆ˜", len(conv_base.trainable_weights))

    data_argumentation = keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.2),
        layers.RandomZoom(0.4)
    ])

    # ëª¨ë¸ ë§Œë“¤ê¸°
    inputs = keras.Input(shape=(180, 180, 3)) # ëª¨ë¸ì˜ ì…ë ¥ë ˆì´ì–´ ì •ì˜ 
    x = data_argumentation(inputs) # ì…ë ¥ì´ë¯¸ì§€ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•œë‹¤.
    x = keras.applications.vgg19.preprocess_input(x) # vgg19 ëª¨ë¸ì— ë§ëŠ” ì „ì²˜ë¦¬ì‘ì—…(í”½ì…€ê°’ë²”ìœ„ì¡°ì • ë“±)

    # ì¸ë¼ì¸ë°©ì‹ìœ¼ë¡œ cnn ì—°ê²°í•˜ê¸°
    x = conv_base(x) # íŠ¹ì„±ì¶”ì¶œì´ ì´ë¤„ì§„ë‹¤. ì˜¤ë˜ ê±¸ë¦°ë‹¤
    #################################################
    x = layers.Flatten()(x)
    x = layers.Dense(256)(x)
    x = layers.Dense(128)(x)
    x = layers.Dense(64)(x)
    outputs = layers.Dense(5, activation="softmax")(x)

    model = keras.Model(inputs, outputs)    
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer="adam",
                  metrics=["accuracy"])
    
    # ì‹œìŠ¤í…œì´ ë‚´ë¶€ì ìœ¼ë¡œ ì¼ ì²˜ë¦¬í•˜ê³  ì¼ ëë‚˜ë©´ ìš°ë¦¬ê°€ ì „ë‹¬í•´ì¤€ ì½œë°±í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•œë‹¤.
    callbacks = [
        keras.callbacks.ModelCheckpoint(
            filepath="ê½ƒ.keras",
            save_best_only=True,
            monitor="val_loss" # ê²€ì¦ë°ì´í„° ì…‹ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ê² ë‹¤ ê°€ì¥ ì ì ˆí•œ ì‹œì ì— í˜¸ì¶œí• ê±°ë‹¤.
        )
    ]

    history = model.fit(train_ds,
                        epochs=10,
                        validation_data=validation_ds,
                        callbacks=callbacks)
    
    with open("ê½ƒ.bin", "wb") as file:
              pickle.dump(history.history, file)

deeplearning()


