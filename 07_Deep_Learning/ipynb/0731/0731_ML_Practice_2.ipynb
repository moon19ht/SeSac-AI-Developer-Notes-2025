{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras TextVectorization을 활용한 텍스트 벡터화\n",
    "\n",
    "## 개요\n",
    "이 노트북에서는 TensorFlow/Keras의 TextVectorization 레이어를 사용하여 텍스트 벡터화를 구현합니다.\n",
    "이전에 직접 구현한 벡터화와 달리, Keras의 내장 기능을 활용하여 더 효율적이고 최적화된 텍스트 처리를 수행합니다.\n",
    "\n",
    "## 학습 목표\n",
    "1. Keras TextVectorization 레이어의 사용법 이해\n",
    "2. 커스텀 표준화 및 토큰화 함수 구현\n",
    "3. 빈도 기반 어휘 사전 구축 과정 이해\n",
    "4. 직접 구현한 벡터화와의 차이점 분석\n",
    "\n",
    "## 주요 특징\n",
    "- **효율성**: TensorFlow 기반의 최적화된 연산\n",
    "- **유연성**: 커스텀 전처리 함수 지원\n",
    "- **확장성**: 대용량 데이터 처리 가능\n",
    "- **통합성**: 딥러닝 모델과 직접 연결 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(f\"구두점 목록: {string.punctuation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 커스텀 전처리 함수 구현\n",
    "\n",
    "TextVectorization에서 사용할 커스텀 표준화 및 토큰화 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization_fn(text):\n",
    "    \"\"\"\n",
    "    커스텀 표준화 함수\n",
    "    \n",
    "    Args:\n",
    "        text: TensorFlow 문자열 텐서\n",
    "        \n",
    "    Returns:\n",
    "        표준화된 텍스트 (소문자 + 구두점 제거)\n",
    "    \"\"\"\n",
    "    # 1. 소문자로 변환\n",
    "    lower_text = tf.strings.lower(text)\n",
    "    # 2. 정규식을 사용하여 구두점 제거\n",
    "    return tf.strings.regex_replace(lower_text, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(text):\n",
    "    \"\"\"\n",
    "    커스텀 토큰화 함수\n",
    "    \n",
    "    Args:\n",
    "        text: 표준화된 TensorFlow 문자열 텐서\n",
    "        \n",
    "    Returns:\n",
    "        토큰화된 문자열 배열\n",
    "    \"\"\"\n",
    "    return tf.strings.split(text)\n",
    "\n",
    "# 함수 테스트\n",
    "test_text = tf.constant(\"Hello, World! How are you?\")\n",
    "print(f\"원본 텍스트: {test_text.numpy().decode('utf-8')}\")\n",
    "print(f\"표준화 결과: {custom_standardization_fn(test_text).numpy().decode('utf-8')}\")\n",
    "print(f\"토큰화 결과: {custom_split_fn(custom_standardization_fn(test_text)).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TextVectorization 레이어 생성 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextVectorization 레이어가 생성되었습니다.\n",
      "출력 모드: int (정수 시퀀스)\n",
      "표준화 함수: 커스텀 함수 사용\n",
      "토큰화 함수: 커스텀 함수 사용\n",
      "\n",
      "=== TextVectorization 레이어 정보 ===\n",
      "레이어 이름: text_vectorization_3\n",
      "레이어 타입: TextVectorization\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization 레이어 생성\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",  # 출력 모드: 정수 시퀀스\n",
    "    standardize=custom_standardization_fn,  # 커스텀 표준화 함수\n",
    "    split=custom_split_fn  # 커스텀 토큰화 함수\n",
    ")\n",
    "\n",
    "print(\"TextVectorization 레이어가 생성되었습니다.\")\n",
    "print(f\"출력 모드: int (정수 시퀀스)\")\n",
    "print(f\"표준화 함수: 커스텀 함수 사용\")\n",
    "print(f\"토큰화 함수: 커스텀 함수 사용\")\n",
    "\n",
    "# TextVectorization 객체의 주요 속성 확인\n",
    "print(f\"\\n=== TextVectorization 레이어 정보 ===\")\n",
    "print(f\"레이어 이름: {text_vectorization.name}\")\n",
    "print(f\"레이어 타입: {type(text_vectorization).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 준비 및 어휘 사전 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 학습 데이터셋 ===\n",
      "1. I write, erase, rewrite\n",
      "2. Erase again, and then\n",
      "3. A poppy blooms\n",
      "4. Dog is pretty\n",
      "\n",
      "어휘 사전 구축이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋 (자연어1.py와 동일한 데이터 + 오타 수정)\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",  # reqrite -> rewrite 수정\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms\",\n",
    "    \"Dog is pretty\"\n",
    "]\n",
    "\n",
    "print(\"=== 학습 데이터셋 ===\")\n",
    "for i, text in enumerate(dataset, 1):\n",
    "    print(f\"{i}. {text}\")\n",
    "\n",
    "# 어휘 사전 구축 (adapt 과정)\n",
    "text_vectorization.adapt(dataset)\n",
    "print(\"\\n어휘 사전 구축이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 어휘 사전 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 구축된 어휘 사전 ===\n",
      "총 어휘 개수: 15\n",
      "\n",
      "어휘 목록 (빈도순 정렬):\n",
      " 0. '[PADDING]' (패딩용)\n",
      " 1. '[UNK]' (미등록 단어)\n",
      " 2. 'erase'\n",
      " 3. 'write'\n",
      " 4. 'then'\n",
      " 5. 'rewrite'\n",
      " 6. 'pretty'\n",
      " 7. 'poppy'\n",
      " 8. 'is'\n",
      " 9. 'i'\n",
      "10. 'dog'\n",
      "11. 'blooms'\n",
      "12. 'and'\n",
      "13. 'again'\n",
      "14. 'a'\n",
      "\n",
      "=== 특수 토큰 ===\n",
      "- 인덱스 0: 패딩용 빈 문자열\n",
      "- 인덱스 1: 미등록 단어 [UNK]\n",
      "- 인덱스 2~: 실제 단어들 (빈도 높은 순)\n"
     ]
    }
   ],
   "source": [
    "# 어휘 사전 추출\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "\n",
    "print(\"=== 구축된 어휘 사전 ===\")\n",
    "print(f\"총 어휘 개수: {len(vocabulary)}\")\n",
    "print(\"\\n어휘 목록 (빈도순 정렬):\")\n",
    "for i, word in enumerate(vocabulary):\n",
    "    if word == \"\":\n",
    "        print(f\"{i:2d}. '[PADDING]' (패딩용)\")\n",
    "    elif word == \"[UNK]\":\n",
    "        print(f\"{i:2d}. '[UNK]' (미등록 단어)\")\n",
    "    else:\n",
    "        print(f\"{i:2d}. '{word}'\")\n",
    "\n",
    "print(\"\\n=== 특수 토큰 ===\")\n",
    "print(\"- 인덱스 0: 패딩용 빈 문자열\")\n",
    "print(\"- 인덱스 1: 미등록 단어 [UNK]\")\n",
    "print(\"- 인덱스 2~: 실제 단어들 (빈도 높은 순)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 텍스트 인코딩 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 텍스트: 'I wruite, rewrite, and still rewrite agian'\n",
      "인코딩 결과: [ 9  1  5 12  1  5  1]\n",
      "\n",
      "=== 인코딩 과정 상세 분석 ===\n",
      "표준화된 텍스트: 'i wruite rewrite and still rewrite agian'\n",
      "토큰들: ['i', 'wruite', 'rewrite', 'and', 'still', 'rewrite', 'agian']\n",
      "\n",
      "토큰별 인덱스 매핑:\n",
      "'i' -> 9 (등록된 단어)\n",
      "'wruite' -> 1 (미등록 단어, [UNK]로 처리)\n",
      "'rewrite' -> 5 (등록된 단어)\n",
      "'and' -> 12 (등록된 단어)\n",
      "'still' -> 1 (미등록 단어, [UNK]로 처리)\n",
      "'rewrite' -> 5 (등록된 단어)\n",
      "'agian' -> 1 (미등록 단어, [UNK]로 처리)\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 테스트 (원본 코드의 오타 포함)\n",
    "test_text = \"I wruite, rewrite, and still rewrite agian\"  # 오타: wruite, agian\n",
    "print(f\"테스트 텍스트: '{test_text}'\")\n",
    "\n",
    "# 인코딩 수행\n",
    "encoded = text_vectorization(test_text)\n",
    "print(f\"인코딩 결과: {encoded.numpy()}\")\n",
    "\n",
    "# 인코딩 과정 상세 분석\n",
    "print(\"\\n=== 인코딩 과정 상세 분석 ===\")\n",
    "standardized_text = custom_standardization_fn(tf.constant(test_text))\n",
    "tokens = custom_split_fn(standardized_text)\n",
    "print(f\"표준화된 텍스트: '{standardized_text.numpy().decode('utf-8')}'\")\n",
    "print(f\"토큰들: {[token.numpy().decode('utf-8') for token in tokens]}\")\n",
    "\n",
    "print(\"\\n토큰별 인덱스 매핑:\")\n",
    "for i, token_bytes in enumerate(tokens.numpy()):\n",
    "    token = token_bytes.decode('utf-8')\n",
    "    if token in vocabulary:\n",
    "        idx = vocabulary.index(token)\n",
    "        print(f\"'{token}' -> {idx} (등록된 단어)\")\n",
    "    else:\n",
    "        print(f\"'{token}' -> 1 (미등록 단어, [UNK]로 처리)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 텍스트 디코딩 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 인덱스-단어 매핑 사전 ===\n",
      "0 -> '[PADDING]'\n",
      "1 -> '[UNK]'\n",
      "2 -> 'erase'\n",
      "3 -> 'write'\n",
      "4 -> 'then'\n",
      "5 -> 'rewrite'\n",
      "6 -> 'pretty'\n",
      "7 -> 'poppy'\n",
      "8 -> 'is'\n",
      "9 -> 'i'\n",
      "10 -> 'dog'\n",
      "11 -> 'blooms'\n",
      "12 -> 'and'\n",
      "13 -> 'again'\n",
      "14 -> 'a'\n",
      "\n",
      "=== 디코딩 결과 ===\n",
      "인코딩된 시퀀스: [ 9  1  5 12  1  5  1]\n",
      "디코딩된 문장: 'i [UNK] rewrite and [UNK] rewrite [UNK]'\n",
      "\n",
      "=== 원본 vs 디코딩 비교 ===\n",
      "원본 텍스트:     'I wruite, rewrite, and still rewrite agian'\n",
      "표준화 텍스트:   'i wruite rewrite and still rewrite agian'\n",
      "디코딩 결과:     'i [UNK] rewrite and [UNK] rewrite [UNK]'\n"
     ]
    }
   ],
   "source": [
    "# 디코딩을 위한 인덱스-단어 매핑 딕셔너리 생성\n",
    "decoded_vocab = dict(enumerate(vocabulary))\n",
    "\n",
    "print(\"=== 인덱스-단어 매핑 사전 ===\")\n",
    "for idx, word in decoded_vocab.items():\n",
    "    if word == \"\":\n",
    "        print(f\"{idx} -> '[PADDING]'\")\n",
    "    else:\n",
    "        print(f\"{idx} -> '{word}'\")\n",
    "\n",
    "# 디코딩 수행\n",
    "encoded_sequence = encoded.numpy()\n",
    "decoded_sentence = \" \".join(decoded_vocab[int(i)] for i in encoded_sequence)\n",
    "\n",
    "print(f\"\\n=== 디코딩 결과 ===\")\n",
    "print(f\"인코딩된 시퀀스: {encoded_sequence}\")\n",
    "print(f\"디코딩된 문장: '{decoded_sentence}'\")\n",
    "\n",
    "# 원본과 디코딩 결과 비교\n",
    "print(f\"\\n=== 원본 vs 디코딩 비교 ===\")\n",
    "print(f\"원본 텍스트:     '{test_text}'\")\n",
    "print(f\"표준화 텍스트:   '{standardized_text.numpy().decode('utf-8')}'\")\n",
    "print(f\"디코딩 결과:     '{decoded_sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 전체 데이터셋 처리 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 전체 데이터셋 인코딩/디코딩 결과 ===\n",
      "\n",
      "1. 원본: 'I write, erase, rewrite'\n",
      "   표준화: 'i write erase rewrite'\n",
      "   인코딩: [9 3 2 5]\n",
      "   디코딩: 'i write erase rewrite'\n",
      "\n",
      "2. 원본: 'Erase again, and then'\n",
      "   표준화: 'erase again and then'\n",
      "   인코딩: [ 2 13 12  4]\n",
      "   디코딩: 'erase again and then'\n",
      "\n",
      "3. 원본: 'A poppy blooms'\n",
      "   표준화: 'a poppy blooms'\n",
      "   인코딩: [14  7 11]\n",
      "   디코딩: 'a poppy blooms'\n",
      "\n",
      "4. 원본: 'Dog is pretty'\n",
      "   표준화: 'dog is pretty'\n",
      "   인코딩: [10  8  6]\n",
      "   디코딩: 'dog is pretty'\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 전체 데이터셋 인코딩/디코딩 결과 ===\")\n",
    "\n",
    "for i, text in enumerate(dataset, 1):\n",
    "    # 인코딩\n",
    "    encoded = text_vectorization(text)\n",
    "    encoded_list = encoded.numpy()\n",
    "    \n",
    "    # 디코딩\n",
    "    decoded = \" \".join(decoded_vocab[int(idx)] for idx in encoded_list)\n",
    "    \n",
    "    # 표준화된 텍스트\n",
    "    standardized = custom_standardization_fn(tf.constant(text)).numpy().decode('utf-8')\n",
    "    \n",
    "    print(f\"\\n{i}. 원본: '{text}'\")\n",
    "    print(f\"   표준화: '{standardized}'\")\n",
    "    print(f\"   인코딩: {encoded_list}\")\n",
    "    print(f\"   디코딩: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 다양한 출력 모드 비교\n",
    "\n",
    "TextVectorization의 다른 출력 모드들을 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 문장: 'I write erase rewrite'\n",
      "어휘 사전: ['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'pretty', 'poppy', 'is', 'i', 'dog', 'blooms', 'and', 'again', 'a']\n",
      "\n",
      "=== 다양한 출력 모드 비교 ===\n",
      "Int 모드:    [9 3 2 5]\n",
      "Binary 모드: [0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Count 모드:  [0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "TF-IDF 모드: [0.         0.84729785 1.0986123  0.         1.0986123  0.\n",
      " 0.         0.         1.0986123  0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 다른 출력 모드들 테스트\n",
    "test_sentence = \"I write erase rewrite\"\n",
    "\n",
    "# 1. Binary 모드 (단어 존재 여부)\n",
    "vectorizer_binary = TextVectorization(\n",
    "    output_mode=\"binary\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn\n",
    ")\n",
    "vectorizer_binary.adapt(dataset)\n",
    "\n",
    "# 2. Count 모드 (단어 빈도)\n",
    "vectorizer_count = TextVectorization(\n",
    "    output_mode=\"count\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn\n",
    ")\n",
    "vectorizer_count.adapt(dataset)\n",
    "\n",
    "# 3. TF-IDF 모드\n",
    "vectorizer_tfidf = TextVectorization(\n",
    "    output_mode=\"tf_idf\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn\n",
    ")\n",
    "vectorizer_tfidf.adapt(dataset)\n",
    "\n",
    "print(f\"테스트 문장: '{test_sentence}'\")\n",
    "print(f\"어휘 사전: {vocabulary}\")\n",
    "print(\"\\n=== 다양한 출력 모드 비교 ===\")\n",
    "print(f\"Int 모드:    {text_vectorization(test_sentence).numpy()}\")\n",
    "print(f\"Binary 모드: {vectorizer_binary(test_sentence).numpy()}\")\n",
    "print(f\"Count 모드:  {vectorizer_count(test_sentence).numpy()}\")\n",
    "print(f\"TF-IDF 모드: {vectorizer_tfidf(test_sentence).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 미등록 단어 처리 및 어휘 크기 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x0000015112B0F6A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "=== 어휘 크기 제한 테스트 ===\n",
      "제한된 어휘 사전 (최대 5개): ['', '[UNK]', 'erase', 'write', 'then']\n",
      "어휘 개수: 5\n",
      "\n",
      "테스트 문장: 'I love programming and coding'\n",
      "제한된 어휘로 인코딩: [1 1 1 1 1]\n",
      "디코딩 결과: '[UNK] [UNK] [UNK] [UNK] [UNK]'\n"
     ]
    }
   ],
   "source": [
    "# 어휘 크기를 제한한 벡터화기\n",
    "limited_vectorizer = TextVectorization(\n",
    "    max_tokens=5,  # 최대 5개 토큰만 유지 (패딩, UNK 포함)\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn\n",
    ")\n",
    "\n",
    "limited_vectorizer.adapt(dataset)\n",
    "limited_vocab = limited_vectorizer.get_vocabulary()\n",
    "\n",
    "print(\"=== 어휘 크기 제한 테스트 ===\")\n",
    "print(f\"제한된 어휘 사전 (최대 5개): {limited_vocab}\")\n",
    "print(f\"어휘 개수: {len(limited_vocab)}\")\n",
    "\n",
    "# 제한된 어휘로 인코딩 테스트\n",
    "test_with_unknown = \"I love programming and coding\"\n",
    "print(f\"\\n테스트 문장: '{test_with_unknown}'\")\n",
    "encoded_limited = limited_vectorizer(test_with_unknown)\n",
    "print(f\"제한된 어휘로 인코딩: {encoded_limited.numpy()}\")\n",
    "\n",
    "# 디코딩\n",
    "limited_decoded_vocab = dict(enumerate(limited_vocab))\n",
    "decoded_limited = \" \".join(limited_decoded_vocab[int(i)] for i in encoded_limited.numpy())\n",
    "print(f\"디코딩 결과: '{decoded_limited}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 성능 및 효율성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 성능 테스트 ===\n",
      "테스트 데이터 크기: 1000 문장\n",
      "\n",
      "Keras TextVectorization 처리 시간: 5.2769초\n",
      "초당 처리량: 189.5 문장/초\n",
      "\n",
      "배치 처리 시간 (100개): 0.0115초\n",
      "배치 처리 결과 shape: (100, 5)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 대용량 데이터 시뮬레이션\n",
    "large_dataset = dataset * 1000  # 4,000개 문장\n",
    "test_texts = [\"I write code every day\"] * 1000\n",
    "\n",
    "print(\"=== 성능 테스트 ===\")\n",
    "print(f\"테스트 데이터 크기: {len(test_texts)} 문장\")\n",
    "\n",
    "# TextVectorization 성능 측정\n",
    "start_time = time.time()\n",
    "for text in test_texts:\n",
    "    _ = text_vectorization(text)\n",
    "keras_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nKeras TextVectorization 처리 시간: {keras_time:.4f}초\")\n",
    "print(f\"초당 처리량: {len(test_texts)/keras_time:.1f} 문장/초\")\n",
    "\n",
    "# 배치 처리 성능\n",
    "start_time = time.time()\n",
    "batch_result = text_vectorization(test_texts[:100])  # 배치로 100개 처리\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n배치 처리 시간 (100개): {batch_time:.4f}초\")\n",
    "print(f\"배치 처리 결과 shape: {batch_result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 직접 구현 vs Keras TextVectorization 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 직접 구현 vs Keras TextVectorization 비교 ===\n",
      "특징              | 직접 구현 (MyVectorize)       | Keras TextVectorization\n",
      "구현 복잡도          | 높음 (모든 기능 직접 구현)          | 낮음 (내장 기능 활용)\n",
      "성능              | 파이썬 기반, 상대적으로 느림          | TensorFlow 최적화, 빠름\n",
      "메모리 효율성         | 기본적인 수준                   | 매우 효율적\n",
      "배치 처리           | 별도 구현 필요                  | 내장 지원\n",
      "GPU 가속          | 지원하지 않음                   | 완전 지원\n",
      "어휘 정렬           | 삽입 순서                     | 빈도 기반 자동 정렬\n",
      "출력 모드           | 정수 시퀀스만                   | int, binary, count, tf-idf\n",
      "모델 통합           | 별도 연결 작업 필요               | Keras 레이어로 직접 연결\n",
      "커스텀 함수          | 클래스 메서드로 구현               | TensorFlow 함수로 구현\n",
      "\n",
      "=== 사용 권장 사항 ===\n",
      "✅ Keras TextVectorization 권장 상황:\n",
      "  - 딥러닝 모델과 통합 사용\n",
      "  - 대용량 데이터 처리\n",
      "  - 성능이 중요한 운영 환경\n",
      "  - 다양한 벡터화 방식 필요\n",
      "\n",
      "✅ 직접 구현 권장 상황:\n",
      "  - 학습 및 이해 목적\n",
      "  - 매우 특수한 전처리 로직\n",
      "  - 외부 의존성 최소화\n",
      "  - 완전한 제어가 필요한 경우\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 직접 구현 vs Keras TextVectorization 비교 ===\")\n",
    "\n",
    "comparison_data = [\n",
    "    [\"특징\", \"직접 구현 (MyVectorize)\", \"Keras TextVectorization\"],\n",
    "    [\"구현 복잡도\", \"높음 (모든 기능 직접 구현)\", \"낮음 (내장 기능 활용)\"],\n",
    "    [\"성능\", \"파이썬 기반, 상대적으로 느림\", \"TensorFlow 최적화, 빠름\"],\n",
    "    [\"메모리 효율성\", \"기본적인 수준\", \"매우 효율적\"],\n",
    "    [\"배치 처리\", \"별도 구현 필요\", \"내장 지원\"],\n",
    "    [\"GPU 가속\", \"지원하지 않음\", \"완전 지원\"],\n",
    "    [\"어휘 정렬\", \"삽입 순서\", \"빈도 기반 자동 정렬\"],\n",
    "    [\"출력 모드\", \"정수 시퀀스만\", \"int, binary, count, tf-idf\"],\n",
    "    [\"모델 통합\", \"별도 연결 작업 필요\", \"Keras 레이어로 직접 연결\"],\n",
    "    [\"커스텀 함수\", \"클래스 메서드로 구현\", \"TensorFlow 함수로 구현\"]\n",
    "]\n",
    "\n",
    "for row in comparison_data:\n",
    "    print(f\"{row[0]:<15} | {row[1]:<25} | {row[2]}\")\n",
    "\n",
    "print(\"\\n=== 사용 권장 사항 ===\")\n",
    "print(\"✅ Keras TextVectorization 권장 상황:\")\n",
    "print(\"  - 딥러닝 모델과 통합 사용\")\n",
    "print(\"  - 대용량 데이터 처리\")\n",
    "print(\"  - 성능이 중요한 운영 환경\")\n",
    "print(\"  - 다양한 벡터화 방식 필요\")\n",
    "\n",
    "print(\"\\n✅ 직접 구현 권장 상황:\")\n",
    "print(\"  - 학습 및 이해 목적\")\n",
    "print(\"  - 매우 특수한 전처리 로직\")\n",
    "print(\"  - 외부 의존성 최소화\")\n",
    "print(\"  - 완전한 제어가 필요한 경우\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 실제 딥러닝 모델과의 통합 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 텍스트 분류 모델 구조 ===\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, None)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 16)          240       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                6272      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6545 (25.57 KB)\n",
      "Trainable params: 6545 (25.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "원시 텍스트 입력: ['I love this movie', 'This is terrible']\n",
      "벡터화 출력: [[9 1 1 1]\n",
      " [1 8 1 0]]\n",
      "\n",
      "✅ 장점: 원시 텍스트를 모델에 직접 입력 가능\n",
      "✅ 장점: 전처리와 모델이 하나의 파이프라인으로 통합\n",
      "✅ 장점: 모델 저장/로드 시 전처리 로직도 함께 저장\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 간단한 텍스트 분류 모델 구성 예제\n",
    "def create_text_classification_model():\n",
    "    \"\"\"\n",
    "    TextVectorization이 포함된 텍스트 분류 모델 생성\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        text_vectorization,  # 전처리 레이어로 직접 사용\n",
    "        Embedding(input_dim=len(vocabulary), output_dim=16),\n",
    "        LSTM(32),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 모델 생성 및 구조 확인\n",
    "model = create_text_classification_model()\n",
    "print(\"=== 텍스트 분류 모델 구조 ===\")\n",
    "model.summary()\n",
    "\n",
    "# 모델에 직접 원시 텍스트 입력 가능\n",
    "sample_input = [\"I love this movie\", \"This is terrible\"]\n",
    "print(f\"\\n원시 텍스트 입력: {sample_input}\")\n",
    "\n",
    "# 모델의 첫 번째 레이어(TextVectorization) 출력 확인\n",
    "vectorized_output = text_vectorization(sample_input)\n",
    "print(f\"벡터화 출력: {vectorized_output.numpy()}\")\n",
    "\n",
    "print(\"\\n✅ 장점: 원시 텍스트를 모델에 직접 입력 가능\")\n",
    "print(\"✅ 장점: 전처리와 모델이 하나의 파이프라인으로 통합\")\n",
    "print(\"✅ 장점: 모델 저장/로드 시 전처리 로직도 함께 저장\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 결론 및 정리\n",
    "\n",
    "### Keras TextVectorization의 주요 특징\n",
    "\n",
    "1. **효율성과 성능**\n",
    "   - TensorFlow 백엔드 기반의 최적화된 연산\n",
    "   - GPU 가속 지원으로 대용량 데이터 처리 가능\n",
    "   - 배치 처리 내장 지원\n",
    "\n",
    "2. **유연성과 확장성**\n",
    "   - 커스텀 전처리 함수 지원\n",
    "   - 다양한 출력 모드 (int, binary, count, tf-idf)\n",
    "   - 어휘 크기 제한 및 빈도 기반 필터링\n",
    "\n",
    "3. **통합성**\n",
    "   - Keras 레이어로서 모델에 직접 통합\n",
    "   - 원시 텍스트 입력 지원\n",
    "   - 모델 저장/로드 시 전처리 로직 보존\n",
    "\n",
    "4. **자동화된 최적화**\n",
    "   - 빈도 기반 어휘 정렬\n",
    "   - 메모리 효율적인 구현\n",
    "   - 프로덕션 환경 최적화\n",
    "\n",
    "### 실제 사용 시 고려사항\n",
    "\n",
    "- **어휘 크기 관리**: `max_tokens` 파라미터로 메모리 사용량 제어\n",
    "- **시퀀스 길이**: `output_sequence_length`로 고정 길이 출력 가능\n",
    "- **특수 토큰**: 패딩과 UNK 토큰의 적절한 활용\n",
    "- **전처리 일관성**: 학습과 추론 시 동일한 전처리 보장\n",
    "\n",
    "### 다음 단계 학습 방향\n",
    "\n",
    "1. **서브워드 토큰화**: BPE, SentencePiece 등 고급 토큰화 기법\n",
    "2. **다국어 처리**: 언어별 특수 전처리 요구사항\n",
    "3. **대화형 모델**: 시퀀스-투-시퀀스 모델에서의 활용\n",
    "4. **트랜스포머**: BERT, GPT 등 최신 모델과의 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 추가 실습 과제\n",
    "\n",
    "다음 과제들을 통해 TextVectorization 활용 능력을 향상시켜보세요:\n",
    "\n",
    "### 기초 과제\n",
    "1. 한국어 텍스트로 동일한 과정 수행하기\n",
    "2. 다양한 `max_tokens` 값에 따른 성능 변화 분석\n",
    "3. 각 출력 모드별 특성과 사용 사례 정리\n",
    "\n",
    "### 중급 과제\n",
    "4. 영화 리뷰 데이터로 감성 분석 모델 구축\n",
    "5. 시퀀스 길이 고정 기능 활용한 배치 처리 최적화\n",
    "6. 커스텀 전처리 함수로 특수한 텍스트 정제 로직 구현\n",
    "\n",
    "### 고급 과제\n",
    "7. 대용량 텍스트 데이터셋으로 성능 벤치마킹\n",
    "8. 다중 언어 텍스트 처리 파이프라인 구축\n",
    "9. 실시간 텍스트 분류 서비스 구현"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
