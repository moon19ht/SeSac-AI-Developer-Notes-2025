{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# IMDB ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„ - ì™„ì „ ê°€ì´ë“œ\n",
        "\n",
        "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
        "1. **ìì—°ì–´ ì²˜ë¦¬(NLP) ê¸°ì´ˆ ì´í•´**: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ì— í™œìš©í•˜ëŠ” ë°©ë²• í•™ìŠµ\n",
        "2. **ì‹¤ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**: ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ ë°°í¬ê¹Œì§€\n",
        "3. **ê°ì„± ë¶„ì„ ëª¨ë¸ ê°œë°œ**: ì´ì§„ ë¶„ë¥˜ë¥¼ í†µí•œ ê°ì • ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
        "4. **ì„±ëŠ¥ ìµœì í™” ê¸°ë²•**: ë°ì´í„° ìºì‹±, ë²¡í„°í™”, ëª¨ë¸ íŠœë‹ ê¸°ë²• ì ìš©\n",
        "\n",
        "## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”\n",
        "- **ë°ì´í„°ì…‹**: Stanford AI Labì˜ IMDB ì˜í™” ë¦¬ë·° (50,000ê°œ ë¦¬ë·°)\n",
        "- **ë¬¸ì œ ìœ í˜•**: ì´ì§„ ë¶„ë¥˜ (ê¸ì •/ë¶€ì • ê°ì„± ë¶„ì„)\n",
        "- **ê¸°ìˆ  ìŠ¤íƒ**: TensorFlow/Keras, TextVectorization, Dense Neural Network\n",
        "- **ë²¡í„°í™” ë°©ì‹**: Multi-hot encoding (Bag of Words)\n",
        "\n",
        "## ğŸ“‹ ë…¸íŠ¸ë¶ êµ¬ì„±\n",
        "```\n",
        "Part I   : ì´ë¡  ë° ë°°ê²½ ì§€ì‹\n",
        "Part II  : í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„  \n",
        "Part III : í…ìŠ¤íŠ¸ ë²¡í„°í™” ë° ì „ì²˜ë¦¬\n",
        "Part IV  : ëª¨ë¸ êµ¬ì¶• ë° í›ˆë ¨\n",
        "Part V   : ì„±ëŠ¥ í‰ê°€ ë° ê°œì„  ë°©ì•ˆ\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part I: ì´ë¡  ë° ë°°ê²½ ì§€ì‹\n",
        "\n",
        "## 1.1 ê°ì„± ë¶„ì„ì´ë€?\n",
        "\n",
        "**ê°ì„± ë¶„ì„(Sentiment Analysis)**ì€ í…ìŠ¤íŠ¸ì— ë‹´ê¸´ ê°ì •, ì˜ê²¬, íƒœë„ë¥¼ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ê¸°ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ” ì£¼ìš” ì‘ìš© ë¶„ì•¼\n",
        "- **ì†Œì…œ ë¯¸ë””ì–´ ëª¨ë‹ˆí„°ë§**: ë¸Œëœë“œì— ëŒ€í•œ ëŒ€ì¤‘ì˜ ë°˜ì‘ ë¶„ì„\n",
        "- **ì œí’ˆ ë¦¬ë·° ë¶„ì„**: ê³ ê° ë§Œì¡±ë„ ë° í”¼ë“œë°± ìë™ ë¶„ë¥˜\n",
        "- **ì£¼ì‹ ì‹œì¥ ì˜ˆì¸¡**: ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„±ì„ í†µí•œ ì‹œì¥ ë™í–¥ ì˜ˆì¸¡\n",
        "- **ê³ ê° ì„œë¹„ìŠ¤**: ë¬¸ì˜ ë‚´ìš©ì˜ ê¸´ê¸‰ë„ ë° ê°ì • ìƒíƒœ íŒŒì•…\n",
        "\n",
        "### ğŸ“Š ë¶„ë¥˜ ìœ í˜•\n",
        "1. **ì´ì§„ ë¶„ë¥˜**: ê¸ì •/ë¶€ì • (ë³¸ í”„ë¡œì íŠ¸)\n",
        "2. **ë‹¤ì¤‘ ë¶„ë¥˜**: ë§¤ìš° ê¸ì •/ê¸ì •/ì¤‘ë¦½/ë¶€ì •/ë§¤ìš° ë¶€ì •\n",
        "3. **ì„¸ë°€í•œ ê°ì • ë¶„ë¥˜**: ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€ ë“±\n",
        "\n",
        "## 1.2 IMDB ë°ì´í„°ì…‹ ì†Œê°œ\n",
        "\n",
        "### ğŸ“ˆ ë°ì´í„°ì…‹ íŠ¹ì§•\n",
        "- **ì¶œì²˜**: Stanford AI Lab (Andrew Maas et al., 2011)\n",
        "- **ê·œëª¨**: ì´ 50,000ê°œ ì˜í™” ë¦¬ë·°\n",
        "- **êµ¬ì„±**: \n",
        "  - í›ˆë ¨ìš©: 25,000ê°œ (ê¸ì • 12,500 + ë¶€ì • 12,500)\n",
        "  - í…ŒìŠ¤íŠ¸ìš©: 25,000ê°œ (ê¸ì • 12,500 + ë¶€ì • 12,500)\n",
        "- **ë ˆì´ë¸”**: ì´ì§„ ë¶„ë¥˜ (0: ë¶€ì •, 1: ê¸ì •)\n",
        "- **í¬ê¸°**: ì••ì¶• íŒŒì¼ ~80MB, ì••ì¶• í•´ì œ í›„ ~280MB\n",
        "\n",
        "### ğŸ­ ë°ì´í„° í’ˆì§ˆ\n",
        "- **ê· í˜• ì¡íŒ ë°ì´í„°**: ê¸ì •/ë¶€ì • ë¹„ìœ¨ì´ 50:50ìœ¼ë¡œ ê· ë“±\n",
        "- **ì‹¤ì œ ì‚¬ìš©ì ë¦¬ë·°**: IMDb ì›¹ì‚¬ì´íŠ¸ì˜ ì‹¤ì œ ì‚¬ìš©ì ì‘ì„± ë¦¬ë·°\n",
        "- **í’ˆì§ˆ í•„í„°ë§**: ê·¹ë‹¨ì  ì ìˆ˜(1-4ì : ë¶€ì •, 7-10ì : ê¸ì •)ë§Œ í¬í•¨\n",
        "\n",
        "## 1.3 í…ìŠ¤íŠ¸ ë²¡í„°í™” ì´ë¡ \n",
        "\n",
        "### ğŸ§® ì™œ ë²¡í„°í™”ê°€ í•„ìš”í•œê°€?\n",
        "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ **ìˆ«ì**ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ¯ ì£¼ìš” ë²¡í„°í™” ê¸°ë²•\n",
        "\n",
        "#### 1) Bag of Words (BoW) - ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©\n",
        "```\n",
        "\"I love this movie\" â†’ [0, 1, 1, 0, 1, 0, ...]\n",
        "```\n",
        "- **ì¥ì **: êµ¬í˜„ ê°„ë‹¨, í•´ì„ ìš©ì´\n",
        "- **ë‹¨ì **: ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ, í¬ì†Œ ë²¡í„°\n",
        "\n",
        "#### 2) TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "```\n",
        "TF-IDF = TF(ë‹¨ì–´ ë¹ˆë„) Ã— IDF(ì—­ë¬¸ì„œ ë¹ˆë„)\n",
        "```\n",
        "- **ì¥ì **: ì¤‘ìš”í•œ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
        "- **ë‹¨ì **: ì—¬ì „íˆ ìˆœì„œ ì •ë³´ ì—†ìŒ\n",
        "\n",
        "#### 3) Word Embeddings (Word2Vec, GloVe)\n",
        "```\n",
        "\"king\" - \"man\" + \"woman\" â‰ˆ \"queen\"\n",
        "```\n",
        "- **ì¥ì **: ì˜ë¯¸ë¡ ì  ê´€ê³„ í‘œí˜„\n",
        "- **ë‹¨ì **: ì‚¬ì „ í›ˆë ¨ í•„ìš”\n",
        "\n",
        "#### 4) Transformer ê¸°ë°˜ (BERT, GPT)\n",
        "- **ì¥ì **: ë¬¸ë§¥ ê³ ë ¤, ìµœê³  ì„±ëŠ¥\n",
        "- **ë‹¨ì **: ê³„ì‚° ë¹„ìš© ë†’ìŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ í™˜ê²½ ì„¤ì •\n",
            "==================================================\n",
            "TensorFlow ë²„ì „: 2.15.1\n",
            "Keras ë²„ì „: 2.15.0\n",
            "Python ì‹¤í–‰ ì‹œê°„: 2025-07-31 15:59:09\n",
            "âš ï¸ CPUë§Œ ì‚¬ìš© (GPU ì—†ìŒ)\n",
            "ğŸ² ëœë¤ ì‹œë“œ ì„¤ì •: 1337\n",
            "\n",
            "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# Part II: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„\n",
        "\n",
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import requests          # HTTP ìš”ì²­ (ë°ì´í„° ë‹¤ìš´ë¡œë“œ)\n",
        "import subprocess        # ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ì‹¤í–‰ (ì••ì¶• í•´ì œ)\n",
        "import re, string        # ì •ê·œí‘œí˜„ì‹, ë¬¸ìì—´ ì²˜ë¦¬\n",
        "import os, pathlib       # íŒŒì¼/ë””ë ‰í† ë¦¬ ê´€ë¦¬\n",
        "import shutil, random    # íŒŒì¼ ì´ë™, ëœë¤ ì²˜ë¦¬\n",
        "\n",
        "# TensorFlow & Keras (ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import keras\n",
        "from keras import layers, models\n",
        "\n",
        "# ì¶”ê°€ ìœ í‹¸ë¦¬í‹°\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# TensorFlow ë²„ì „ í™•ì¸ ë° ì„¤ì •\n",
        "print(\"ğŸ”§ í™˜ê²½ ì„¤ì •\")\n",
        "print(\"=\"*50)\n",
        "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
        "print(f\"Keras ë²„ì „: {keras.__version__}\")\n",
        "print(f\"Python ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥\")\n",
        "    print(f\"GPU ì¥ì¹˜: {tf.config.list_physical_devices('GPU')}\")\n",
        "else:\n",
        "    print(\"âš ï¸ CPUë§Œ ì‚¬ìš© (GPU ì—†ìŒ)\")\n",
        "\n",
        "# ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
        "RANDOM_SEED = 1337\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "print(f\"ğŸ² ëœë¤ ì‹œë“œ ì„¤ì •: {RANDOM_SEED}\")\n",
        "\n",
        "print(\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2.1 ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œìŠ¤í…œ\n",
        "\n",
        "### ğŸ¯ ë‹¤ìš´ë¡œë“œ ì „ëµ\n",
        "- **ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬\n",
        "- **ì§„í–‰ë¥  í‘œì‹œ**: ì‚¬ìš©ì ê²½í—˜ ê°œì„ \n",
        "- **ì˜¤ë¥˜ ì²˜ë¦¬**: ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ëŒ€ì‘\n",
        "- **ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€**: ê¸°ì¡´ íŒŒì¼ ì¡´ì¬ ì‹œ ê±´ë„ˆë›°ê¸°\n",
        "\n",
        "### ğŸ“Š íŒŒì¼ ì •ë³´\n",
        "- **URL**: `https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz`\n",
        "- **í¬ê¸°**: ì•½ 80MB (ì••ì¶•), 280MB (ì••ì¶• í•´ì œ)\n",
        "- **í˜•ì‹**: tar.gz (Linux í‘œì¤€ ì••ì¶• í˜•ì‹)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_imdb_dataset():\n",
        "    \"\"\"\n",
        "    IMDB ë°ì´í„°ì…‹ì„ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n",
        "    \n",
        "    Features:\n",
        "    - ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´\n",
        "    - ì§„í–‰ë¥  í‘œì‹œ ë° ì†ë„ ì¸¡ì •\n",
        "    - ê¸°ì¡´ íŒŒì¼ ê²€ì¦ ë° ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€\n",
        "    - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì²˜ë¦¬\n",
        "    \n",
        "    Returns:\n",
        "        bool: ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì—¬ë¶€\n",
        "    \"\"\"\n",
        "    \n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    file_name = \"../../data/aclImdb_v1.tar.gz\"\n",
        "    \n",
        "    # ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "    os.makedirs(\"../../data\", exist_ok=True)\n",
        "    \n",
        "    # ê¸°ì¡´ íŒŒì¼ í™•ì¸\n",
        "    if os.path.exists(file_name):\n",
        "        file_size = os.path.getsize(file_name) / (1024 * 1024)  # MB ë‹¨ìœ„\n",
        "        print(f\"âœ… íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {file_name}\")\n",
        "        print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.1f}MB\")\n",
        "        return True\n",
        "    \n",
        "    print(f\"ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\")\n",
        "    print(f\"ğŸ”— URL: {url}\")\n",
        "    print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {file_name}\")\n",
        "    \n",
        "    try:\n",
        "        # HTTP ìš”ì²­ ì‹œì‘\n",
        "        response = requests.get(url, stream=True, timeout=30)\n",
        "        response.raise_for_status()  # HTTP ì—ëŸ¬ í™•ì¸\n",
        "        \n",
        "        # íŒŒì¼ í¬ê¸° í™•ì¸\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        downloaded = 0\n",
        "        chunk_size = 8192  # 8KB\n",
        "        \n",
        "        print(f\"ğŸ“Š ì´ íŒŒì¼ í¬ê¸°: {total_size / (1024*1024):.1f}MB\")\n",
        "        \n",
        "        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
        "        with open(file_name, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                if chunk:  # ë¹ˆ ì²­í¬ í•„í„°ë§\n",
        "                    file.write(chunk)\n",
        "                    downloaded += len(chunk)\n",
        "                    \n",
        "                    # ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 1MBë§ˆë‹¤)\n",
        "                    if downloaded % (1024 * 1024) < chunk_size:\n",
        "                        progress = (downloaded / total_size) * 100 if total_size else 0\n",
        "                        print(f\"â¬ ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ : {progress:.1f}% ({downloaded/(1024*1024):.1f}MB)\")\n",
        "        \n",
        "        print(f\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
        "        print(f\"ğŸ“ ì €ì¥ëœ íŒŒì¼: {file_name}\")\n",
        "        return True\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return False\n",
        "\n",
        "# ë‹¤ìš´ë¡œë“œ ì‹¤í–‰ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
        "# success = download_imdb_dataset()\n",
        "# if success:\n",
        "#     print(\"ğŸ‰ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "# else:\n",
        "#     print(\"ğŸ’¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2.2 ì••ì¶• í•´ì œ ë° íŒŒì¼ êµ¬ì¡° ë¶„ì„\n",
        "\n",
        "### ğŸ—‚ï¸ tar.gz í˜•ì‹ ì´í•´\n",
        "- **tar**: ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë¬¶ëŠ” ì•„ì¹´ì´ë¸Œ í˜•ì‹\n",
        "- **gz**: gzip ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì ìš©\n",
        "- **Linux/Unix í‘œì¤€**: ì†ŒìŠ¤ ì½”ë“œ ë°°í¬ì— ë„ë¦¬ ì‚¬ìš©\n",
        "\n",
        "### ğŸ“ ì˜ˆìƒ ë””ë ‰í† ë¦¬ êµ¬ì¡°\n",
        "```\n",
        "aclImdb/\n",
        "â”œâ”€â”€ train/\n",
        "â”‚   â”œâ”€â”€ pos/          # ê¸ì • ë¦¬ë·° (12,500ê°œ)\n",
        "â”‚   â”œâ”€â”€ neg/          # ë¶€ì • ë¦¬ë·° (12,500ê°œ)\n",
        "â”‚   â””â”€â”€ unsup/        # ë¬´ë ˆì´ë¸” ë°ì´í„° (ì œê±° ì˜ˆì •)\n",
        "â”œâ”€â”€ test/\n",
        "â”‚   â”œâ”€â”€ pos/          # ê¸ì • ë¦¬ë·° (12,500ê°œ)\n",
        "â”‚   â””â”€â”€ neg/          # ë¶€ì • ë¦¬ë·° (12,500ê°œ)\n",
        "â””â”€â”€ README           # ë°ì´í„°ì…‹ ì„¤ëª…\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_imdb_dataset():\n",
        "    \"\"\"\n",
        "    IMDB ë°ì´í„°ì…‹ ì••ì¶• íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ í•´ì œí•˜ëŠ” í•¨ìˆ˜\n",
        "    \n",
        "    Features:\n",
        "    - í¬ë¡œìŠ¤ í”Œë«í¼ ì§€ì› (Python tarfile ëª¨ë“ˆ ì‚¬ìš©)\n",
        "    - ì§„í–‰ë¥  í‘œì‹œ ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§\n",
        "    - ê¸°ì¡´ ë””ë ‰í† ë¦¬ ê²€ì¦\n",
        "    - íŒŒì¼ êµ¬ì¡° ê²€ì¦\n",
        "    \n",
        "    Returns:\n",
        "        bool: ì••ì¶• í•´ì œ ì„±ê³µ ì—¬ë¶€\n",
        "    \"\"\"\n",
        "    \n",
        "    import tarfile\n",
        "    \n",
        "    archive_path = \"../../data/aclImdb_v1.tar.gz\"\n",
        "    extract_path = \"../../data/\"\n",
        "    target_dir = \"../../data/aclImdb\"\n",
        "    \n",
        "    # ì´ë¯¸ í•´ì œëœ ë””ë ‰í† ë¦¬ í™•ì¸\n",
        "    if os.path.exists(target_dir):\n",
        "        print(f\"âœ… ì´ë¯¸ ì••ì¶• í•´ì œëœ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤: {target_dir}\")\n",
        "        \n",
        "        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ê²€ì¦\n",
        "        required_dirs = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
        "        missing_dirs = []\n",
        "        \n",
        "        for dir_path in required_dirs:\n",
        "            full_path = os.path.join(target_dir, dir_path)\n",
        "            if not os.path.exists(full_path):\n",
        "                missing_dirs.append(dir_path)\n",
        "        \n",
        "        if missing_dirs:\n",
        "            print(f\"âš ï¸ ëˆ„ë½ëœ ë””ë ‰í† ë¦¬: {missing_dirs}\")\n",
        "            print(\"ğŸ”„ ì¬í•´ì œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
        "        else:\n",
        "            print(\"âœ… ë””ë ‰í† ë¦¬ êµ¬ì¡° ê²€ì¦ ì™„ë£Œ!\")\n",
        "            return True\n",
        "    \n",
        "    # ì••ì¶• íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
        "    if not os.path.exists(archive_path):\n",
        "        print(f\"âŒ ì••ì¶• íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {archive_path}\")\n",
        "        print(\"ğŸ’¡ ë¨¼ì € ë‹¤ìš´ë¡œë“œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"ğŸ“¦ ì••ì¶• í•´ì œ ì‹œì‘...\")\n",
        "    print(f\"ğŸ—‚ï¸ ì••ì¶• íŒŒì¼: {archive_path}\")\n",
        "    print(f\"ğŸ“ í•´ì œ ìœ„ì¹˜: {extract_path}\")\n",
        "    \n",
        "    try:\n",
        "        # tar íŒŒì¼ ì—´ê¸° ë° ê²€ì¦\n",
        "        with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "            # íŒŒì¼ ëª©ë¡ í™•ì¸\n",
        "            members = tar.getmembers()\n",
        "            total_files = len(members)\n",
        "            \n",
        "            print(f\"ğŸ“Š ì´ íŒŒì¼ ìˆ˜: {total_files:,}ê°œ\")\n",
        "            print(f\"â³ ì••ì¶• í•´ì œ ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
        "            \n",
        "            # ì•ˆì „í•œ ì••ì¶• í•´ì œ (ê²½ë¡œ ê²€ì¦)\n",
        "            def safe_extract(tarinfo, path):\n",
        "                # ê²½ë¡œ traversal ê³µê²© ë°©ì§€\n",
        "                if os.path.isabs(tarinfo.name) or \"..\" in tarinfo.name:\n",
        "                    print(f\"âš ï¸ ì•ˆì „í•˜ì§€ ì•Šì€ ê²½ë¡œ ê±´ë„ˆë›°ê¸°: {tarinfo.name}\")\n",
        "                    return None\n",
        "                return tarinfo\n",
        "            \n",
        "            # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ í•´ì œ\n",
        "            extracted_count = 0\n",
        "            for member in members:\n",
        "                if safe_extract(member, extract_path):\n",
        "                    tar.extract(member, extract_path)\n",
        "                    extracted_count += 1\n",
        "                    \n",
        "                    # ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 1000ê°œ íŒŒì¼ë§ˆë‹¤)\n",
        "                    if extracted_count % 1000 == 0:\n",
        "                        progress = (extracted_count / total_files) * 100\n",
        "                        print(f\"â¬ ì••ì¶• í•´ì œ ì§„í–‰ë¥ : {progress:.1f}% ({extracted_count:,}/{total_files:,})\")\n",
        "        \n",
        "        print(f\"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n",
        "        \n",
        "        # í•´ì œ ê²°ê³¼ ê²€ì¦\n",
        "        if os.path.exists(target_dir):\n",
        "            print(f\"ğŸ“ ìƒì„±ëœ ë””ë ‰í† ë¦¬: {target_dir}\")\n",
        "            \n",
        "            # ê° í•˜ìœ„ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ìˆ˜ í™•ì¸\n",
        "            subdirs = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
        "            for subdir in subdirs:\n",
        "                subdir_path = os.path.join(target_dir, subdir)\n",
        "                if os.path.exists(subdir_path):\n",
        "                    file_count = len([f for f in os.listdir(subdir_path) if f.endswith('.txt')])\n",
        "                    print(f\"  ğŸ“‚ {subdir}: {file_count:,}ê°œ íŒŒì¼\")\n",
        "            \n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ ëŒ€ìƒ ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "            return False\n",
        "            \n",
        "    except tarfile.TarError as e:\n",
        "        print(f\"âŒ tar íŒŒì¼ ì˜¤ë¥˜: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì••ì¶• í•´ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return False\n",
        "\n",
        "# ì••ì¶• í•´ì œ ì‹¤í–‰ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
        "# success = extract_imdb_dataset()\n",
        "# if success:\n",
        "#     print(\"ğŸ‰ ë°ì´í„° ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n",
        "# else:\n",
        "#     print(\"ğŸ’¥ ì••ì¶• í•´ì œ ì‹¤íŒ¨!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2.3 ë°ì´í„° ë¶„í•  ì „ëµ\n",
        "\n",
        "### ğŸ¯ ë¶„í• ì˜ ì¤‘ìš”ì„±\n",
        "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë°ì´í„° ë¶„í• ì€ ëª¨ë¸ì˜ ì‹ ë¢°ì„± ìˆëŠ” í‰ê°€ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ“Š ë¶„í•  ì „ëµ\n",
        "```\n",
        "ì›ë³¸ ë°ì´í„°: train(25,000) + test(25,000)\n",
        "        â†“\n",
        "ìµœì¢… êµ¬ì¡°: train(20,000) + val(5,000) + test(25,000)\n",
        "```\n",
        "\n",
        "### ğŸ”„ ë¶„í•  ê³¼ì •\n",
        "1. **Train ë°ì´í„° ë¶„í• **: 25,000ê°œ â†’ 20,000ê°œ(í›ˆë ¨) + 5,000ê°œ(ê²€ì¦)\n",
        "2. **ë¬´ì‘ìœ„ ìƒ˜í”Œë§**: í¸í–¥ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ì…”í”Œ\n",
        "3. **í´ë˜ìŠ¤ ê· í˜• ìœ ì§€**: ê¸ì •/ë¶€ì • ë¹„ìœ¨ 50:50 ìœ ì§€\n",
        "4. **Unsupervised ë°ì´í„° ì œê±°**: ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„° ì‚­ì œ\n",
        "\n",
        "### ğŸ’¡ ë¶„í•  ë¹„ìœ¨ ì„ íƒ ì´ìœ \n",
        "- **Train 80%**: ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„° í™•ë³´\n",
        "- **Validation 20%**: ì‹ ë¢°í•  ë§Œí•œ ì„±ëŠ¥ ì¶”ì •\n",
        "- **Test**: ì™„ì „ ë…ë¦½ì  ìµœì¢… í‰ê°€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_validation_split(validation_ratio=0.2, random_seed=1337):\n",
        "    \"\"\"\n",
        "    í›ˆë ¨ ë°ì´í„°ë¥¼ train/validationìœ¼ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        validation_ratio (float): ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 20%)\n",
        "        random_seed (int): ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ\n",
        "    \n",
        "    Features:\n",
        "    - í´ë˜ìŠ¤ë³„ ê· ë“± ë¶„í•  (ê¸ì •/ë¶€ì • ë¹„ìœ¨ ìœ ì§€)\n",
        "    - ê¸°ì¡´ ë¶„í•  ê²€ì¦ ë° ê±´ë„ˆë›°ê¸°\n",
        "    - ìƒì„¸í•œ ë¶„í•  í†µê³„ ì œê³µ\n",
        "    - ë¬´ë ˆì´ë¸” ë°ì´í„° ìë™ ì œê±°\n",
        "    \n",
        "    Returns:\n",
        "        bool: ë¶„í•  ì„±ê³µ ì—¬ë¶€\n",
        "    \"\"\"\n",
        "    \n",
        "    base_dir = pathlib.Path(\"../../data/aclImdb\")\n",
        "    train_dir = base_dir / \"train\"\n",
        "    val_dir = base_dir / \"val\"\n",
        "    \n",
        "    # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸\n",
        "    if not base_dir.exists():\n",
        "        print(f\"âŒ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}\")\n",
        "        print(\"ğŸ’¡ ë¨¼ì € ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
        "        return False\n",
        "    \n",
        "    # ì´ë¯¸ ë¶„í• ì´ ì™„ë£Œëœ ê²½ìš° í™•ì¸\n",
        "    if val_dir.exists():\n",
        "        print(f\"âœ… Validation ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {val_dir}\")\n",
        "        \n",
        "        # ë¶„í•  ìƒíƒœ ê²€ì¦\n",
        "        categories = ['pos', 'neg']\n",
        "        split_valid = True\n",
        "        \n",
        "        for category in categories:\n",
        "            train_count = len(list((train_dir / category).glob('*.txt'))) if (train_dir / category).exists() else 0\n",
        "            val_count = len(list((val_dir / category).glob('*.txt'))) if (val_dir / category).exists() else 0\n",
        "            \n",
        "            print(f\"  ğŸ“‚ {category}: train({train_count:,}) + val({val_count:,}) = {train_count + val_count:,}\")\n",
        "            \n",
        "            if train_count == 0 or val_count == 0:\n",
        "                split_valid = False\n",
        "        \n",
        "        if split_valid:\n",
        "            print(\"âœ… ë°ì´í„° ë¶„í• ì´ ì´ë¯¸ ì™„ë£Œë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âš ï¸ ë¶„í•  ìƒíƒœê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ì¬ë¶„í• ì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
        "    \n",
        "    print(f\"ğŸ”„ ë°ì´í„° ë¶„í•  ì‹œì‘\")\n",
        "    print(f\"ğŸ“Š ë¶„í•  ë¹„ìœ¨: Train {(1-validation_ratio)*100:.0f}% | Validation {validation_ratio*100:.0f}%\")\n",
        "    print(f\"ğŸ² ëœë¤ ì‹œë“œ: {random_seed}\")\n",
        "    \n",
        "    try:\n",
        "        # Validation ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "        val_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # ë¬´ë ˆì´ë¸” ë°ì´í„° ì œê±°\n",
        "        unsup_dir = train_dir / \"unsup\"\n",
        "        if unsup_dir.exists():\n",
        "            print(f\"ğŸ—‘ï¸ ë¬´ë ˆì´ë¸” ë°ì´í„° ì œê±° ì¤‘: {unsup_dir}\")\n",
        "            shutil.rmtree(unsup_dir)\n",
        "            print(\"âœ… ë¬´ë ˆì´ë¸” ë°ì´í„° ì œê±° ì™„ë£Œ\")\n",
        "        \n",
        "        total_moved = 0\n",
        "        \n",
        "        # ê° ì¹´í…Œê³ ë¦¬ë³„ ë¶„í•  ì²˜ë¦¬\n",
        "        for category in [\"neg\", \"pos\"]:\n",
        "            print(f\"\\nğŸ“‚ {category.upper()} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘...\")\n",
        "            \n",
        "            # ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "            train_category_dir = train_dir / category\n",
        "            val_category_dir = val_dir / category\n",
        "            val_category_dir.mkdir(exist_ok=True)\n",
        "            \n",
        "            # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "            if not train_category_dir.exists():\n",
        "                print(f\"âš ï¸ ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {train_category_dir}\")\n",
        "                continue\n",
        "                \n",
        "            files = [f for f in train_category_dir.glob('*.txt')]\n",
        "            original_count = len(files)\n",
        "            \n",
        "            print(f\"  ğŸ“Š ì›ë³¸ íŒŒì¼ ìˆ˜: {original_count:,}ê°œ\")\n",
        "            \n",
        "            # íŒŒì¼ ëª©ë¡ ëœë¤ ì…”í”Œ\n",
        "            rng = random.Random(random_seed)\n",
        "            rng.shuffle(files)\n",
        "            \n",
        "            # Validationìš© íŒŒì¼ ìˆ˜ ê³„ì‚°\n",
        "            num_val_samples = int(validation_ratio * len(files))\n",
        "            val_files = files[:num_val_samples]  # ì²˜ìŒ 20%ë¥¼ validationìœ¼ë¡œ\n",
        "            \n",
        "            print(f\"  â¬‡ï¸ Validationìœ¼ë¡œ ì´ë™: {num_val_samples:,}ê°œ\")\n",
        "            print(f\"  ğŸ“‹ Trainì— ìœ ì§€: {len(files) - num_val_samples:,}ê°œ\")\n",
        "            \n",
        "            # íŒŒì¼ ì´ë™\n",
        "            moved_count = 0\n",
        "            failed_count = 0\n",
        "            \n",
        "            for file_path in val_files:\n",
        "                try:\n",
        "                    dest_path = val_category_dir / file_path.name\n",
        "                    shutil.move(str(file_path), str(dest_path))\n",
        "                    moved_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  âš ï¸ íŒŒì¼ ì´ë™ ì‹¤íŒ¨ {file_path.name}: {e}\")\n",
        "                    failed_count += 1\n",
        "            \n",
        "            print(f\"  âœ… ì„±ê³µì ìœ¼ë¡œ ì´ë™: {moved_count:,}ê°œ\")\n",
        "            if failed_count > 0:\n",
        "                print(f\"  âŒ ì´ë™ ì‹¤íŒ¨: {failed_count:,}ê°œ\")\n",
        "            \n",
        "            total_moved += moved_count\n",
        "        \n",
        "        print(f\"\\nğŸ‰ ë°ì´í„° ë¶„í•  ì™„ë£Œ!\")\n",
        "        print(f\"ğŸ“Š ì´ ì´ë™ëœ íŒŒì¼: {total_moved:,}ê°œ\")\n",
        "        \n",
        "        # ìµœì¢… ë¶„í•  ê²°ê³¼ í™•ì¸\n",
        "        print(f\"\\nğŸ“ˆ ìµœì¢… ë¶„í•  ê²°ê³¼:\")\n",
        "        print(\"=\" * 40)\n",
        "        \n",
        "        for split_name in ['train', 'val']:\n",
        "            split_dir = base_dir / split_name\n",
        "            if split_dir.exists():\n",
        "                pos_count = len(list((split_dir / 'pos').glob('*.txt')))\n",
        "                neg_count = len(list((split_dir / 'neg').glob('*.txt')))\n",
        "                total_count = pos_count + neg_count\n",
        "                \n",
        "                print(f\"{split_name.upper():>5}: ê¸ì • {pos_count:,} + ë¶€ì • {neg_count:,} = ì´ {total_count:,}ê°œ\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ë°ì´í„° ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return False\n",
        "\n",
        "# ë°ì´í„° ë¶„í•  ì‹¤í–‰ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
        "# success = create_validation_split()\n",
        "# if success:\n",
        "#     print(\"ğŸ‰ ë°ì´í„° ë¶„í•  ì™„ë£Œ!\")\n",
        "# else:\n",
        "#     print(\"ğŸ’¥ ë°ì´í„° ë¶„í•  ì‹¤íŒ¨!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part III: í…ìŠ¤íŠ¸ ë²¡í„°í™” ë° ì „ì²˜ë¦¬\n",
        "\n",
        "## 3.1 ë°ì´í„°ì…‹ ë¡œë”© ì‹œìŠ¤í…œ\n",
        "\n",
        "### ğŸ¯ Keras Dataset API í™œìš©\n",
        "`text_dataset_from_directory`ëŠ” ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ“ ì˜ˆìƒ ë””ë ‰í† ë¦¬ êµ¬ì¡°\n",
        "```\n",
        "../../data/aclImdb/\n",
        "â”œâ”€â”€ train/\n",
        "â”‚   â”œâ”€â”€ pos/        # ê¸ì • ë¦¬ë·° â†’ ë¼ë²¨ 1\n",
        "â”‚   â””â”€â”€ neg/        # ë¶€ì • ë¦¬ë·° â†’ ë¼ë²¨ 0\n",
        "â”œâ”€â”€ val/\n",
        "â”‚   â”œâ”€â”€ pos/        # ê¸ì • ë¦¬ë·° â†’ ë¼ë²¨ 1  \n",
        "â”‚   â””â”€â”€ neg/        # ë¶€ì • ë¦¬ë·° â†’ ë¼ë²¨ 0\n",
        "â””â”€â”€ test/\n",
        "    â”œâ”€â”€ pos/        # ê¸ì • ë¦¬ë·° â†’ ë¼ë²¨ 1\n",
        "    â””â”€â”€ neg/        # ë¶€ì • ë¦¬ë·° â†’ ë¼ë²¨ 0\n",
        "```\n",
        "\n",
        "### âš™ï¸ ì£¼ìš” ì„¤ì • íŒŒë¼ë¯¸í„°\n",
        "- **batch_size**: í•œ ë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜ (ë©”ëª¨ë¦¬ vs í•™ìŠµ ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)\n",
        "- **shuffle**: í›ˆë ¨ ë°ì´í„° ì…”í”Œë§ (ê³¼ì í•© ë°©ì§€)\n",
        "- **validation_split**: ìë™ ë¶„í•  ëŒ€ì‹  ìˆ˜ë™ ë¶„í•  ì‚¬ìš©\n",
        "- **subset**: íŠ¹ì • ë°ì´í„° ë¶„í• ë§Œ ë¡œë“œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ IMDB ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘\n",
            "==================================================\n",
            "ğŸ“Š ë°°ì¹˜ í¬ê¸°: 32\n",
            "ğŸ“ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬: ../../data/aclImdb\n",
            "\n",
            "ğŸ”„ í›ˆë ¨ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
            "Found 20000 files belonging to 2 classes.\n",
            "ğŸ”„ ê²€ì¦ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
            "Found 5000 files belonging to 2 classes.\n",
            "ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
            "Found 25000 files belonging to 2 classes.\n",
            "\n",
            "âœ… ëª¨ë“  ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ!\n",
            "\n",
            "ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:\n",
            "------------------------------\n",
            "       TRAIN: ~20,000ê°œ ìƒ˜í”Œ (625 ë°°ì¹˜)\n",
            "  VALIDATION: ~5,024ê°œ ìƒ˜í”Œ (157 ë°°ì¹˜)\n",
            "        TEST: ~25,024ê°œ ìƒ˜í”Œ (782 ë°°ì¹˜)\n",
            "\n",
            "ğŸ·ï¸ í´ë˜ìŠ¤ ì •ë³´: ['neg', 'pos']\n",
            "   - neg â†’ ë¼ë²¨ 0\n",
            "   - pos â†’ ë¼ë²¨ 1\n",
            "\n",
            "ğŸ‰ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!\n",
            "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° 32 ì‚¬ìš©\n"
          ]
        }
      ],
      "source": [
        "def load_imdb_datasets(batch_size=32, max_length=None):\n",
        "    \"\"\"\n",
        "    IMDB ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        batch_size (int): ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 32)\n",
        "        max_length (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: None)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (train_ds, val_ds, test_ds) ë˜ëŠ” None\n",
        "    \"\"\"\n",
        "    \n",
        "    base_dir = \"../../data/aclImdb\"\n",
        "    \n",
        "    print(\"ğŸ“‚ IMDB ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "    print(f\"ğŸ“ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬: {base_dir}\")\n",
        "    \n",
        "    # ë°ì´í„° ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸\n",
        "    directories = {\n",
        "        'train': f\"{base_dir}/train\",\n",
        "        'val': f\"{base_dir}/val\", \n",
        "        'test': f\"{base_dir}/test\"\n",
        "    }\n",
        "    \n",
        "    for name, path in directories.items():\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"âŒ {name} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}\")\n",
        "            return None, None, None\n",
        "    \n",
        "    try:\n",
        "        # í›ˆë ¨ ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "        print(\"\\nğŸ”„ í›ˆë ¨ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
        "        train_ds = keras.utils.text_dataset_from_directory(\n",
        "            directories['train'],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,           # í›ˆë ¨ ë°ì´í„°ëŠ” ì…”í”Œ\n",
        "            seed=1337,             # ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ì‹œë“œ\n",
        "            max_length=max_length   # ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì˜µì…˜)\n",
        "        )\n",
        "        \n",
        "        # ê²€ì¦ ë°ì´í„°ì…‹ ë¡œë“œ  \n",
        "        print(\"ğŸ”„ ê²€ì¦ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
        "        val_ds = keras.utils.text_dataset_from_directory(\n",
        "            directories['val'],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,          # ê²€ì¦ ë°ì´í„°ëŠ” ì…”í”Œí•˜ì§€ ì•ŠìŒ\n",
        "            max_length=max_length\n",
        "        )\n",
        "        \n",
        "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "        print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
        "        test_ds = keras.utils.text_dataset_from_directory(\n",
        "            directories['test'],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,          # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì…”í”Œí•˜ì§€ ì•ŠìŒ\n",
        "            max_length=max_length\n",
        "        )\n",
        "        \n",
        "        print(\"\\nâœ… ëª¨ë“  ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ!\")\n",
        "        \n",
        "        # ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸\n",
        "        print(\"\\nğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        datasets = [\n",
        "            (\"TRAIN\", train_ds),\n",
        "            (\"VALIDATION\", val_ds), \n",
        "            (\"TEST\", test_ds)\n",
        "        ]\n",
        "        \n",
        "        for name, dataset in datasets:\n",
        "            # ë°°ì¹˜ ìˆ˜ ê³„ì‚° (ê·¼ì‚¬ì¹˜)\n",
        "            batch_count = 0\n",
        "            total_samples = 0\n",
        "            \n",
        "            for batch in dataset.take(5):  # ì²˜ìŒ 5ê°œ ë°°ì¹˜ë§Œ í™•ì¸\n",
        "                batch_count += 1\n",
        "                total_samples += len(batch[0])\n",
        "            \n",
        "            # ì „ì²´ ë°°ì¹˜ ìˆ˜ ì¶”ì •\n",
        "            estimated_batches = len(list(dataset))\n",
        "            estimated_samples = estimated_batches * batch_size\n",
        "            \n",
        "            print(f\"  {name:>10}: ~{estimated_samples:,}ê°œ ìƒ˜í”Œ ({estimated_batches:,} ë°°ì¹˜)\")\n",
        "        \n",
        "        # í´ë˜ìŠ¤ ì •ë³´ í™•ì¸\n",
        "        class_names = train_ds.class_names\n",
        "        print(f\"\\nğŸ·ï¸ í´ë˜ìŠ¤ ì •ë³´: {class_names}\")\n",
        "        print(f\"   - {class_names[0]} â†’ ë¼ë²¨ 0\")\n",
        "        print(f\"   - {class_names[1]} â†’ ë¼ë²¨ 1\")\n",
        "        \n",
        "        return train_ds, val_ds, test_ds\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¡œë”© ì‹¤í–‰\n",
        "BATCH_SIZE = 32\n",
        "train_ds, val_ds, test_ds = load_imdb_datasets(batch_size=BATCH_SIZE)\n",
        "\n",
        "if train_ds is not None:\n",
        "    print(\"\\nğŸ‰ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° {BATCH_SIZE} ì‚¬ìš©\")\n",
        "else:\n",
        "    print(\"\\nğŸ’¥ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨!\")\n",
        "    print(\"ğŸ’¡ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¶„í• ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3.2 ë°ì´í„° êµ¬ì¡° ë¶„ì„\n",
        "\n",
        "### ğŸ” ì›ì‹œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
        "ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì „ì²˜ë¦¬ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ“Š ë¶„ì„ í•­ëª©\n",
        "- **ë°ì´í„° íƒ€ì…**: ë¬¸ìì—´ í…ì„œ í™•ì¸\n",
        "- **ë°°ì¹˜ êµ¬ì¡°**: (í…ìŠ¤íŠ¸, ë¼ë²¨) ìŒ ê²€ì¦  \n",
        "- **í…ìŠ¤íŠ¸ ê¸¸ì´**: ìµœì†Œ/ìµœëŒ€/í‰ê·  ê¸¸ì´ ë¶„ì„\n",
        "- **ë¼ë²¨ ë¶„í¬**: í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸\n",
        "- **ì‹¤ì œ ë‚´ìš©**: ìƒ˜í”Œ ë¦¬ë·° ë‚´ìš© ê²€í† \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” TRAIN ë°ì´í„° êµ¬ì¡° ë¶„ì„\n",
            "==================================================\n",
            "ğŸ“Š ë°°ì¹˜ ì •ë³´:\n",
            "  í…ìŠ¤íŠ¸ í˜•íƒœ: (32,) (ë°°ì¹˜í¬ê¸°=32)\n",
            "  í…ìŠ¤íŠ¸ íƒ€ì…: <dtype: 'string'>\n",
            "  ë¼ë²¨ í˜•íƒœ: (32,)\n",
            "  ë¼ë²¨ íƒ€ì…: <dtype: 'int32'>\n",
            "\n",
            "ğŸ·ï¸ í˜„ì¬ ë°°ì¹˜ì˜ ë¼ë²¨ ë¶„í¬:\n",
            "  ë¼ë²¨ 0 (ë¶€ì •): 15ê°œ\n",
            "  ë¼ë²¨ 1 (ê¸ì •): 17ê°œ\n",
            "\n",
            "ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„ (ì²˜ìŒ 2ê°œ ìƒ˜í”Œ):\n",
            "  ìƒ˜í”Œ 1: 871ë‹¨ì–´\n",
            "  ìƒ˜í”Œ 2: 140ë‹¨ì–´\n",
            "\n",
            "ğŸ“ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë‚´ìš©:\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ¬ ìƒ˜í”Œ 1: ğŸ˜ ë¶€ì • (ë¼ë²¨: 0)\n",
            "ğŸ“Š ê¸¸ì´: 871ë‹¨ì–´, 4771ì\n",
            "ğŸ“– ë‚´ìš©: I just got the UK 4-disc special edition of Superman 1 for about $5. The additional stuff includes the 1951 feature Superman and the Mole-Men. So I slapped it into the DVD player last night, and here ...\n",
            "âš ï¸ HTML íƒœê·¸ ë°œê²¬ - ì „ì²˜ë¦¬ í•„ìš”\n",
            "\n",
            "ğŸ¬ ìƒ˜í”Œ 2: ğŸ˜ ë¶€ì • (ë¼ë²¨: 0)\n",
            "ğŸ“Š ê¸¸ì´: 140ë‹¨ì–´, 763ì\n",
            "ğŸ“– ë‚´ìš©: There is one good thing in this movie: Lola Glaudini's ass! Sorry to be so blunt but it's the truth. Too bad she didn't do a nude. It would at least have made this mess tolerable. We see another chick...\n",
            "\n",
            "======================================================================\n",
            "ğŸ’¡ ë°ì´í„° ë¶„ì„ ê²°ê³¼ ìš”ì•½\n",
            "======================================================================\n",
            "âœ… ë°ì´í„° í˜•íƒœ: ë¬¸ìì—´ í…ì„œ (string tensor)\n",
            "âœ… ë¼ë²¨ ì¸ì½”ë”©: 0=ë¶€ì •, 1=ê¸ì •\n",
            "âš ï¸ HTML íƒœê·¸ ì¡´ì¬: ì „ì²˜ë¦¬ì—ì„œ ì œê±° í•„ìš”\n",
            "ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: ë‹¤ì–‘í•¨ (ë²¡í„°í™”ì—ì„œ í‘œì¤€í™” í•„ìš”)\n",
            "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: TextVectorizationìœ¼ë¡œ ìˆ˜ì¹˜í™”\n"
          ]
        }
      ],
      "source": [
        "def analyze_raw_data(dataset, dataset_name=\"DATASET\", max_samples=3):\n",
        "    \"\"\"\n",
        "    ì›ì‹œ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŠ¹ì„±ì„ ìƒì„¸ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        dataset: ë¶„ì„í•  TensorFlow ë°ì´í„°ì…‹\n",
        "        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (ì¶œë ¥ìš©)\n",
        "        max_samples: ë¶„ì„í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜\n",
        "    \"\"\"\n",
        "    \n",
        "    if dataset is None:\n",
        "        print(f\"âŒ {dataset_name} ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"ğŸ” {dataset_name} ë°ì´í„° êµ¬ì¡° ë¶„ì„\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # ì²« ë²ˆì§¸ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°\n",
        "    sample_batch = next(iter(dataset))\n",
        "    texts, labels = sample_batch\n",
        "    \n",
        "    # ê¸°ë³¸ êµ¬ì¡° ì •ë³´\n",
        "    print(f\"ğŸ“Š ë°°ì¹˜ ì •ë³´:\")\n",
        "    print(f\"  í…ìŠ¤íŠ¸ í˜•íƒœ: {texts.shape} (ë°°ì¹˜í¬ê¸°={texts.shape[0]})\")\n",
        "    print(f\"  í…ìŠ¤íŠ¸ íƒ€ì…: {texts.dtype}\")\n",
        "    print(f\"  ë¼ë²¨ í˜•íƒœ: {labels.shape}\")\n",
        "    print(f\"  ë¼ë²¨ íƒ€ì…: {labels.dtype}\")\n",
        "    \n",
        "    # ë¼ë²¨ ë¶„í¬ í™•ì¸\n",
        "    unique_labels, _, counts = tf.unique_with_counts(labels)\n",
        "    print(f\"\\nğŸ·ï¸ í˜„ì¬ ë°°ì¹˜ì˜ ë¼ë²¨ ë¶„í¬:\")\n",
        "    for label, count in zip(unique_labels.numpy(), counts.numpy()):\n",
        "        label_name = \"ê¸ì •\" if label == 1 else \"ë¶€ì •\"\n",
        "        print(f\"  ë¼ë²¨ {label} ({label_name}): {count}ê°œ\")\n",
        "    \n",
        "    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
        "    text_lengths = [len(text.numpy().decode('utf-8').split()) for text in texts[:max_samples]]\n",
        "    \n",
        "    print(f\"\\nğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„ (ì²˜ìŒ {max_samples}ê°œ ìƒ˜í”Œ):\")\n",
        "    for i, length in enumerate(text_lengths):\n",
        "        print(f\"  ìƒ˜í”Œ {i+1}: {length}ë‹¨ì–´\")\n",
        "    \n",
        "    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë‚´ìš© í™•ì¸\n",
        "    print(f\"\\nğŸ“ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë‚´ìš©:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for i in range(min(max_samples, len(texts))):\n",
        "        text_content = texts[i].numpy().decode('utf-8')\n",
        "        label_value = labels[i].numpy()\n",
        "        label_name = \"ğŸ˜Š ê¸ì •\" if label_value == 1 else \"ğŸ˜ ë¶€ì •\"\n",
        "        \n",
        "        # í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ìš”ì•½\n",
        "        words = text_content.split()\n",
        "        word_count = len(words)\n",
        "        \n",
        "        print(f\"\\nğŸ¬ ìƒ˜í”Œ {i+1}: {label_name} (ë¼ë²¨: {label_value})\")\n",
        "        print(f\"ğŸ“Š ê¸¸ì´: {word_count}ë‹¨ì–´, {len(text_content)}ì\")\n",
        "        \n",
        "        # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì)\n",
        "        preview = text_content[:200]\n",
        "        if len(text_content) > 200:\n",
        "            preview += \"...\"\n",
        "        \n",
        "        print(f\"ğŸ“– ë‚´ìš©: {preview}\")\n",
        "        \n",
        "        # HTML íƒœê·¸ í™•ì¸\n",
        "        if '<br />' in text_content or '<' in text_content:\n",
        "            print(\"âš ï¸ HTML íƒœê·¸ ë°œê²¬ - ì „ì²˜ë¦¬ í•„ìš”\")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° ë¶„ì„\n",
        "if train_ds is not None:\n",
        "    analyze_raw_data(train_ds, \"TRAIN\", max_samples=2)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ’¡ ë°ì´í„° ë¶„ì„ ê²°ê³¼ ìš”ì•½\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"âœ… ë°ì´í„° í˜•íƒœ: ë¬¸ìì—´ í…ì„œ (string tensor)\")\n",
        "    print(\"âœ… ë¼ë²¨ ì¸ì½”ë”©: 0=ë¶€ì •, 1=ê¸ì •\")\n",
        "    print(\"âš ï¸ HTML íƒœê·¸ ì¡´ì¬: ì „ì²˜ë¦¬ì—ì„œ ì œê±° í•„ìš”\")\n",
        "    print(\"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: ë‹¤ì–‘í•¨ (ë²¡í„°í™”ì—ì„œ í‘œì¤€í™” í•„ìš”)\")\n",
        "    print(\"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: TextVectorizationìœ¼ë¡œ ìˆ˜ì¹˜í™”\")\n",
        "else:\n",
        "    print(\"âŒ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. í…ìŠ¤íŠ¸ ë²¡í„°í™” ì„¤ì •\n",
        "\n",
        "TextVectorizationì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "- Multi-hot encoding (Bag of Words) ë°©ì‹ ì‚¬ìš©\n",
        "- ìƒìœ„ 20,000ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤€ë¹„ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# í…ìŠ¤íŠ¸ ë²¡í„°í™” ë ˆì´ì–´ ìƒì„±\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,  # ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ 20,000ê°œë§Œ ì§€ì •\n",
        "    output_mode=\"multi_hot\"  # Multi-hot encoding (BoW ë°©ì‹)\n",
        "    # ê° ë¦¬ë·°ë§ˆë‹¤ 20,000ê°œ ìš”ì†Œë¥¼ ê°–ëŠ” ë°°ì—´ ìƒì„±\n",
        "    # ë°°ì—´ì—ì„œ ë¬¸ì¥ ì¤‘ ë‹¨ì–´ê°€ ìˆëŠ” ê³³ì€ 1, ì—†ìœ¼ë©´ 0\n",
        ")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ë ˆì´ë¸” ì œê±°)\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "print(\"í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤€ë¹„ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. ë°ì´í„°ì…‹ ë²¡í„°í™”\n",
        "\n",
        "ê° ë°ì´í„°ì…‹ì— í…ìŠ¤íŠ¸ ë²¡í„°í™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
        "- ìœ ë‹ˆê·¸ë¨(Unigram): ë‹¨ì–´ë¥¼ 1ê°œì”© ì²˜ë¦¬í•˜ëŠ” ë°©ì‹\n",
        "- ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë°ì´í„°ì…‹ ë²¡í„°í™” ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ CPU ì½”ì–´ 4ê°œ ì‚¬ìš©í•˜ì—¬ ë²¡í„°í™” ìˆ˜í–‰\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "print(\"ë°ì´í„°ì…‹ ë²¡í„°í™” ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. ë²¡í„°í™” ê²°ê³¼ í™•ì¸\n",
        "\n",
        "ë²¡í„°í™”ëœ ë°ì´í„°ì˜ í˜•íƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë²¡í„°í™” í›„ ë°ì´í„° í™•ì¸\n",
            "==================================================\n",
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(\n",
            "[[1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]], shape=(3, 20000), dtype=float32)\n",
            "targets[0]: tf.Tensor([0 0 1], shape=(3,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(\"ë²¡í„°í™” í›„ ë°ì´í„° í™•ì¸\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[:3])\n",
        "    print(\"targets[0]:\", targets[:3])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. ëª¨ë¸ ì •ì˜\n",
        "\n",
        "ê°„ë‹¨í•œ Dense ì‹ ê²½ë§ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "- ì…ë ¥ì¸µ: 20,000ì°¨ì› (ì–´íœ˜ ì‚¬ì „ í¬ê¸°)\n",
        "- ì€ë‹‰ì¸µ: 16ê°œ ë‰´ëŸ°, ReLU í™œì„±í™”\n",
        "- ë“œë¡­ì•„ì›ƒ: 0.5 (ê³¼ì í•© ë°©ì§€)\n",
        "- ì¶œë ¥ì¸µ: 1ê°œ ë‰´ëŸ°, ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” (ì´ì§„ ë¶„ë¥˜)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def getModel(max_tokens=20000, hidden_dim=16):\n",
        "    \"\"\"ëª¨ë¸ ìƒì„± ë° ë°˜í™˜ í•¨ìˆ˜\"\"\"\n",
        "    inputs = keras.Input(shape=(max_tokens,))  # ì…ë ¥ì¸µ\n",
        "    x = layers.Dense(hidden_dim, activation='relu')(inputs)  # ì€ë‹‰ì¸µ\n",
        "    x = layers.Dropout(0.5)(x)  # ë“œë¡­ì•„ì›ƒ\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)  # ì¶œë ¥ì¸µ\n",
        "    \n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "model = getModel()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 11. ëª¨ë¸ í›ˆë ¨\n",
        "\n",
        "ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.\n",
        "- cache(): ì²« ë²ˆì§¸ ì—í¬í¬ì—ì„œ ì „ì²˜ë¦¬ë¥¼ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ê³  ë©”ëª¨ë¦¬ì— ìºì‹±\n",
        "- ë©”ëª¨ë¦¬ì— ë“¤ì–´ê°ˆ ë§Œí¼ ì‘ì€ ë°ì´í„°ì…‹ì¼ ë•Œë§Œ íš¨ê³¼ì \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2973 - accuracy: 0.8869 - val_loss: 0.2946 - val_accuracy: 0.8814\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2532 - accuracy: 0.9104 - val_loss: 0.3049 - val_accuracy: 0.8828\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2242 - accuracy: 0.9205 - val_loss: 0.3155 - val_accuracy: 0.8860\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2232 - accuracy: 0.9230 - val_loss: 0.3225 - val_accuracy: 0.8870\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2118 - accuracy: 0.9279 - val_loss: 0.3545 - val_accuracy: 0.8760\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2067 - accuracy: 0.9337 - val_loss: 0.3493 - val_accuracy: 0.8886\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2074 - accuracy: 0.9331 - val_loss: 0.3645 - val_accuracy: 0.8818\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2022 - accuracy: 0.9346 - val_loss: 0.3732 - val_accuracy: 0.8822\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2001 - accuracy: 0.9380 - val_loss: 0.3834 - val_accuracy: 0.8812\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1924 - accuracy: 0.9369 - val_loss: 0.3928 - val_accuracy: 0.8776\n",
            "ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ì½œë°± ì„¤ì •: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"binary_1gram.keras\", \n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨\n",
        "model.fit(\n",
        "    binary_1gram_train_ds.cache(),  # í›ˆë ¨ ë°ì´í„° (ìºì‹± ì ìš©)\n",
        "    validation_data=binary_1gram_val_ds,  # ê²€ì¦ ë°ì´í„°\n",
        "    epochs=10,  # ì—í¬í¬ ìˆ˜\n",
        "    callbacks=callbacks  # ì½œë°±\n",
        ")\n",
        "\n",
        "print(\"ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 12. ëª¨ë¸ í‰ê°€\n",
        "\n",
        "ì €ì¥ëœ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3027 - accuracy: 0.8742\n",
            "í…ŒìŠ¤íŠ¸ì…‹ ì •í™•ë„: 0.8742\n"
          ]
        }
      ],
      "source": [
        "# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
        "model = models.load_model(\"binary_1gram.keras\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
        "test_results = model.evaluate(binary_1gram_test_ds)\n",
        "print(f\"í…ŒìŠ¤íŠ¸ì…‹ ì •í™•ë„: {test_results[1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part V: í”„ë¡œì íŠ¸ ê²°ë¡  ë° ì‹¤ë¬´ ì ìš©\n",
        "\n",
        "## 5.1 êµ¬í˜„ ì„±ê³¼ ìš”ì•½\n",
        "\n",
        "### ğŸ¯ ë‹¬ì„±í•œ ëª©í‘œ\n",
        "1. **ì™„ì „í•œ NLP íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**\n",
        "   - âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬ ìë™í™”\n",
        "   - âœ… ì²´ê³„ì ì¸ ë°ì´í„° ë¶„í•  (train/val/test)\n",
        "   - âœ… íš¨ìœ¨ì ì¸ í…ìŠ¤íŠ¸ ë²¡í„°í™” (Bag of Words)\n",
        "   - âœ… ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶• ë° í›ˆë ¨\n",
        "\n",
        "2. **ì‹¤ë¬´ ìˆ˜ì¤€ì˜ ì½”ë“œ í’ˆì§ˆ**\n",
        "   - âœ… ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì˜ˆì™¸ ìƒí™© ëŒ€ì‘\n",
        "   - âœ… ì§„í–‰ë¥  í‘œì‹œ ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§\n",
        "   - âœ… ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ë³´ì¥ (ì‹œë“œ ì„¤ì •)\n",
        "   - âœ… ëª¨ë“ˆí™”ëœ í•¨ìˆ˜ ì„¤ê³„\n",
        "\n",
        "3. **ì„±ëŠ¥ ìµœì í™” ê¸°ë²• ì ìš©**\n",
        "   - âœ… ë°ì´í„° ìºì‹±ìœ¼ë¡œ í›ˆë ¨ ì†ë„ í–¥ìƒ\n",
        "   - âœ… ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©\n",
        "   - âœ… ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”\n",
        "   - âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ë¡œë”©\n",
        "\n",
        "## 5.2 ê¸°ìˆ ì  ì„±ê³¼\n",
        "\n",
        "### ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ\n",
        "```\n",
        "ì˜ˆìƒ ì„±ëŠ¥ (IMDB ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€):\n",
        "- Baseline (Random): 50% ì •í™•ë„\n",
        "- Bag of Words + Dense: 85-89% ì •í™•ë„  \n",
        "- ê³ ê¸‰ ëª¨ë¸ (RNN/BERT): 90-95% ì •í™•ë„\n",
        "```\n",
        "\n",
        "### ğŸ› ï¸ ì‚¬ìš©ëœ í•µì‹¬ ê¸°ìˆ \n",
        "- **TensorFlow/Keras**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬\n",
        "- **TextVectorization**: íš¨ìœ¨ì ì¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "- **tf.data API**: ê³ ì„±ëŠ¥ ë°ì´í„° íŒŒì´í”„ë¼ì¸\n",
        "- **Multi-hot Encoding**: í•´ì„ ê°€ëŠ¥í•œ íŠ¹ì„± í‘œí˜„\n",
        "\n",
        "## 5.3 ì‹¤ë¬´ ì ìš© ë°©ì•ˆ\n",
        "\n",
        "### ğŸ¢ ì‚°ì—…ë³„ í™œìš© ì‚¬ë¡€\n",
        "\n",
        "#### 1) ì „ììƒê±°ë˜\n",
        "```python\n",
        "# ì œí’ˆ ë¦¬ë·° ìë™ ë¶„ì„\n",
        "review_sentiment = model.predict(\"This product is amazing!\")\n",
        "if review_sentiment > 0.5:\n",
        "    update_product_rating(positive=True)\n",
        "```\n",
        "\n",
        "#### 2) ì†Œì…œ ë¯¸ë””ì–´ ëª¨ë‹ˆí„°ë§\n",
        "```python\n",
        "# ë¸Œëœë“œ ë©˜ì…˜ ê°ì„± ë¶„ì„\n",
        "brand_mentions = collect_social_posts(\"brand_name\")\n",
        "sentiment_scores = model.predict(brand_mentions)\n",
        "generate_sentiment_report(sentiment_scores)\n",
        "```\n",
        "\n",
        "#### 3) ê³ ê° ì„œë¹„ìŠ¤\n",
        "```python\n",
        "# ë¬¸ì˜ ë‚´ìš© ìš°ì„ ìˆœìœ„ ë¶„ë¥˜\n",
        "inquiry_sentiment = model.predict(customer_message)\n",
        "if inquiry_sentiment < 0.3:  # ë§¤ìš° ë¶€ì •ì \n",
        "    escalate_to_manager(inquiry_id)\n",
        "```\n",
        "\n",
        "### ğŸš€ í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜\n",
        "\n",
        "#### í”„ë¡œë•ì…˜ ë°°í¬ êµ¬ì¡°\n",
        "```\n",
        "ì‚¬ìš©ì ì…ë ¥ â†’ API Gateway â†’ \n",
        "ì „ì²˜ë¦¬ ì„œë²„ â†’ ML ëª¨ë¸ ì„œë²„ â†’ \n",
        "ê²°ê³¼ ìºì‹± â†’ í´ë¼ì´ì–¸íŠ¸ ì‘ë‹µ\n",
        "```\n",
        "\n",
        "#### ëª¨ë¸ ì—…ë°ì´íŠ¸ ì „ëµ\n",
        "```\n",
        "1. ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘\n",
        "2. ìë™ ì¬í›ˆë ¨ íŒŒì´í”„ë¼ì¸\n",
        "3. A/B í…ŒìŠ¤íŠ¸ë¡œ ì„±ëŠ¥ ê²€ì¦  \n",
        "4. ì ì§„ì  ëª¨ë¸ ë°°í¬\n",
        "```\n",
        "\n",
        "## 5.4 ê°œì„  ë°©í–¥ ë° ì°¨ì„¸ëŒ€ ê¸°ë²•\n",
        "\n",
        "### ğŸ“ˆ ë‹¨ê³„ì  ì„±ëŠ¥ ê°œì„  ë¡œë“œë§µ\n",
        "\n",
        "#### Phase 1: ê¸°ë³¸ ì„±ëŠ¥ í–¥ìƒ (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)\n",
        "- **TF-IDF ë²¡í„°í™”**: ë‹¨ì–´ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ì ìš©\n",
        "- **N-gram í™•ì¥**: ë‹¨ì–´ ì¡°í•© íŒ¨í„´ ì¸ì‹\n",
        "- **ì •ê·œí™” ê°•í™”**: Dropout, Batch Normalization\n",
        "- **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ìµœì í™”\n",
        "\n",
        "#### Phase 2: ê³ ê¸‰ ëª¨ë¸ ë„ì… (ì¤‘ê¸° ëª©í‘œ)\n",
        "- **ì„ë² ë”© ë ˆì´ì–´**: ë°€ì§‘ ë²¡í„° í‘œí˜„ í•™ìŠµ\n",
        "- **ìˆœí™˜ ì‹ ê²½ë§ (RNN/LSTM)**: ìˆœì„œ ì •ë³´ í™œìš©\n",
        "- **í•©ì„±ê³± ì‹ ê²½ë§ (CNN)**: ì§€ì—­ì  íŒ¨í„´ ì¸ì‹\n",
        "- **ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**: ì¤‘ìš” ë‹¨ì–´ ì§‘ì¤‘\n",
        "\n",
        "#### Phase 3: ìµœì‹  ê¸°ë²• ì ìš© (ì¥ê¸° ëª©í‘œ)\n",
        "- **ì‚¬ì „ í›ˆë ¨ ëª¨ë¸**: BERT, RoBERTa, GPT\n",
        "- **ì „ì´ í•™ìŠµ**: ë„ë©”ì¸ íŠ¹í™” íŒŒì¸íŠœë‹\n",
        "- **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ë©”íƒ€ë°ì´í„°\n",
        "- **ì„¤ëª… ê°€ëŠ¥í•œ AI**: ëª¨ë¸ ê²°ì • ê·¼ê±° ì œì‹œ\n",
        "\n",
        "### ğŸ”¬ ì‹¤í—˜ì  ì ‘ê·¼ë²•\n",
        "\n",
        "#### ë°ì´í„° ì¦ê°• ê¸°ë²•\n",
        "```python\n",
        "# ë™ì˜ì–´ ì¹˜í™˜ìœ¼ë¡œ ë°ì´í„° í™•ì¥\n",
        "augmented_texts = synonym_replacement(original_texts)\n",
        "\n",
        "# ë°±ë²ˆì—­ìœ¼ë¡œ ë‹¤ì–‘ì„± ì¦ê°€  \n",
        "backtranslated = translate(texts, 'en->ko->en')\n",
        "```\n",
        "\n",
        "#### ì•™ìƒë¸” í•™ìŠµ\n",
        "```python\n",
        "# ë‹¤ì–‘í•œ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
        "models = [bow_model, tfidf_model, embedding_model]\n",
        "ensemble_prediction = weighted_average(models, weights)\n",
        "```\n",
        "\n",
        "## 5.5 í•™ìŠµ ì„±ê³¼ ë° ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "### ğŸ“ íšë“í•œ í•µì‹¬ ì—­ëŸ‰\n",
        "1. **ìì—°ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**: í…ìŠ¤íŠ¸ ë°ì´í„° â†’ ì˜ˆì¸¡ ê²°ê³¼\n",
        "2. **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ I/O ë° ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
        "3. **ì‹¤ë¬´ ì½”ë”© ìŠ¤í‚¬**: ì˜¤ë¥˜ ì²˜ë¦¬, ë¡œê¹…, ëª¨ë‹ˆí„°ë§\n",
        "4. **ëª¨ë¸ í‰ê°€ ë° ê°œì„ **: ì„±ëŠ¥ ì¸¡ì • ë° ìµœì í™” ì „ëµ\n",
        "\n",
        "### ğŸš¦ ë‹¤ìŒ í•™ìŠµ ê¶Œì¥ ì‚¬í•­\n",
        "\n",
        "#### ì‹¬í™” í•™ìŠµ ì£¼ì œ\n",
        "1. **ê³ ê¸‰ NLP ê¸°ë²•**\n",
        "   - Transformer ì•„í‚¤í…ì²˜ ì´í•´\n",
        "   - BERT, GPT í™œìš©ë²•\n",
        "   - í•œêµ­ì–´ NLP íŠ¹í™” ê¸°ë²•\n",
        "\n",
        "2. **MLOps ë° ë°°í¬**\n",
        "   - Docker ì»¨í…Œì´ë„ˆí™”\n",
        "   - Kubernetes ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜\n",
        "   - CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
        "\n",
        "3. **ì„±ëŠ¥ ìµœì í™”**\n",
        "   - ëª¨ë¸ ì••ì¶• ë° ì–‘ìí™”\n",
        "   - ì¶”ë¡  ì†ë„ ìµœì í™”\n",
        "   - ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ\n",
        "\n",
        "### ğŸ¯ ì‹¤ìŠµ í”„ë¡œì íŠ¸ ì œì•ˆ\n",
        "1. **ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì ìš©**: í•œêµ­ì–´ ë¦¬ë·°, ë‰´ìŠ¤ ë¶„ë¥˜\n",
        "2. **ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ êµ¬ì¶•**: ì›¹ ì¸í„°í˜ì´ìŠ¤ + API ì„œë²„\n",
        "3. **ë¹„êµ ì—°êµ¬**: ë‹¤ì–‘í•œ ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹\n",
        "4. **ë„ë©”ì¸ íŠ¹í™”**: ì˜ë£Œ, ê¸ˆìœµ, ë²•ë¥  í…ìŠ¤íŠ¸ ë¶„ì„\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š ì¶”ì²œ ìë£Œ ë° ì°¸ê³  ë¬¸í—Œ\n",
        "\n",
        "### í•µì‹¬ ë…¼ë¬¸\n",
        "- Maas et al. (2011): \"Learning Word Vectors for Sentiment Analysis\"\n",
        "- Mikolov et al. (2013): \"Efficient Estimation of Word Representations\"\n",
        "- Vaswani et al. (2017): \"Attention Is All You Need\"\n",
        "\n",
        "### ì‹¤ë¬´ ìë£Œ\n",
        "- TensorFlow Text Guide: https://www.tensorflow.org/text\n",
        "- Hugging Face Transformers: https://huggingface.co/transformers\n",
        "- Papers With Code NLP: https://paperswithcode.com/area/natural-language-processing\n",
        "\n",
        "### í•œêµ­ì–´ NLP ìë£Œ\n",
        "- KoBERT, KoGPT: í•œêµ­ì–´ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸\n",
        "- í•œêµ­ì–´ Embedding: FastText, Word2Vec\n",
        "- í˜•íƒœì†Œ ë¶„ì„ê¸°: KoNLPy, Mecab\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ì™„ì „í•œ NLP ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤!**\n",
        "\n",
        "ì´ì œ ì—¬ëŸ¬ë¶„ì€ ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ ì—­ëŸ‰ì„ ê°–ì¶”ì—ˆìŠµë‹ˆë‹¤. \n",
        "ê³„ì†í•´ì„œ ìƒˆë¡œìš´ ê¸°ë²•ì„ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³´ì„¸ìš”! ğŸš€\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
