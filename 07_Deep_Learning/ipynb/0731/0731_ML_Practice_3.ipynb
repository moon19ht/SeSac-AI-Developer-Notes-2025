{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# IMDB 영화 리뷰 감성 분석 - 완전 가이드\n",
        "\n",
        "## 📚 학습 목표\n",
        "1. **자연어 처리(NLP) 기초 이해**: 텍스트 데이터를 머신러닝에 활용하는 방법 학습\n",
        "2. **실제 데이터 파이프라인 구축**: 대용량 텍스트 데이터 전처리부터 모델 배포까지\n",
        "3. **감성 분석 모델 개발**: 이진 분류를 통한 감정 예측 시스템 구축\n",
        "4. **성능 최적화 기법**: 데이터 캐싱, 벡터화, 모델 튜닝 기법 적용\n",
        "\n",
        "## 🎯 프로젝트 개요\n",
        "- **데이터셋**: Stanford AI Lab의 IMDB 영화 리뷰 (50,000개 리뷰)\n",
        "- **문제 유형**: 이진 분류 (긍정/부정 감성 분석)\n",
        "- **기술 스택**: TensorFlow/Keras, TextVectorization, Dense Neural Network\n",
        "- **벡터화 방식**: Multi-hot encoding (Bag of Words)\n",
        "\n",
        "## 📋 노트북 구성\n",
        "```\n",
        "Part I   : 이론 및 배경 지식\n",
        "Part II  : 환경 설정 및 데이터 준비  \n",
        "Part III : 텍스트 벡터화 및 전처리\n",
        "Part IV  : 모델 구축 및 훈련\n",
        "Part V   : 성능 평가 및 개선 방안\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part I: 이론 및 배경 지식\n",
        "\n",
        "## 1.1 감성 분석이란?\n",
        "\n",
        "**감성 분석(Sentiment Analysis)**은 텍스트에 담긴 감정, 의견, 태도를 자동으로 분석하는 자연어 처리 기법입니다.\n",
        "\n",
        "### 🔍 주요 응용 분야\n",
        "- **소셜 미디어 모니터링**: 브랜드에 대한 대중의 반응 분석\n",
        "- **제품 리뷰 분석**: 고객 만족도 및 피드백 자동 분류\n",
        "- **주식 시장 예측**: 뉴스 기사의 감성을 통한 시장 동향 예측\n",
        "- **고객 서비스**: 문의 내용의 긴급도 및 감정 상태 파악\n",
        "\n",
        "### 📊 분류 유형\n",
        "1. **이진 분류**: 긍정/부정 (본 프로젝트)\n",
        "2. **다중 분류**: 매우 긍정/긍정/중립/부정/매우 부정\n",
        "3. **세밀한 감정 분류**: 기쁨, 슬픔, 분노, 두려움 등\n",
        "\n",
        "## 1.2 IMDB 데이터셋 소개\n",
        "\n",
        "### 📈 데이터셋 특징\n",
        "- **출처**: Stanford AI Lab (Andrew Maas et al., 2011)\n",
        "- **규모**: 총 50,000개 영화 리뷰\n",
        "- **구성**: \n",
        "  - 훈련용: 25,000개 (긍정 12,500 + 부정 12,500)\n",
        "  - 테스트용: 25,000개 (긍정 12,500 + 부정 12,500)\n",
        "- **레이블**: 이진 분류 (0: 부정, 1: 긍정)\n",
        "- **크기**: 압축 파일 ~80MB, 압축 해제 후 ~280MB\n",
        "\n",
        "### 🎭 데이터 품질\n",
        "- **균형 잡힌 데이터**: 긍정/부정 비율이 50:50으로 균등\n",
        "- **실제 사용자 리뷰**: IMDb 웹사이트의 실제 사용자 작성 리뷰\n",
        "- **품질 필터링**: 극단적 점수(1-4점: 부정, 7-10점: 긍정)만 포함\n",
        "\n",
        "## 1.3 텍스트 벡터화 이론\n",
        "\n",
        "### 🧮 왜 벡터화가 필요한가?\n",
        "머신러닝 모델은 **숫자**만 처리할 수 있습니다. 따라서 텍스트를 숫자로 변환하는 과정이 필수입니다.\n",
        "\n",
        "### 🎯 주요 벡터화 기법\n",
        "\n",
        "#### 1) Bag of Words (BoW) - 본 프로젝트에서 사용\n",
        "```\n",
        "\"I love this movie\" → [0, 1, 1, 0, 1, 0, ...]\n",
        "```\n",
        "- **장점**: 구현 간단, 해석 용이\n",
        "- **단점**: 단어 순서 무시, 희소 벡터\n",
        "\n",
        "#### 2) TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "```\n",
        "TF-IDF = TF(단어 빈도) × IDF(역문서 빈도)\n",
        "```\n",
        "- **장점**: 중요한 단어에 가중치 부여\n",
        "- **단점**: 여전히 순서 정보 없음\n",
        "\n",
        "#### 3) Word Embeddings (Word2Vec, GloVe)\n",
        "```\n",
        "\"king\" - \"man\" + \"woman\" ≈ \"queen\"\n",
        "```\n",
        "- **장점**: 의미론적 관계 표현\n",
        "- **단점**: 사전 훈련 필요\n",
        "\n",
        "#### 4) Transformer 기반 (BERT, GPT)\n",
        "- **장점**: 문맥 고려, 최고 성능\n",
        "- **단점**: 계산 비용 높음\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 환경 설정\n",
            "==================================================\n",
            "TensorFlow 버전: 2.15.1\n",
            "Keras 버전: 2.15.0\n",
            "Python 실행 시간: 2025-07-31 15:59:09\n",
            "⚠️ CPU만 사용 (GPU 없음)\n",
            "🎲 랜덤 시드 설정: 1337\n",
            "\n",
            "✅ 환경 설정 완료!\n"
          ]
        }
      ],
      "source": [
        "# Part II: 환경 설정 및 데이터 준비\n",
        "\n",
        "# 필수 라이브러리 임포트\n",
        "import requests          # HTTP 요청 (데이터 다운로드)\n",
        "import subprocess        # 시스템 명령어 실행 (압축 해제)\n",
        "import re, string        # 정규표현식, 문자열 처리\n",
        "import os, pathlib       # 파일/디렉토리 관리\n",
        "import shutil, random    # 파일 이동, 랜덤 처리\n",
        "\n",
        "# TensorFlow & Keras (딥러닝 프레임워크)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import keras\n",
        "from keras import layers, models\n",
        "\n",
        "# 추가 유틸리티\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# TensorFlow 버전 확인 및 설정\n",
        "print(\"🔧 환경 설정\")\n",
        "print(\"=\"*50)\n",
        "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
        "print(f\"Keras 버전: {keras.__version__}\")\n",
        "print(f\"Python 실행 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# GPU 사용 가능 여부 확인\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"✅ GPU 사용 가능\")\n",
        "    print(f\"GPU 장치: {tf.config.list_physical_devices('GPU')}\")\n",
        "else:\n",
        "    print(\"⚠️ CPU만 사용 (GPU 없음)\")\n",
        "\n",
        "# 재현 가능한 결과를 위한 시드 설정\n",
        "RANDOM_SEED = 1337\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "print(f\"🎲 랜덤 시드 설정: {RANDOM_SEED}\")\n",
        "\n",
        "print(\"\\n✅ 환경 설정 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2.1 데이터 다운로드 시스템\n",
        "\n",
        "### 🎯 다운로드 전략\n",
        "- **스트리밍 방식**: 메모리 효율적인 대용량 파일 처리\n",
        "- **진행률 표시**: 사용자 경험 개선\n",
        "- **오류 처리**: 네트워크 문제 대응\n",
        "- **중복 다운로드 방지**: 기존 파일 존재 시 건너뛰기\n",
        "\n",
        "### 📊 파일 정보\n",
        "- **URL**: `https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz`\n",
        "- **크기**: 약 80MB (압축), 280MB (압축 해제)\n",
        "- **형식**: tar.gz (Linux 표준 압축 형식)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_imdb_dataset():\n",
        "    \"\"\"\n",
        "    IMDB 데이터셋을 안전하고 효율적으로 다운로드하는 함수\n",
        "    \n",
        "    Features:\n",
        "    - 스트리밍 다운로드로 메모리 효율성 확보\n",
        "    - 진행률 표시 및 속도 측정\n",
        "    - 기존 파일 검증 및 중복 다운로드 방지\n",
        "    - 네트워크 오류 처리\n",
        "    \n",
        "    Returns:\n",
        "        bool: 다운로드 성공 여부\n",
        "    \"\"\"\n",
        "    \n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    file_name = \"../../data/aclImdb_v1.tar.gz\"\n",
        "    \n",
        "    # 디렉토리 생성\n",
        "    os.makedirs(\"../../data\", exist_ok=True)\n",
        "    \n",
        "    # 기존 파일 확인\n",
        "    if os.path.exists(file_name):\n",
        "        file_size = os.path.getsize(file_name) / (1024 * 1024)  # MB 단위\n",
        "        print(f\"✅ 파일이 이미 존재합니다: {file_name}\")\n",
        "        print(f\"📊 파일 크기: {file_size:.1f}MB\")\n",
        "        return True\n",
        "    \n",
        "    print(f\"📥 데이터셋 다운로드 시작...\")\n",
        "    print(f\"🔗 URL: {url}\")\n",
        "    print(f\"📁 저장 위치: {file_name}\")\n",
        "    \n",
        "    try:\n",
        "        # HTTP 요청 시작\n",
        "        response = requests.get(url, stream=True, timeout=30)\n",
        "        response.raise_for_status()  # HTTP 에러 확인\n",
        "        \n",
        "        # 파일 크기 확인\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        downloaded = 0\n",
        "        chunk_size = 8192  # 8KB\n",
        "        \n",
        "        print(f\"📊 총 파일 크기: {total_size / (1024*1024):.1f}MB\")\n",
        "        \n",
        "        # 파일 다운로드\n",
        "        with open(file_name, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                if chunk:  # 빈 청크 필터링\n",
        "                    file.write(chunk)\n",
        "                    downloaded += len(chunk)\n",
        "                    \n",
        "                    # 진행률 표시 (매 1MB마다)\n",
        "                    if downloaded % (1024 * 1024) < chunk_size:\n",
        "                        progress = (downloaded / total_size) * 100 if total_size else 0\n",
        "                        print(f\"⏬ 다운로드 진행률: {progress:.1f}% ({downloaded/(1024*1024):.1f}MB)\")\n",
        "        \n",
        "        print(f\"✅ 다운로드 완료!\")\n",
        "        print(f\"📁 저장된 파일: {file_name}\")\n",
        "        return True\n",
        "        \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ 네트워크 오류: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 다운로드 중 오류 발생: {e}\")\n",
        "        return False\n",
        "\n",
        "# 다운로드 실행 (필요시 주석 해제)\n",
        "# success = download_imdb_dataset()\n",
        "# if success:\n",
        "#     print(\"🎉 데이터 준비 완료!\")\n",
        "# else:\n",
        "#     print(\"💥 데이터 다운로드 실패!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2.2 압축 해제 및 파일 구조 분석\n",
        "\n",
        "### 🗂️ tar.gz 형식 이해\n",
        "- **tar**: 여러 파일을 하나로 묶는 아카이브 형식\n",
        "- **gz**: gzip 압축 알고리즘 적용\n",
        "- **Linux/Unix 표준**: 소스 코드 배포에 널리 사용\n",
        "\n",
        "### 📁 예상 디렉토리 구조\n",
        "```\n",
        "aclImdb/\n",
        "├── train/\n",
        "│   ├── pos/          # 긍정 리뷰 (12,500개)\n",
        "│   ├── neg/          # 부정 리뷰 (12,500개)\n",
        "│   └── unsup/        # 무레이블 데이터 (제거 예정)\n",
        "├── test/\n",
        "│   ├── pos/          # 긍정 리뷰 (12,500개)\n",
        "│   └── neg/          # 부정 리뷰 (12,500개)\n",
        "└── README           # 데이터셋 설명\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_imdb_dataset():\n",
        "    \"\"\"\n",
        "    IMDB 데이터셋 압축 파일을 안전하게 해제하는 함수\n",
        "    \n",
        "    Features:\n",
        "    - 크로스 플랫폼 지원 (Python tarfile 모듈 사용)\n",
        "    - 진행률 표시 및 상태 모니터링\n",
        "    - 기존 디렉토리 검증\n",
        "    - 파일 구조 검증\n",
        "    \n",
        "    Returns:\n",
        "        bool: 압축 해제 성공 여부\n",
        "    \"\"\"\n",
        "    \n",
        "    import tarfile\n",
        "    \n",
        "    archive_path = \"../../data/aclImdb_v1.tar.gz\"\n",
        "    extract_path = \"../../data/\"\n",
        "    target_dir = \"../../data/aclImdb\"\n",
        "    \n",
        "    # 이미 해제된 디렉토리 확인\n",
        "    if os.path.exists(target_dir):\n",
        "        print(f\"✅ 이미 압축 해제된 디렉토리가 존재합니다: {target_dir}\")\n",
        "        \n",
        "        # 디렉토리 구조 검증\n",
        "        required_dirs = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
        "        missing_dirs = []\n",
        "        \n",
        "        for dir_path in required_dirs:\n",
        "            full_path = os.path.join(target_dir, dir_path)\n",
        "            if not os.path.exists(full_path):\n",
        "                missing_dirs.append(dir_path)\n",
        "        \n",
        "        if missing_dirs:\n",
        "            print(f\"⚠️ 누락된 디렉토리: {missing_dirs}\")\n",
        "            print(\"🔄 재해제를 진행합니다...\")\n",
        "        else:\n",
        "            print(\"✅ 디렉토리 구조 검증 완료!\")\n",
        "            return True\n",
        "    \n",
        "    # 압축 파일 존재 확인\n",
        "    if not os.path.exists(archive_path):\n",
        "        print(f\"❌ 압축 파일을 찾을 수 없습니다: {archive_path}\")\n",
        "        print(\"💡 먼저 다운로드를 실행해주세요.\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"📦 압축 해제 시작...\")\n",
        "    print(f\"🗂️ 압축 파일: {archive_path}\")\n",
        "    print(f\"📁 해제 위치: {extract_path}\")\n",
        "    \n",
        "    try:\n",
        "        # tar 파일 열기 및 검증\n",
        "        with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "            # 파일 목록 확인\n",
        "            members = tar.getmembers()\n",
        "            total_files = len(members)\n",
        "            \n",
        "            print(f\"📊 총 파일 수: {total_files:,}개\")\n",
        "            print(f\"⏳ 압축 해제 중... (시간이 소요될 수 있습니다)\")\n",
        "            \n",
        "            # 안전한 압축 해제 (경로 검증)\n",
        "            def safe_extract(tarinfo, path):\n",
        "                # 경로 traversal 공격 방지\n",
        "                if os.path.isabs(tarinfo.name) or \"..\" in tarinfo.name:\n",
        "                    print(f\"⚠️ 안전하지 않은 경로 건너뛰기: {tarinfo.name}\")\n",
        "                    return None\n",
        "                return tarinfo\n",
        "            \n",
        "            # 진행률 표시와 함께 해제\n",
        "            extracted_count = 0\n",
        "            for member in members:\n",
        "                if safe_extract(member, extract_path):\n",
        "                    tar.extract(member, extract_path)\n",
        "                    extracted_count += 1\n",
        "                    \n",
        "                    # 진행률 표시 (매 1000개 파일마다)\n",
        "                    if extracted_count % 1000 == 0:\n",
        "                        progress = (extracted_count / total_files) * 100\n",
        "                        print(f\"⏬ 압축 해제 진행률: {progress:.1f}% ({extracted_count:,}/{total_files:,})\")\n",
        "        \n",
        "        print(f\"✅ 압축 해제 완료!\")\n",
        "        \n",
        "        # 해제 결과 검증\n",
        "        if os.path.exists(target_dir):\n",
        "            print(f\"📁 생성된 디렉토리: {target_dir}\")\n",
        "            \n",
        "            # 각 하위 디렉토리의 파일 수 확인\n",
        "            subdirs = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
        "            for subdir in subdirs:\n",
        "                subdir_path = os.path.join(target_dir, subdir)\n",
        "                if os.path.exists(subdir_path):\n",
        "                    file_count = len([f for f in os.listdir(subdir_path) if f.endswith('.txt')])\n",
        "                    print(f\"  📂 {subdir}: {file_count:,}개 파일\")\n",
        "            \n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ 대상 디렉토리가 생성되지 않았습니다.\")\n",
        "            return False\n",
        "            \n",
        "    except tarfile.TarError as e:\n",
        "        print(f\"❌ tar 파일 오류: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 압축 해제 중 오류 발생: {e}\")\n",
        "        return False\n",
        "\n",
        "# 압축 해제 실행 (필요시 주석 해제)\n",
        "# success = extract_imdb_dataset()\n",
        "# if success:\n",
        "#     print(\"🎉 데이터 압축 해제 완료!\")\n",
        "# else:\n",
        "#     print(\"💥 압축 해제 실패!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2.3 데이터 분할 전략\n",
        "\n",
        "### 🎯 분할의 중요성\n",
        "머신러닝에서 데이터 분할은 모델의 신뢰성 있는 평가를 위해 필수적입니다.\n",
        "\n",
        "### 📊 분할 전략\n",
        "```\n",
        "원본 데이터: train(25,000) + test(25,000)\n",
        "        ↓\n",
        "최종 구조: train(20,000) + val(5,000) + test(25,000)\n",
        "```\n",
        "\n",
        "### 🔄 분할 과정\n",
        "1. **Train 데이터 분할**: 25,000개 → 20,000개(훈련) + 5,000개(검증)\n",
        "2. **무작위 샘플링**: 편향 방지를 위한 랜덤 셔플\n",
        "3. **클래스 균형 유지**: 긍정/부정 비율 50:50 유지\n",
        "4. **Unsupervised 데이터 제거**: 레이블이 없는 데이터 삭제\n",
        "\n",
        "### 💡 분할 비율 선택 이유\n",
        "- **Train 80%**: 충분한 학습 데이터 확보\n",
        "- **Validation 20%**: 신뢰할 만한 성능 추정\n",
        "- **Test**: 완전 독립적 최종 평가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_validation_split(validation_ratio=0.2, random_seed=1337):\n",
        "    \"\"\"\n",
        "    훈련 데이터를 train/validation으로 분할하는 함수\n",
        "    \n",
        "    Args:\n",
        "        validation_ratio (float): 검증 데이터 비율 (기본: 20%)\n",
        "        random_seed (int): 재현 가능한 결과를 위한 시드\n",
        "    \n",
        "    Features:\n",
        "    - 클래스별 균등 분할 (긍정/부정 비율 유지)\n",
        "    - 기존 분할 검증 및 건너뛰기\n",
        "    - 상세한 분할 통계 제공\n",
        "    - 무레이블 데이터 자동 제거\n",
        "    \n",
        "    Returns:\n",
        "        bool: 분할 성공 여부\n",
        "    \"\"\"\n",
        "    \n",
        "    base_dir = pathlib.Path(\"../../data/aclImdb\")\n",
        "    train_dir = base_dir / \"train\"\n",
        "    val_dir = base_dir / \"val\"\n",
        "    \n",
        "    # 기본 디렉토리 존재 확인\n",
        "    if not base_dir.exists():\n",
        "        print(f\"❌ 데이터셋 디렉토리가 존재하지 않습니다: {base_dir}\")\n",
        "        print(\"💡 먼저 데이터 다운로드 및 압축 해제를 실행해주세요.\")\n",
        "        return False\n",
        "    \n",
        "    # 이미 분할이 완료된 경우 확인\n",
        "    if val_dir.exists():\n",
        "        print(f\"✅ Validation 디렉토리가 이미 존재합니다: {val_dir}\")\n",
        "        \n",
        "        # 분할 상태 검증\n",
        "        categories = ['pos', 'neg']\n",
        "        split_valid = True\n",
        "        \n",
        "        for category in categories:\n",
        "            train_count = len(list((train_dir / category).glob('*.txt'))) if (train_dir / category).exists() else 0\n",
        "            val_count = len(list((val_dir / category).glob('*.txt'))) if (val_dir / category).exists() else 0\n",
        "            \n",
        "            print(f\"  📂 {category}: train({train_count:,}) + val({val_count:,}) = {train_count + val_count:,}\")\n",
        "            \n",
        "            if train_count == 0 or val_count == 0:\n",
        "                split_valid = False\n",
        "        \n",
        "        if split_valid:\n",
        "            print(\"✅ 데이터 분할이 이미 완료되어 있습니다!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"⚠️ 분할 상태가 불완전합니다. 재분할을 진행합니다...\")\n",
        "    \n",
        "    print(f\"🔄 데이터 분할 시작\")\n",
        "    print(f\"📊 분할 비율: Train {(1-validation_ratio)*100:.0f}% | Validation {validation_ratio*100:.0f}%\")\n",
        "    print(f\"🎲 랜덤 시드: {random_seed}\")\n",
        "    \n",
        "    try:\n",
        "        # Validation 디렉토리 생성\n",
        "        val_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # 무레이블 데이터 제거\n",
        "        unsup_dir = train_dir / \"unsup\"\n",
        "        if unsup_dir.exists():\n",
        "            print(f\"🗑️ 무레이블 데이터 제거 중: {unsup_dir}\")\n",
        "            shutil.rmtree(unsup_dir)\n",
        "            print(\"✅ 무레이블 데이터 제거 완료\")\n",
        "        \n",
        "        total_moved = 0\n",
        "        \n",
        "        # 각 카테고리별 분할 처리\n",
        "        for category in [\"neg\", \"pos\"]:\n",
        "            print(f\"\\n📂 {category.upper()} 카테고리 처리 중...\")\n",
        "            \n",
        "            # 디렉토리 생성\n",
        "            train_category_dir = train_dir / category\n",
        "            val_category_dir = val_dir / category\n",
        "            val_category_dir.mkdir(exist_ok=True)\n",
        "            \n",
        "            # 파일 목록 가져오기\n",
        "            if not train_category_dir.exists():\n",
        "                print(f\"⚠️ 카테고리 디렉토리가 존재하지 않습니다: {train_category_dir}\")\n",
        "                continue\n",
        "                \n",
        "            files = [f for f in train_category_dir.glob('*.txt')]\n",
        "            original_count = len(files)\n",
        "            \n",
        "            print(f\"  📊 원본 파일 수: {original_count:,}개\")\n",
        "            \n",
        "            # 파일 목록 랜덤 셔플\n",
        "            rng = random.Random(random_seed)\n",
        "            rng.shuffle(files)\n",
        "            \n",
        "            # Validation용 파일 수 계산\n",
        "            num_val_samples = int(validation_ratio * len(files))\n",
        "            val_files = files[:num_val_samples]  # 처음 20%를 validation으로\n",
        "            \n",
        "            print(f\"  ⬇️ Validation으로 이동: {num_val_samples:,}개\")\n",
        "            print(f\"  📋 Train에 유지: {len(files) - num_val_samples:,}개\")\n",
        "            \n",
        "            # 파일 이동\n",
        "            moved_count = 0\n",
        "            failed_count = 0\n",
        "            \n",
        "            for file_path in val_files:\n",
        "                try:\n",
        "                    dest_path = val_category_dir / file_path.name\n",
        "                    shutil.move(str(file_path), str(dest_path))\n",
        "                    moved_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ⚠️ 파일 이동 실패 {file_path.name}: {e}\")\n",
        "                    failed_count += 1\n",
        "            \n",
        "            print(f\"  ✅ 성공적으로 이동: {moved_count:,}개\")\n",
        "            if failed_count > 0:\n",
        "                print(f\"  ❌ 이동 실패: {failed_count:,}개\")\n",
        "            \n",
        "            total_moved += moved_count\n",
        "        \n",
        "        print(f\"\\n🎉 데이터 분할 완료!\")\n",
        "        print(f\"📊 총 이동된 파일: {total_moved:,}개\")\n",
        "        \n",
        "        # 최종 분할 결과 확인\n",
        "        print(f\"\\n📈 최종 분할 결과:\")\n",
        "        print(\"=\" * 40)\n",
        "        \n",
        "        for split_name in ['train', 'val']:\n",
        "            split_dir = base_dir / split_name\n",
        "            if split_dir.exists():\n",
        "                pos_count = len(list((split_dir / 'pos').glob('*.txt')))\n",
        "                neg_count = len(list((split_dir / 'neg').glob('*.txt')))\n",
        "                total_count = pos_count + neg_count\n",
        "                \n",
        "                print(f\"{split_name.upper():>5}: 긍정 {pos_count:,} + 부정 {neg_count:,} = 총 {total_count:,}개\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 데이터 분할 중 오류 발생: {e}\")\n",
        "        return False\n",
        "\n",
        "# 데이터 분할 실행 (필요시 주석 해제)\n",
        "# success = create_validation_split()\n",
        "# if success:\n",
        "#     print(\"🎉 데이터 분할 완료!\")\n",
        "# else:\n",
        "#     print(\"💥 데이터 분할 실패!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part III: 텍스트 벡터화 및 전처리\n",
        "\n",
        "## 3.1 데이터셋 로딩 시스템\n",
        "\n",
        "### 🎯 Keras Dataset API 활용\n",
        "`text_dataset_from_directory`는 디렉토리 구조에서 자동으로 텍스트 분류 데이터셋을 생성하는 강력한 도구입니다.\n",
        "\n",
        "### 📁 예상 디렉토리 구조\n",
        "```\n",
        "../../data/aclImdb/\n",
        "├── train/\n",
        "│   ├── pos/        # 긍정 리뷰 → 라벨 1\n",
        "│   └── neg/        # 부정 리뷰 → 라벨 0\n",
        "├── val/\n",
        "│   ├── pos/        # 긍정 리뷰 → 라벨 1  \n",
        "│   └── neg/        # 부정 리뷰 → 라벨 0\n",
        "└── test/\n",
        "    ├── pos/        # 긍정 리뷰 → 라벨 1\n",
        "    └── neg/        # 부정 리뷰 → 라벨 0\n",
        "```\n",
        "\n",
        "### ⚙️ 주요 설정 파라미터\n",
        "- **batch_size**: 한 번에 처리할 샘플 수 (메모리 vs 학습 속도 트레이드오프)\n",
        "- **shuffle**: 훈련 데이터 셔플링 (과적합 방지)\n",
        "- **validation_split**: 자동 분할 대신 수동 분할 사용\n",
        "- **subset**: 특정 데이터 분할만 로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 IMDB 데이터셋 로딩 시작\n",
            "==================================================\n",
            "📊 배치 크기: 32\n",
            "📁 베이스 디렉토리: ../../data/aclImdb\n",
            "\n",
            "🔄 훈련 데이터셋 로딩 중...\n",
            "Found 20000 files belonging to 2 classes.\n",
            "🔄 검증 데이터셋 로딩 중...\n",
            "Found 5000 files belonging to 2 classes.\n",
            "🔄 테스트 데이터셋 로딩 중...\n",
            "Found 25000 files belonging to 2 classes.\n",
            "\n",
            "✅ 모든 데이터셋 로딩 완료!\n",
            "\n",
            "📊 데이터셋 정보:\n",
            "------------------------------\n",
            "       TRAIN: ~20,000개 샘플 (625 배치)\n",
            "  VALIDATION: ~5,024개 샘플 (157 배치)\n",
            "        TEST: ~25,024개 샘플 (782 배치)\n",
            "\n",
            "🏷️ 클래스 정보: ['neg', 'pos']\n",
            "   - neg → 라벨 0\n",
            "   - pos → 라벨 1\n",
            "\n",
            "🎉 데이터셋 준비 완료!\n",
            "💾 메모리 사용량 최적화를 위해 배치 크기 32 사용\n"
          ]
        }
      ],
      "source": [
        "def load_imdb_datasets(batch_size=32, max_length=None):\n",
        "    \"\"\"\n",
        "    IMDB 데이터셋을 로드하고 기본 전처리를 수행하는 함수\n",
        "    \n",
        "    Args:\n",
        "        batch_size (int): 배치 크기 (기본: 32)\n",
        "        max_length (int): 최대 시퀀스 길이 (기본: None)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (train_ds, val_ds, test_ds) 또는 None\n",
        "    \"\"\"\n",
        "    \n",
        "    base_dir = \"../../data/aclImdb\"\n",
        "    \n",
        "    print(\"📂 IMDB 데이터셋 로딩 시작\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"📊 배치 크기: {batch_size}\")\n",
        "    print(f\"📁 베이스 디렉토리: {base_dir}\")\n",
        "    \n",
        "    # 데이터 디렉토리 존재 확인\n",
        "    directories = {\n",
        "        'train': f\"{base_dir}/train\",\n",
        "        'val': f\"{base_dir}/val\", \n",
        "        'test': f\"{base_dir}/test\"\n",
        "    }\n",
        "    \n",
        "    for name, path in directories.items():\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"❌ {name} 디렉토리가 존재하지 않습니다: {path}\")\n",
        "            return None, None, None\n",
        "    \n",
        "    try:\n",
        "        # 훈련 데이터셋 로드\n",
        "        print(\"\\n🔄 훈련 데이터셋 로딩 중...\")\n",
        "        train_ds = keras.utils.text_dataset_from_directory(\n",
        "            directories['train'],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,           # 훈련 데이터는 셔플\n",
        "            seed=1337,             # 재현 가능성을 위한 시드\n",
        "            max_length=max_length   # 최대 길이 제한 (옵션)\n",
        "        )\n",
        "        \n",
        "        # 검증 데이터셋 로드  \n",
        "        print(\"🔄 검증 데이터셋 로딩 중...\")\n",
        "        val_ds = keras.utils.text_dataset_from_directory(\n",
        "            directories['val'],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,          # 검증 데이터는 셔플하지 않음\n",
        "            max_length=max_length\n",
        "        )\n",
        "        \n",
        "        # 테스트 데이터셋 로드\n",
        "        print(\"🔄 테스트 데이터셋 로딩 중...\")\n",
        "        test_ds = keras.utils.text_dataset_from_directory(\n",
        "            directories['test'],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,          # 테스트 데이터는 셔플하지 않음\n",
        "            max_length=max_length\n",
        "        )\n",
        "        \n",
        "        print(\"\\n✅ 모든 데이터셋 로딩 완료!\")\n",
        "        \n",
        "        # 데이터셋 정보 확인\n",
        "        print(\"\\n📊 데이터셋 정보:\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        datasets = [\n",
        "            (\"TRAIN\", train_ds),\n",
        "            (\"VALIDATION\", val_ds), \n",
        "            (\"TEST\", test_ds)\n",
        "        ]\n",
        "        \n",
        "        for name, dataset in datasets:\n",
        "            # 배치 수 계산 (근사치)\n",
        "            batch_count = 0\n",
        "            total_samples = 0\n",
        "            \n",
        "            for batch in dataset.take(5):  # 처음 5개 배치만 확인\n",
        "                batch_count += 1\n",
        "                total_samples += len(batch[0])\n",
        "            \n",
        "            # 전체 배치 수 추정\n",
        "            estimated_batches = len(list(dataset))\n",
        "            estimated_samples = estimated_batches * batch_size\n",
        "            \n",
        "            print(f\"  {name:>10}: ~{estimated_samples:,}개 샘플 ({estimated_batches:,} 배치)\")\n",
        "        \n",
        "        # 클래스 정보 확인\n",
        "        class_names = train_ds.class_names\n",
        "        print(f\"\\n🏷️ 클래스 정보: {class_names}\")\n",
        "        print(f\"   - {class_names[0]} → 라벨 0\")\n",
        "        print(f\"   - {class_names[1]} → 라벨 1\")\n",
        "        \n",
        "        return train_ds, val_ds, test_ds\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 데이터셋 로딩 중 오류 발생: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# 데이터셋 로딩 실행\n",
        "BATCH_SIZE = 32\n",
        "train_ds, val_ds, test_ds = load_imdb_datasets(batch_size=BATCH_SIZE)\n",
        "\n",
        "if train_ds is not None:\n",
        "    print(\"\\n🎉 데이터셋 준비 완료!\")\n",
        "    print(f\"💾 메모리 사용량 최적화를 위해 배치 크기 {BATCH_SIZE} 사용\")\n",
        "else:\n",
        "    print(\"\\n💥 데이터셋 로딩 실패!\")\n",
        "    print(\"💡 데이터 다운로드 및 분할을 먼저 실행해주세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3.2 데이터 구조 분석\n",
        "\n",
        "### 🔍 원시 데이터 특성 파악\n",
        "실제 텍스트 데이터의 구조와 특성을 분석하여 전처리 전략을 수립합니다.\n",
        "\n",
        "### 📊 분석 항목\n",
        "- **데이터 타입**: 문자열 텐서 확인\n",
        "- **배치 구조**: (텍스트, 라벨) 쌍 검증  \n",
        "- **텍스트 길이**: 최소/최대/평균 길이 분석\n",
        "- **라벨 분포**: 클래스 불균형 확인\n",
        "- **실제 내용**: 샘플 리뷰 내용 검토\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 TRAIN 데이터 구조 분석\n",
            "==================================================\n",
            "📊 배치 정보:\n",
            "  텍스트 형태: (32,) (배치크기=32)\n",
            "  텍스트 타입: <dtype: 'string'>\n",
            "  라벨 형태: (32,)\n",
            "  라벨 타입: <dtype: 'int32'>\n",
            "\n",
            "🏷️ 현재 배치의 라벨 분포:\n",
            "  라벨 0 (부정): 15개\n",
            "  라벨 1 (긍정): 17개\n",
            "\n",
            "📏 텍스트 길이 분석 (처음 2개 샘플):\n",
            "  샘플 1: 871단어\n",
            "  샘플 2: 140단어\n",
            "\n",
            "📝 샘플 텍스트 내용:\n",
            "--------------------------------------------------\n",
            "\n",
            "🎬 샘플 1: 😞 부정 (라벨: 0)\n",
            "📊 길이: 871단어, 4771자\n",
            "📖 내용: I just got the UK 4-disc special edition of Superman 1 for about $5. The additional stuff includes the 1951 feature Superman and the Mole-Men. So I slapped it into the DVD player last night, and here ...\n",
            "⚠️ HTML 태그 발견 - 전처리 필요\n",
            "\n",
            "🎬 샘플 2: 😞 부정 (라벨: 0)\n",
            "📊 길이: 140단어, 763자\n",
            "📖 내용: There is one good thing in this movie: Lola Glaudini's ass! Sorry to be so blunt but it's the truth. Too bad she didn't do a nude. It would at least have made this mess tolerable. We see another chick...\n",
            "\n",
            "======================================================================\n",
            "💡 데이터 분석 결과 요약\n",
            "======================================================================\n",
            "✅ 데이터 형태: 문자열 텐서 (string tensor)\n",
            "✅ 라벨 인코딩: 0=부정, 1=긍정\n",
            "⚠️ HTML 태그 존재: 전처리에서 제거 필요\n",
            "📏 텍스트 길이: 다양함 (벡터화에서 표준화 필요)\n",
            "🎯 다음 단계: TextVectorization으로 수치화\n"
          ]
        }
      ],
      "source": [
        "def analyze_raw_data(dataset, dataset_name=\"DATASET\", max_samples=3):\n",
        "    \"\"\"\n",
        "    원시 텍스트 데이터의 구조와 특성을 상세 분석하는 함수\n",
        "    \n",
        "    Args:\n",
        "        dataset: 분석할 TensorFlow 데이터셋\n",
        "        dataset_name: 데이터셋 이름 (출력용)\n",
        "        max_samples: 분석할 최대 샘플 수\n",
        "    \"\"\"\n",
        "    \n",
        "    if dataset is None:\n",
        "        print(f\"❌ {dataset_name} 데이터셋이 없습니다.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"🔍 {dataset_name} 데이터 구조 분석\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 첫 번째 배치 가져오기\n",
        "    sample_batch = next(iter(dataset))\n",
        "    texts, labels = sample_batch\n",
        "    \n",
        "    # 기본 구조 정보\n",
        "    print(f\"📊 배치 정보:\")\n",
        "    print(f\"  텍스트 형태: {texts.shape} (배치크기={texts.shape[0]})\")\n",
        "    print(f\"  텍스트 타입: {texts.dtype}\")\n",
        "    print(f\"  라벨 형태: {labels.shape}\")\n",
        "    print(f\"  라벨 타입: {labels.dtype}\")\n",
        "    \n",
        "    # 라벨 분포 확인\n",
        "    unique_labels, _, counts = tf.unique_with_counts(labels)\n",
        "    print(f\"\\n🏷️ 현재 배치의 라벨 분포:\")\n",
        "    for label, count in zip(unique_labels.numpy(), counts.numpy()):\n",
        "        label_name = \"긍정\" if label == 1 else \"부정\"\n",
        "        print(f\"  라벨 {label} ({label_name}): {count}개\")\n",
        "    \n",
        "    # 텍스트 길이 분석\n",
        "    text_lengths = [len(text.numpy().decode('utf-8').split()) for text in texts[:max_samples]]\n",
        "    \n",
        "    print(f\"\\n📏 텍스트 길이 분석 (처음 {max_samples}개 샘플):\")\n",
        "    for i, length in enumerate(text_lengths):\n",
        "        print(f\"  샘플 {i+1}: {length}단어\")\n",
        "    \n",
        "    # 샘플 텍스트 내용 확인\n",
        "    print(f\"\\n📝 샘플 텍스트 내용:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for i in range(min(max_samples, len(texts))):\n",
        "        text_content = texts[i].numpy().decode('utf-8')\n",
        "        label_value = labels[i].numpy()\n",
        "        label_name = \"😊 긍정\" if label_value == 1 else \"😞 부정\"\n",
        "        \n",
        "        # 텍스트 길이와 요약\n",
        "        words = text_content.split()\n",
        "        word_count = len(words)\n",
        "        \n",
        "        print(f\"\\n🎬 샘플 {i+1}: {label_name} (라벨: {label_value})\")\n",
        "        print(f\"📊 길이: {word_count}단어, {len(text_content)}자\")\n",
        "        \n",
        "        # 텍스트 미리보기 (처음 200자)\n",
        "        preview = text_content[:200]\n",
        "        if len(text_content) > 200:\n",
        "            preview += \"...\"\n",
        "        \n",
        "        print(f\"📖 내용: {preview}\")\n",
        "        \n",
        "        # HTML 태그 확인\n",
        "        if '<br />' in text_content or '<' in text_content:\n",
        "            print(\"⚠️ HTML 태그 발견 - 전처리 필요\")\n",
        "\n",
        "# 훈련 데이터 분석\n",
        "if train_ds is not None:\n",
        "    analyze_raw_data(train_ds, \"TRAIN\", max_samples=2)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"💡 데이터 분석 결과 요약\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"✅ 데이터 형태: 문자열 텐서 (string tensor)\")\n",
        "    print(\"✅ 라벨 인코딩: 0=부정, 1=긍정\")\n",
        "    print(\"⚠️ HTML 태그 존재: 전처리에서 제거 필요\")\n",
        "    print(\"📏 텍스트 길이: 다양함 (벡터화에서 표준화 필요)\")\n",
        "    print(\"🎯 다음 단계: TextVectorization으로 수치화\")\n",
        "else:\n",
        "    print(\"❌ 훈련 데이터셋을 먼저 로드해주세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. 텍스트 벡터화 설정\n",
        "\n",
        "TextVectorization을 사용하여 텍스트를 수치 벡터로 변환합니다.\n",
        "- Multi-hot encoding (Bag of Words) 방식 사용\n",
        "- 상위 20,000개 단어만 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "텍스트 벡터화 준비 완료\n"
          ]
        }
      ],
      "source": [
        "# 텍스트 벡터화 레이어 생성\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,  # 자주 사용하는 단어 20,000개만 지정\n",
        "    output_mode=\"multi_hot\"  # Multi-hot encoding (BoW 방식)\n",
        "    # 각 리뷰마다 20,000개 요소를 갖는 배열 생성\n",
        "    # 배열에서 문장 중 단어가 있는 곳은 1, 없으면 0\n",
        ")\n",
        "\n",
        "# 훈련 데이터에서 텍스트만 추출 (레이블 제거)\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# 어휘 사전 구축\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "print(\"텍스트 벡터화 준비 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. 데이터셋 벡터화\n",
        "\n",
        "각 데이터셋에 텍스트 벡터화를 적용합니다.\n",
        "- 유니그램(Unigram): 단어를 1개씩 처리하는 방식\n",
        "- 멀티프로세싱으로 성능 향상\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터셋 벡터화 완료\n"
          ]
        }
      ],
      "source": [
        "# 멀티프로세싱으로 CPU 코어 4개 사용하여 벡터화 수행\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), \n",
        "    num_parallel_calls=4\n",
        ")\n",
        "\n",
        "print(\"데이터셋 벡터화 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. 벡터화 결과 확인\n",
        "\n",
        "벡터화된 데이터의 형태를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "벡터화 후 데이터 확인\n",
            "==================================================\n",
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(\n",
            "[[1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]], shape=(3, 20000), dtype=float32)\n",
            "targets[0]: tf.Tensor([0 0 1], shape=(3,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(\"벡터화 후 데이터 확인\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[:3])\n",
        "    print(\"targets[0]:\", targets[:3])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. 모델 정의\n",
        "\n",
        "간단한 Dense 신경망 모델을 생성합니다.\n",
        "- 입력층: 20,000차원 (어휘 사전 크기)\n",
        "- 은닉층: 16개 뉴런, ReLU 활성화\n",
        "- 드롭아웃: 0.5 (과적합 방지)\n",
        "- 출력층: 1개 뉴런, 시그모이드 활성화 (이진 분류)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def getModel(max_tokens=20000, hidden_dim=16):\n",
        "    \"\"\"모델 생성 및 반환 함수\"\"\"\n",
        "    inputs = keras.Input(shape=(max_tokens,))  # 입력층\n",
        "    x = layers.Dense(hidden_dim, activation='relu')(inputs)  # 은닉층\n",
        "    x = layers.Dropout(0.5)(x)  # 드롭아웃\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)  # 출력층\n",
        "    \n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# 모델 생성\n",
        "model = getModel()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 11. 모델 훈련\n",
        "\n",
        "캐싱을 사용하여 효율적으로 모델을 훈련합니다.\n",
        "- cache(): 첫 번째 에포크에서 전처리를 한 번만 수행하고 메모리에 캐싱\n",
        "- 메모리에 들어갈 만큼 작은 데이터셋일 때만 효과적\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2973 - accuracy: 0.8869 - val_loss: 0.2946 - val_accuracy: 0.8814\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2532 - accuracy: 0.9104 - val_loss: 0.3049 - val_accuracy: 0.8828\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2242 - accuracy: 0.9205 - val_loss: 0.3155 - val_accuracy: 0.8860\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2232 - accuracy: 0.9230 - val_loss: 0.3225 - val_accuracy: 0.8870\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2118 - accuracy: 0.9279 - val_loss: 0.3545 - val_accuracy: 0.8760\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2067 - accuracy: 0.9337 - val_loss: 0.3493 - val_accuracy: 0.8886\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2074 - accuracy: 0.9331 - val_loss: 0.3645 - val_accuracy: 0.8818\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2022 - accuracy: 0.9346 - val_loss: 0.3732 - val_accuracy: 0.8822\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2001 - accuracy: 0.9380 - val_loss: 0.3834 - val_accuracy: 0.8812\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1924 - accuracy: 0.9369 - val_loss: 0.3928 - val_accuracy: 0.8776\n",
            "모델 훈련 완료\n"
          ]
        }
      ],
      "source": [
        "# 콜백 설정: 최고 성능 모델 저장\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"binary_1gram.keras\", \n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(\n",
        "    binary_1gram_train_ds.cache(),  # 훈련 데이터 (캐싱 적용)\n",
        "    validation_data=binary_1gram_val_ds,  # 검증 데이터\n",
        "    epochs=10,  # 에포크 수\n",
        "    callbacks=callbacks  # 콜백\n",
        ")\n",
        "\n",
        "print(\"모델 훈련 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 12. 모델 평가\n",
        "\n",
        "저장된 최고 성능 모델을 로드하여 테스트 데이터로 평가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3027 - accuracy: 0.8742\n",
            "테스트셋 정확도: 0.8742\n"
          ]
        }
      ],
      "source": [
        "# 저장된 모델 로드\n",
        "model = models.load_model(\"binary_1gram.keras\")\n",
        "\n",
        "# 테스트 데이터로 평가\n",
        "test_results = model.evaluate(binary_1gram_test_ds)\n",
        "print(f\"테스트셋 정확도: {test_results[1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part V: 프로젝트 결론 및 실무 적용\n",
        "\n",
        "## 5.1 구현 성과 요약\n",
        "\n",
        "### 🎯 달성한 목표\n",
        "1. **완전한 NLP 파이프라인 구축**\n",
        "   - ✅ 대용량 데이터 다운로드 및 전처리 자동화\n",
        "   - ✅ 체계적인 데이터 분할 (train/val/test)\n",
        "   - ✅ 효율적인 텍스트 벡터화 (Bag of Words)\n",
        "   - ✅ 신경망 모델 구축 및 훈련\n",
        "\n",
        "2. **실무 수준의 코드 품질**\n",
        "   - ✅ 오류 처리 및 예외 상황 대응\n",
        "   - ✅ 진행률 표시 및 상태 모니터링\n",
        "   - ✅ 재현 가능한 결과 보장 (시드 설정)\n",
        "   - ✅ 모듈화된 함수 설계\n",
        "\n",
        "3. **성능 최적화 기법 적용**\n",
        "   - ✅ 데이터 캐싱으로 훈련 속도 향상\n",
        "   - ✅ 멀티프로세싱 활용\n",
        "   - ✅ 배치 처리 최적화\n",
        "   - ✅ 메모리 효율적 데이터 로딩\n",
        "\n",
        "## 5.2 기술적 성과\n",
        "\n",
        "### 📊 모델 성능 지표\n",
        "```\n",
        "예상 성능 (IMDB 벤치마크 기준):\n",
        "- Baseline (Random): 50% 정확도\n",
        "- Bag of Words + Dense: 85-89% 정확도  \n",
        "- 고급 모델 (RNN/BERT): 90-95% 정확도\n",
        "```\n",
        "\n",
        "### 🛠️ 사용된 핵심 기술\n",
        "- **TensorFlow/Keras**: 딥러닝 프레임워크\n",
        "- **TextVectorization**: 효율적인 텍스트 전처리\n",
        "- **tf.data API**: 고성능 데이터 파이프라인\n",
        "- **Multi-hot Encoding**: 해석 가능한 특성 표현\n",
        "\n",
        "## 5.3 실무 적용 방안\n",
        "\n",
        "### 🏢 산업별 활용 사례\n",
        "\n",
        "#### 1) 전자상거래\n",
        "```python\n",
        "# 제품 리뷰 자동 분석\n",
        "review_sentiment = model.predict(\"This product is amazing!\")\n",
        "if review_sentiment > 0.5:\n",
        "    update_product_rating(positive=True)\n",
        "```\n",
        "\n",
        "#### 2) 소셜 미디어 모니터링\n",
        "```python\n",
        "# 브랜드 멘션 감성 분석\n",
        "brand_mentions = collect_social_posts(\"brand_name\")\n",
        "sentiment_scores = model.predict(brand_mentions)\n",
        "generate_sentiment_report(sentiment_scores)\n",
        "```\n",
        "\n",
        "#### 3) 고객 서비스\n",
        "```python\n",
        "# 문의 내용 우선순위 분류\n",
        "inquiry_sentiment = model.predict(customer_message)\n",
        "if inquiry_sentiment < 0.3:  # 매우 부정적\n",
        "    escalate_to_manager(inquiry_id)\n",
        "```\n",
        "\n",
        "### 🚀 확장 가능한 아키텍처\n",
        "\n",
        "#### 프로덕션 배포 구조\n",
        "```\n",
        "사용자 입력 → API Gateway → \n",
        "전처리 서버 → ML 모델 서버 → \n",
        "결과 캐싱 → 클라이언트 응답\n",
        "```\n",
        "\n",
        "#### 모델 업데이트 전략\n",
        "```\n",
        "1. 새로운 데이터 수집\n",
        "2. 자동 재훈련 파이프라인\n",
        "3. A/B 테스트로 성능 검증  \n",
        "4. 점진적 모델 배포\n",
        "```\n",
        "\n",
        "## 5.4 개선 방향 및 차세대 기법\n",
        "\n",
        "### 📈 단계적 성능 개선 로드맵\n",
        "\n",
        "#### Phase 1: 기본 성능 향상 (즉시 적용 가능)\n",
        "- **TF-IDF 벡터화**: 단어 중요도 가중치 적용\n",
        "- **N-gram 확장**: 단어 조합 패턴 인식\n",
        "- **정규화 강화**: Dropout, Batch Normalization\n",
        "- **하이퍼파라미터 튜닝**: 학습률, 배치 크기 최적화\n",
        "\n",
        "#### Phase 2: 고급 모델 도입 (중기 목표)\n",
        "- **임베딩 레이어**: 밀집 벡터 표현 학습\n",
        "- **순환 신경망 (RNN/LSTM)**: 순서 정보 활용\n",
        "- **합성곱 신경망 (CNN)**: 지역적 패턴 인식\n",
        "- **어텐션 메커니즘**: 중요 단어 집중\n",
        "\n",
        "#### Phase 3: 최신 기법 적용 (장기 목표)\n",
        "- **사전 훈련 모델**: BERT, RoBERTa, GPT\n",
        "- **전이 학습**: 도메인 특화 파인튜닝\n",
        "- **멀티모달**: 텍스트 + 이미지 + 메타데이터\n",
        "- **설명 가능한 AI**: 모델 결정 근거 제시\n",
        "\n",
        "### 🔬 실험적 접근법\n",
        "\n",
        "#### 데이터 증강 기법\n",
        "```python\n",
        "# 동의어 치환으로 데이터 확장\n",
        "augmented_texts = synonym_replacement(original_texts)\n",
        "\n",
        "# 백번역으로 다양성 증가  \n",
        "backtranslated = translate(texts, 'en->ko->en')\n",
        "```\n",
        "\n",
        "#### 앙상블 학습\n",
        "```python\n",
        "# 다양한 모델 조합으로 성능 향상\n",
        "models = [bow_model, tfidf_model, embedding_model]\n",
        "ensemble_prediction = weighted_average(models, weights)\n",
        "```\n",
        "\n",
        "## 5.5 학습 성과 및 다음 단계\n",
        "\n",
        "### 🎓 획득한 핵심 역량\n",
        "1. **자연어 처리 파이프라인**: 텍스트 데이터 → 예측 결과\n",
        "2. **대용량 데이터 처리**: 효율적인 I/O 및 메모리 관리\n",
        "3. **실무 코딩 스킬**: 오류 처리, 로깅, 모니터링\n",
        "4. **모델 평가 및 개선**: 성능 측정 및 최적화 전략\n",
        "\n",
        "### 🚦 다음 학습 권장 사항\n",
        "\n",
        "#### 심화 학습 주제\n",
        "1. **고급 NLP 기법**\n",
        "   - Transformer 아키텍처 이해\n",
        "   - BERT, GPT 활용법\n",
        "   - 한국어 NLP 특화 기법\n",
        "\n",
        "2. **MLOps 및 배포**\n",
        "   - Docker 컨테이너화\n",
        "   - Kubernetes 오케스트레이션\n",
        "   - CI/CD 파이프라인 구축\n",
        "\n",
        "3. **성능 최적화**\n",
        "   - 모델 압축 및 양자화\n",
        "   - 추론 속도 최적화\n",
        "   - 분산 학습 시스템\n",
        "\n",
        "### 🎯 실습 프로젝트 제안\n",
        "1. **다른 데이터셋 적용**: 한국어 리뷰, 뉴스 분류\n",
        "2. **실시간 시스템 구축**: 웹 인터페이스 + API 서버\n",
        "3. **비교 연구**: 다양한 모델 성능 벤치마킹\n",
        "4. **도메인 특화**: 의료, 금융, 법률 텍스트 분석\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 추천 자료 및 참고 문헌\n",
        "\n",
        "### 핵심 논문\n",
        "- Maas et al. (2011): \"Learning Word Vectors for Sentiment Analysis\"\n",
        "- Mikolov et al. (2013): \"Efficient Estimation of Word Representations\"\n",
        "- Vaswani et al. (2017): \"Attention Is All You Need\"\n",
        "\n",
        "### 실무 자료\n",
        "- TensorFlow Text Guide: https://www.tensorflow.org/text\n",
        "- Hugging Face Transformers: https://huggingface.co/transformers\n",
        "- Papers With Code NLP: https://paperswithcode.com/area/natural-language-processing\n",
        "\n",
        "### 한국어 NLP 자료\n",
        "- KoBERT, KoGPT: 한국어 사전 훈련 모델\n",
        "- 한국어 Embedding: FastText, Word2Vec\n",
        "- 형태소 분석기: KoNLPy, Mecab\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 축하합니다! 완전한 NLP 감성 분석 시스템을 구축했습니다!**\n",
        "\n",
        "이제 여러분은 실무에서 바로 활용할 수 있는 텍스트 분석 역량을 갖추었습니다. \n",
        "계속해서 새로운 기법을 학습하고 실제 프로젝트에 적용해보세요! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
