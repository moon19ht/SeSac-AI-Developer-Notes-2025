{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리: 텍스트 벡터화 구현\n",
    "\n",
    "## 개요\n",
    "이 노트북에서는 자연어 처리의 기본인 텍스트 벡터화를 직접 구현해봅니다.\n",
    "- 텍스트 표준화 (Standardization)\n",
    "- 토큰화 (Tokenization)\n",
    "- 어휘 사전 구축 (Vocabulary Building)\n",
    "- 인코딩/디코딩 (Encoding/Decoding)\n",
    "\n",
    "## 학습 목표\n",
    "1. 텍스트를 컴퓨터가 이해할 수 있는 숫자로 변환하는 과정 이해\n",
    "2. 벡터화 과정의 각 단계별 역할 파악\n",
    "3. 직접 구현을 통한 텍스트 전처리 원리 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구두점 목록: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(\"구두점 목록:\", string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MyVectorize 클래스 구현\n",
    "\n",
    "텍스트 벡터화를 위한 커스텀 클래스를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVectorize:\n",
    "    \"\"\"\n",
    "    텍스트를 벡터로 변환하는 클래스\n",
    "    \n",
    "    주요 기능:\n",
    "    - 텍스트 표준화 (소문자 변환, 구두점 제거)\n",
    "    - 토큰화 (단어 단위로 분할)\n",
    "    - 어휘 사전 구축\n",
    "    - 텍스트 인코딩/디코딩\n",
    "    \"\"\"\n",
    "    \n",
    "    def standardize(self, text):\n",
    "        \"\"\"\n",
    "        텍스트 표준화 함수\n",
    "        \n",
    "        Args:\n",
    "            text (str): 입력 텍스트\n",
    "            \n",
    "        Returns:\n",
    "            str: 표준화된 텍스트 (소문자 + 구두점 제거)\n",
    "        \"\"\"\n",
    "        # 1. 전부 소문자로 변환\n",
    "        text = text.lower()\n",
    "        # 2. 구두점 제거\n",
    "        return \"\".join(c for c in text if c not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        토큰화 함수\n",
    "        \n",
    "        Args:\n",
    "            text (str): 표준화된 텍스트\n",
    "            \n",
    "        Returns:\n",
    "            list: 토큰(단어) 리스트\n",
    "        \"\"\"\n",
    "        return text.split()  # 공백 기준으로 분할\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        \"\"\"\n",
    "        어휘 사전 구축 함수\n",
    "        \n",
    "        Args:\n",
    "            dataset (list): 텍스트 데이터셋\n",
    "            \n",
    "        특수 토큰:\n",
    "        - \"\": 패딩용 (인덱스 0)\n",
    "        - \"[UNK]\": 미등록 단어용 (인덱스 1)\n",
    "        \"\"\"\n",
    "        # 특수 토큰으로 초기화\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        \n",
    "        # 데이터셋 순회하며 어휘 사전 구축\n",
    "        for text in dataset:\n",
    "            # 표준화 -> 토큰화\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            \n",
    "            # 새로운 토큰 발견시 사전에 추가\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        \n",
    "        # 역방향 사전 생성 (숫자 -> 단어)\n",
    "        self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        텍스트를 숫자 시퀀스로 인코딩\n",
    "        \n",
    "        Args:\n",
    "            text (str): 인코딩할 텍스트\n",
    "            \n",
    "        Returns:\n",
    "            list: 숫자 시퀀스\n",
    "        \"\"\"\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        # 사전에 없는 단어는 [UNK] 토큰(1)으로 처리\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        \"\"\"\n",
    "        숫자 시퀀스를 텍스트로 디코딩\n",
    "        \n",
    "        Args:\n",
    "            int_sequence (list): 숫자 시퀀스\n",
    "            \n",
    "        Returns:\n",
    "            str: 디코딩된 텍스트\n",
    "        \"\"\"\n",
    "        return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 벡터화 클래스 테스트\n",
    "\n",
    "구현한 클래스를 테스트 데이터로 검증해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 테스트 데이터셋 ===\n",
      "1. I write, erase, reqrite\n",
      "2. Erase again, and then\n",
      "3. A poppy blooms\n",
      "4. Dog is pretty\n"
     ]
    }
   ],
   "source": [
    "# 벡터화 인스턴스 생성\n",
    "mv = MyVectorize()\n",
    "\n",
    "# 테스트 데이터셋\n",
    "dataset = [\n",
    "    \"I write, erase, reqrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms\",\n",
    "    \"Dog is pretty\"\n",
    "]\n",
    "\n",
    "print(\"=== 테스트 데이터셋 ===\")\n",
    "for i, text in enumerate(dataset):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 표준화 및 토큰화 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문장: 'Erase again, and then'\n",
      "표준화 결과: 'erase again and then'\n",
      "토큰화 결과: ['erase', 'again', 'and', 'then']\n"
     ]
    }
   ],
   "source": [
    "# 두 번째 문장으로 표준화/토큰화 테스트\n",
    "test_sentence = dataset[1]\n",
    "print(f\"원본 문장: '{test_sentence}'\")\n",
    "\n",
    "# 표준화 테스트\n",
    "standardized = mv.standardize(test_sentence)\n",
    "print(f\"표준화 결과: '{standardized}'\")\n",
    "\n",
    "# 토큰화 테스트\n",
    "tokens = mv.tokenize(standardized)\n",
    "print(f\"토큰화 결과: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 어휘 사전 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 어휘 사전 (단어 -> 인덱스) ===\n",
      "'' -> 0\n",
      "'[UNK]' -> 1\n",
      "'i' -> 2\n",
      "'write' -> 3\n",
      "'erase' -> 4\n",
      "'reqrite' -> 5\n",
      "'again' -> 6\n",
      "'and' -> 7\n",
      "'then' -> 8\n",
      "'a' -> 9\n",
      "'poppy' -> 10\n",
      "'blooms' -> 11\n",
      "'dog' -> 12\n",
      "'is' -> 13\n",
      "'pretty' -> 14\n",
      "\n",
      "=== 역방향 사전 (인덱스 -> 단어) ===\n",
      "0 -> ''\n",
      "1 -> '[UNK]'\n",
      "2 -> 'i'\n",
      "3 -> 'write'\n",
      "4 -> 'erase'\n",
      "5 -> 'reqrite'\n",
      "6 -> 'again'\n",
      "7 -> 'and'\n",
      "8 -> 'then'\n",
      "9 -> 'a'\n",
      "10 -> 'poppy'\n",
      "11 -> 'blooms'\n",
      "12 -> 'dog'\n",
      "13 -> 'is'\n",
      "14 -> 'pretty'\n",
      "\n",
      "총 어휘 개수: 15\n"
     ]
    }
   ],
   "source": [
    "# 어휘 사전 구축\n",
    "mv.make_vocabulary(dataset)\n",
    "\n",
    "print(\"=== 어휘 사전 (단어 -> 인덱스) ===\")\n",
    "for word, idx in mv.vocabulary.items():\n",
    "    print(f\"'{word}' -> {idx}\")\n",
    "\n",
    "print(\"\\n=== 역방향 사전 (인덱스 -> 단어) ===\")\n",
    "for idx, word in mv.inverse_vocabulary.items():\n",
    "    print(f\"{idx} -> '{word}'\")\n",
    "\n",
    "print(f\"\\n총 어휘 개수: {len(mv.vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 인코딩/디코딩 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텍스트: 'I write erase'\n",
      "인코딩 결과: [2, 3, 4]\n",
      "\n",
      "=== 인코딩 과정 상세 분석 ===\n",
      "'i' -> 2\n",
      "'write' -> 3\n",
      "'erase' -> 4\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 테스트\n",
    "test_text = \"I write erase\"\n",
    "encoded = mv.encode(test_text)\n",
    "print(f\"입력 텍스트: '{test_text}'\")\n",
    "print(f\"인코딩 결과: {encoded}\")\n",
    "\n",
    "# 각 단어의 인코딩 과정 상세 분석\n",
    "print(\"\\n=== 인코딩 과정 상세 분석 ===\")\n",
    "standardized_test = mv.standardize(test_text)\n",
    "tokens_test = mv.tokenize(standardized_test)\n",
    "for token in tokens_test:\n",
    "    idx = mv.vocabulary.get(token, 1)\n",
    "    print(f\"'{token}' -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 시퀀스: [2, 3, 4, 23]\n",
      "디코딩 결과: 'i write erase [UNK]'\n",
      "\n",
      "=== 디코딩 과정 상세 분석 ===\n",
      "2 -> 'i'\n",
      "3 -> 'write'\n",
      "4 -> 'erase'\n",
      "23 -> '[UNK]'\n"
     ]
    }
   ],
   "source": [
    "# 디코딩 테스트\n",
    "test_sequence = [2, 3, 4, 23]  # 일부는 존재하지 않는 인덱스\n",
    "decoded = mv.decode(test_sequence)\n",
    "print(f\"입력 시퀀스: {test_sequence}\")\n",
    "print(f\"디코딩 결과: '{decoded}'\")\n",
    "\n",
    "# 각 인덱스의 디코딩 과정 상세 분석\n",
    "print(\"\\n=== 디코딩 과정 상세 분석 ===\")\n",
    "for idx in test_sequence:\n",
    "    word = mv.inverse_vocabulary.get(idx, \"[UNK]\")\n",
    "    print(f\"{idx} -> '{word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 전체 데이터셋 처리 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 전체 데이터셋 인코딩 결과 ===\n",
      "\n",
      "1. 원본: 'I write, erase, reqrite'\n",
      "   표준화: 'i write erase reqrite'\n",
      "   인코딩: [2, 3, 4, 5]\n",
      "   디코딩: 'i write erase reqrite'\n",
      "\n",
      "2. 원본: 'Erase again, and then'\n",
      "   표준화: 'erase again and then'\n",
      "   인코딩: [4, 6, 7, 8]\n",
      "   디코딩: 'erase again and then'\n",
      "\n",
      "3. 원본: 'A poppy blooms'\n",
      "   표준화: 'a poppy blooms'\n",
      "   인코딩: [9, 10, 11]\n",
      "   디코딩: 'a poppy blooms'\n",
      "\n",
      "4. 원본: 'Dog is pretty'\n",
      "   표준화: 'dog is pretty'\n",
      "   인코딩: [12, 13, 14]\n",
      "   디코딩: 'dog is pretty'\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 전체 데이터셋 인코딩 결과 ===\")\n",
    "for i, text in enumerate(dataset):\n",
    "    encoded = mv.encode(text)\n",
    "    decoded = mv.decode(encoded)\n",
    "    print(f\"\\n{i+1}. 원본: '{text}'\")\n",
    "    print(f\"   표준화: '{mv.standardize(text)}'\")\n",
    "    print(f\"   인코딩: {encoded}\")\n",
    "    print(f\"   디코딩: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 미등록 단어(UNK) 처리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 텍스트: 'I love programming and coding'\n",
      "토큰들: ['i', 'love', 'programming', 'and', 'coding']\n",
      "\n",
      "=== 토큰별 어휘 사전 존재 여부 ===\n",
      "'i' -> 등록됨 (인덱스: 2)\n",
      "'love' -> 미등록 ([UNK]로 처리)\n",
      "'programming' -> 미등록 ([UNK]로 처리)\n",
      "'and' -> 등록됨 (인덱스: 7)\n",
      "'coding' -> 미등록 ([UNK]로 처리)\n",
      "\n",
      "인코딩 결과: [2, 1, 1, 7, 1]\n",
      "디코딩 결과: 'i [UNK] [UNK] and [UNK]'\n"
     ]
    }
   ],
   "source": [
    "# 어휘 사전에 없는 새로운 단어가 포함된 문장 테스트\n",
    "new_text = \"I love programming and coding\"\n",
    "print(f\"새로운 텍스트: '{new_text}'\")\n",
    "\n",
    "# 표준화 및 토큰화\n",
    "standardized_new = mv.standardize(new_text)\n",
    "tokens_new = mv.tokenize(standardized_new)\n",
    "print(f\"토큰들: {tokens_new}\")\n",
    "\n",
    "# 각 토큰이 어휘 사전에 있는지 확인\n",
    "print(\"\\n=== 토큰별 어휘 사전 존재 여부 ===\")\n",
    "for token in tokens_new:\n",
    "    if token in mv.vocabulary:\n",
    "        print(f\"'{token}' -> 등록됨 (인덱스: {mv.vocabulary[token]})\")\n",
    "    else:\n",
    "        print(f\"'{token}' -> 미등록 ([UNK]로 처리)\")\n",
    "\n",
    "# 인코딩 결과\n",
    "encoded_new = mv.encode(new_text)\n",
    "decoded_new = mv.decode(encoded_new)\n",
    "print(f\"\\n인코딩 결과: {encoded_new}\")\n",
    "print(f\"디코딩 결과: '{decoded_new}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결론 및 정리\n",
    "\n",
    "### 구현한 벡터화 시스템의 특징:\n",
    "\n",
    "1. **표준화 (Standardization)**\n",
    "   - 대소문자 통일 (소문자 변환)\n",
    "   - 구두점 제거로 일관성 확보\n",
    "\n",
    "2. **토큰화 (Tokenization)**\n",
    "   - 공백 기준 단어 분할\n",
    "   - 간단하지만 효과적인 방법\n",
    "\n",
    "3. **어휘 사전 (Vocabulary)**\n",
    "   - 특수 토큰 활용 (\"\", \"[UNK]\")\n",
    "   - 동적 사전 구축\n",
    "   - 양방향 매핑 지원\n",
    "\n",
    "4. **인코딩/디코딩**\n",
    "   - 텍스트 ↔ 숫자 시퀀스 변환\n",
    "   - 미등록 단어 처리 (UNK 토큰)\n",
    "   - 정보 손실 최소화\n",
    "\n",
    "### 실제 응용에서의 고려사항:\n",
    "- 더 정교한 토큰화 (서브워드, BPE 등)\n",
    "- 빈도 기반 어휘 필터링\n",
    "- 패딩 및 시퀀스 길이 관리\n",
    "- 대용량 데이터 처리 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 추가 실습 문제\n",
    "\n",
    "다음 문제들을 해결해보며 이해도를 높여보세요:\n",
    "\n",
    "1. 한국어 텍스트로 동일한 과정을 수행해보기\n",
    "2. 빈도수 기반 어휘 필터링 기능 추가하기\n",
    "3. 최대 시퀀스 길이 제한 및 패딩 기능 구현하기\n",
    "4. 문자 단위 토큰화와 단어 단위 토큰화 비교하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
