{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Transfer Learning을 활용한 개-고양이 분류 프로젝트\n",
    "\n",
    "이 노트북은 사전학습된 VGG19 모델을 사용하여 개와 고양이 이미지를 분류하는 Transfer Learning 프로젝트입니다.\n",
    "\n",
    "## 프로젝트 개요\n",
    "- **목표**: VGG19 사전학습 모델을 활용한 개-고양이 이진 분류\n",
    "- **방법**: Transfer Learning (특성 추출 방식)\n",
    "- **데이터**: Cats and Dogs 데이터셋\n",
    "- **프레임워크**: TensorFlow/Keras\n",
    "\n",
    "## Transfer Learning이란?\n",
    "이미 수십만 장의 이미지로 학습된 모델(VGG19)을 재활용하여:\n",
    "- **학습 시간 단축**: CNN 부분은 동결하고 분류층만 학습\n",
    "- **높은 성능**: 적은 데이터로도 좋은 성능 달성\n",
    "- **효율성**: 컴퓨터 자원이 적어도 학습 가능\n",
    "\n",
    "## 두 가지 Transfer Learning 방식\n",
    "\n",
    "### 1. 투스테이지 방식 (Two-Stage)\n",
    "- **방법**: VGG19의 특징을 미리 계산하고 numpy 배열로 저장 → 분류 학습\n",
    "- **장점**: 훈련 속도가 매우 빠름\n",
    "- **단점**: 메모리 사용량 많음, 데이터 증강 제한적\n",
    "\n",
    "### 2. 인라인 방식 (Inline)  \n",
    "- **방법**: VGG19 특징 추출부를 전체 모델에 포함하여 학습\n",
    "- **장점**: 원본 이미지에 데이터 증강 직접 적용, 과적합 방지\n",
    "- **단점**: 투스테이지보다 속도 느림\n",
    "\n",
    "**이 프로젝트에서는 투스테이지 방식을 사용합니다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. 라이브러리 임포트 및 환경 설정\n",
    "\n",
    "필요한 라이브러리들을 임포트합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 관련 라이브러리\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras import models, layers\n",
    "\n",
    "# 데이터 처리 및 시각화 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 파일 시스템 관련 라이브러리\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gdown  # 구글 드라이브에서 데이터 다운로드용\n",
    "\n",
    "print(\"라이브러리 임포트 완료!\")\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(f\"Keras 버전: {keras.__version__}\")\n",
    "\n",
    "# 경고 메시지 제거 (선택사항)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ GPU/CPU 디바이스 자동 선택\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f\"✅ GPU({tf.test.gpu_device_name()}) 사용 설정 완료\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        DEVICE = '/GPU:0'\n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # GPU 사용 방지\n",
    "    DEVICE = '/CPU:0'\n",
    "    print(\"⚠️ GPU를 찾을 수 없습니다. CPU를 사용합니다.\")\n",
    "\n",
    "print(\"현재 사용 디바이스:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. 데이터 전처리 및 분할\n",
    "\n",
    "Cats and Dogs 데이터셋을 Train/Validation/Test로 분할합니다.\n",
    "\n",
    "### 데이터 구조:\n",
    "- **원본**: `cats_and_dogs/train/` (총 25,000장)\n",
    "- **분할 후**: \n",
    "  - Train: 0~999번 이미지 (각 클래스 1,000장)\n",
    "  - Validation: 1000~1499번 이미지 (각 클래스 500장)\n",
    "  - Test: 1500~1999번 이미지 (각 클래스 500장)\n",
    "\n",
    "### 폴더 구조:\n",
    "```\n",
    "cats_and_dogs_small/\n",
    "├── train/\n",
    "│   ├── cat/\n",
    "│   └── dog/\n",
    "├── validation/\n",
    "│   ├── cat/\n",
    "│   └── dog/\n",
    "└── test/\n",
    "    ├── cat/\n",
    "    └── dog/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정\n",
    "original_dir = pathlib.Path(\"../../data/cats_and_dogs/train\")\n",
    "new_base_dir = pathlib.Path(\"../../data/cats_and_dogs/cats_and_dogs_small\")\n",
    "\n",
    "print(\"📁 데이터셋 경로 설정 완료!\")\n",
    "print(f\"원본 데이터: {original_dir}\")\n",
    "print(f\"분할 저장: {new_base_dir}\")\n",
    "\n",
    "# 선택사항: 구글 드라이브에서 데이터 다운로드\n",
    "# gdown.download(id='18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd', output='dogs-vs-cats.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subset(subset_name, start_index, end_index):\n",
    "    \"\"\"\n",
    "    특정 범위의 이미지들을 지정된 폴더로 복사\n",
    "    \n",
    "    Args:\n",
    "        subset_name: 'train', 'validation', 'test'\n",
    "        start_index: 시작 인덱스\n",
    "        end_index: 끝 인덱스 (포함되지 않음)\n",
    "    \"\"\"\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        # 목적지 폴더 생성\n",
    "        dir_path = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # 파일명 리스트 생성 (예: cat.0.jpg, cat.1.jpg, ...)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        \n",
    "        # 파일 복사\n",
    "        for fname in fnames:\n",
    "            src_path = original_dir / fname\n",
    "            dst_path = dir_path / fname\n",
    "            if src_path.exists():  # 원본 파일이 존재하는 경우에만 복사\n",
    "                shutil.copyfile(src_path, dst_path)\n",
    "        \n",
    "        print(f\"✅ {subset_name}/{category}: {len(fnames)}개 이미지 복사 완료\")\n",
    "\n",
    "print(\"데이터 분할 함수 정의 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 실행\n",
    "print(\"🔄 데이터 분할 시작...\")\n",
    "\n",
    "# Train 세트: 각 클래스 1,000장 (0~999)\n",
    "make_subset(\"train\", 0, 1000)\n",
    "\n",
    "# Validation 세트: 각 클래스 500장 (1000~1499)\n",
    "make_subset(\"validation\", 1000, 1500)\n",
    "\n",
    "# Test 세트: 각 클래스 500장 (1500~1999)\n",
    "make_subset(\"test\", 1500, 2000)\n",
    "\n",
    "print(\"\\n✅ 모든 데이터 분할 완료!\")\n",
    "print(f\"📊 총 데이터: Train({1000*2}), Validation({500*2}), Test({500*2})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로더 생성\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16,\n",
    "    label_mode='binary'  # 이진 분류 (0: cat, 1: dog)\n",
    ")\n",
    "\n",
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "print(\"✅ 데이터셋 로더 생성 완료!\")\n",
    "print(f\"📁 Train: {train_ds}\")\n",
    "print(f\"📁 Validation: {validation_ds}\")\n",
    "print(f\"📁 Test: {test_ds}\")\n",
    "\n",
    "# 클래스 이름 확인\n",
    "class_names = train_ds.class_names\n",
    "print(f\"🏷️ 클래스: {class_names}\")  # ['cat', 'dog']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. VGG19 사전학습 모델 로드 및 특성 추출\n",
    "\n",
    "ImageNet으로 사전학습된 VGG19 모델을 로드하고 특성을 추출합니다.\n",
    "\n",
    "### VGG19 모델 특징:\n",
    "- **ImageNet**: 1400만 장의 이미지, 1000개 클래스로 학습됨\n",
    "- **구조**: 16개 Conv 레이어 + 3개 FC 레이어 = 총 19개 레이어\n",
    "- **특성 추출**: 마지막 Conv 블록(block5_pool)에서 특성 벡터 추출\n",
    "- **출력 크기**: (5, 5, 512) → 각 이미지당 12,800차원 특성 벡터\n",
    "\n",
    "### Transfer Learning 과정:\n",
    "1. VGG19 모델 로드 (가중치 동결)\n",
    "2. 각 이미지를 VGG19에 통과시켜 특성 추출\n",
    "3. 추출된 특성으로 새로운 분류 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 사전학습 모델 로드\n",
    "conv_base = keras.applications.vgg19.VGG19(\n",
    "    weights=\"imagenet\",        # ImageNet 가중치 사용\n",
    "    include_top=False,         # 최상위 분류층 제외 (CNN 부분만 사용)\n",
    "    input_shape=(180, 180, 3)  # 입력 이미지 크기 지정\n",
    ")\n",
    "\n",
    "print(\"🚀 VGG19 모델 로드 완료!\")\n",
    "print(f\"📊 모델 입력 크기: {conv_base.input_shape}\")\n",
    "print(f\"📊 모델 출력 크기: {conv_base.output_shape}\")\n",
    "\n",
    "# VGG19 모델 구조 요약\n",
    "conv_base.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(dataset):\n",
    "    \"\"\"\n",
    "    VGG19 모델을 사용하여 데이터셋에서 특성과 라벨을 추출\n",
    "    \n",
    "    Args:\n",
    "        dataset: tf.data.Dataset 객체\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (features, labels) numpy 배열\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"🔄 특성 추출 중...\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataset):\n",
    "        # VGG19 전처리 (ImageNet 정규화)\n",
    "        preprocessed_images = keras.applications.vgg19.preprocess_input(images)\n",
    "        \n",
    "        # VGG19으로 특성 추출\n",
    "        features = conv_base.predict(preprocessed_images, verbose=0)\n",
    "        \n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        # 진행상황 출력\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"   배치 {batch_idx + 1} 처리 완료...\")\n",
    "    \n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "print(\"특성 추출 함수 정의 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features():\n",
    "    \"\"\"\n",
    "    모든 데이터셋에서 특성을 추출하고 파일로 저장\n",
    "    \"\"\"\n",
    "    print(\"📥 모든 데이터셋에서 특성 추출 시작...\")\n",
    "    \n",
    "    # 각 데이터셋에서 특성 추출\n",
    "    print(\"\\n1️⃣ Train 데이터 특성 추출\")\n",
    "    train_features, train_labels = get_features_and_labels(train_ds)\n",
    "    \n",
    "    print(\"\\n2️⃣ Validation 데이터 특성 추출\")\n",
    "    validation_features, validation_labels = get_features_and_labels(validation_ds)\n",
    "    \n",
    "    print(\"\\n3️⃣ Test 데이터 특성 추출\")\n",
    "    test_features, test_labels = get_features_and_labels(test_ds)\n",
    "    \n",
    "    # 모든 데이터를 하나의 리스트로 묶기\n",
    "    data = [\n",
    "        train_features, train_labels,\n",
    "        validation_features, validation_labels,\n",
    "        test_features, test_labels\n",
    "    ]\n",
    "    \n",
    "    # pickle로 저장\n",
    "    with open(\"개고양이특성.bin\", \"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "    \n",
    "    print(\"\\n✅ 특성 추출 및 저장 완료!\")\n",
    "    print(f\"📁 저장 파일: 개고양이특성.bin\")\n",
    "    print(f\"📊 특성 크기: {train_features.shape}\")\n",
    "\n",
    "\n",
    "def load_features():\n",
    "    \"\"\"\n",
    "    저장된 특성 파일을 로드\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_features, train_labels, val_features, val_labels, test_features, test_labels)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"개고양이특성.bin\", \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        print(\"✅ 저장된 특성 파일 로드 완료!\")\n",
    "        return data[0], data[1], data[2], data[3], data[4], data[5]\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ 특성 파일이 없습니다. save_features()를 먼저 실행하세요.\")\n",
    "        return None\n",
    "\n",
    "print(\"특성 저장/로드 함수 정의 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. 분류 모델 구축 및 학습\n",
    "\n",
    "VGG19에서 추출한 특성을 사용하여 개-고양이 분류 모델을 구축하고 학습합니다.\n",
    "\n",
    "### 모델 구조:\n",
    "1. **입력**: VGG19 특성 (5, 5, 512)\n",
    "2. **데이터 증강**: RandomFlip, RandomRotation, RandomZoom\n",
    "3. **레이어들**:\n",
    "   - Flatten: 특성을 1차원으로 변환\n",
    "   - Dense(256): 첫 번째 완전연결층\n",
    "   - Dense(128): 두 번째 완전연결층  \n",
    "   - Dropout(0.5): 과적합 방지\n",
    "   - Dense(1, sigmoid): 이진 분류 출력층\n",
    "\n",
    "### 학습 설정:\n",
    "- **옵티마이저**: RMSprop\n",
    "- **손실 함수**: Binary Crossentropy\n",
    "- **평가 지표**: Accuracy\n",
    "- **콜백**: ModelCheckpoint (최적 모델 자동 저장)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model():\n",
    "    \"\"\"\n",
    "    VGG19 특성을 입력으로 받는 분류 모델 생성\n",
    "    \n",
    "    Returns:\n",
    "        keras.Model: 컴파일된 분류 모델\n",
    "    \"\"\"\n",
    "    # 데이터 증강 레이어 정의\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),     # 수평 뒤집기\n",
    "        layers.RandomRotation(0.1),         # ±10% 회전 (36도)\n",
    "        layers.RandomZoom(0.1),             # ±10% 확대/축소\n",
    "    ])\n",
    "    \n",
    "    # 모델 구조 정의\n",
    "    inputs = keras.Input(shape=(5, 5, 512))  # VGG19 출력 크기\n",
    "    x = data_augmentation(inputs)\n",
    "    x = layers.Flatten()(x)                  # 12,800 차원으로 변환\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)               # 과적합 방지\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  # 이진 분류\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 분류 모델 생성 완료!\")\n",
    "    return model\n",
    "\n",
    "print(\"분류 모델 생성 함수 정의 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    저장된 특성을 사용하여 분류 모델을 학습\n",
    "    \"\"\"\n",
    "    # 저장된 특성 로드\n",
    "    features_data = load_features()\n",
    "    if features_data is None:\n",
    "        print(\"❌ 특성 파일을 먼저 생성하세요!\")\n",
    "        return None\n",
    "    \n",
    "    train_features, train_labels, validation_features, validation_labels, test_features, test_labels = features_data\n",
    "    \n",
    "    print(f\"📊 데이터 크기:\")\n",
    "    print(f\"   Train: {train_features.shape} / {train_labels.shape}\")\n",
    "    print(f\"   Validation: {validation_features.shape} / {validation_labels.shape}\")\n",
    "    print(f\"   Test: {test_features.shape} / {test_labels.shape}\")\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = create_classification_model()\n",
    "    model.summary()\n",
    "    \n",
    "    # 콜백 설정 (최적 모델 자동 저장)\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"특성추출.keras\",\n",
    "            save_best_only=True,        # 최고 성능 모델만 저장\n",
    "            monitor=\"val_loss\",         # 검증 손실 기준\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🚀 모델 학습 시작...\")\n",
    "    \n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        train_features, train_labels,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(validation_features, validation_labels),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 모델 학습 완료!\")\n",
    "    \n",
    "    # 최종 성능 출력\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"\\n📊 최종 성능:\")\n",
    "    print(f\"   훈련 정확도: {final_train_acc:.4f}\")\n",
    "    print(f\"   검증 정확도: {final_val_acc:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"모델 학습 함수 정의 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. 모델 예측 및 성능 평가\n",
    "\n",
    "학습된 모델을 사용하여 테스트 데이터에 대한 예측을 수행하고 성능을 평가합니다.\n",
    "\n",
    "### 평가 방법:\n",
    "1. **저장된 모델 로드**: 최적 성능 모델(`특성추출.keras`) 로드\n",
    "2. **테스트 예측**: 테스트 특성에 대한 예측 수행\n",
    "3. **이진 분류 변환**: 확률값(0~1)을 클래스(0: cat, 1: dog)로 변환\n",
    "4. **정확도 계산**: 예측값과 실제값 비교\n",
    "\n",
    "### 예측 임계값:\n",
    "- **0.5 이상**: 개(dog) 클래스로 분류\n",
    "- **0.5 미만**: 고양이(cat) 클래스로 분류\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate():\n",
    "    \"\"\"\n",
    "    저장된 모델을 사용하여 테스트 데이터 예측 및 평가\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 저장된 모델 로드\n",
    "        model = keras.models.load_model(\"특성추출.keras\")\n",
    "        print(\"✅ 저장된 모델 로드 완료!\")\n",
    "        \n",
    "        # 저장된 특성 로드\n",
    "        features_data = load_features()\n",
    "        if features_data is None:\n",
    "            return\n",
    "        \n",
    "        train_features, train_labels, validation_features, validation_labels, test_features, test_labels = features_data\n",
    "        \n",
    "        print(f\"🔍 테스트 데이터 예측 중...\")\n",
    "        print(f\"   테스트 데이터 크기: {test_features.shape}\")\n",
    "        \n",
    "        # 테스트 데이터 예측\n",
    "        test_predictions = model.predict(test_features, verbose=0)\n",
    "        \n",
    "        # 확률값을 클래스로 변환 (0.5 기준)\n",
    "        test_pred_classes = (test_predictions > 0.5).astype(\"int\").flatten()\n",
    "        \n",
    "        print(f\"\\n📊 예측 결과 (처음 20개):\")\n",
    "        print(f\"   예측값: {test_pred_classes[:20]}\")\n",
    "        print(f\"   실제값: {test_labels[:20].astype(int)}\")\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct_predictions = np.sum(test_pred_classes == test_labels.astype(int))\n",
    "        total_predictions = len(test_labels)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        print(f\"\\n🎯 성능 평가 결과:\")\n",
    "        print(f\"   전체 테스트 샘플: {total_predictions}개\")\n",
    "        print(f\"   정확히 예측한 수: {correct_predictions}개\")\n",
    "        print(f\"   틀리게 예측한 수: {total_predictions - correct_predictions}개\")\n",
    "        print(f\"   테스트 정확도: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        # 클래스별 세부 분석\n",
    "        cat_indices = test_labels == 0\n",
    "        dog_indices = test_labels == 1\n",
    "        \n",
    "        cat_accuracy = np.sum(test_pred_classes[cat_indices] == test_labels[cat_indices]) / np.sum(cat_indices)\n",
    "        dog_accuracy = np.sum(test_pred_classes[dog_indices] == test_labels[dog_indices]) / np.sum(dog_indices)\n",
    "        \n",
    "        print(f\"\\n📈 클래스별 정확도:\")\n",
    "        print(f\"   고양이(Cat) 정확도: {cat_accuracy:.4f} ({cat_accuracy*100:.2f}%)\")\n",
    "        print(f\"   개(Dog) 정확도: {dog_accuracy:.4f} ({dog_accuracy*100:.2f}%)\")\n",
    "        \n",
    "        return test_predictions, test_pred_classes\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ 저장된 모델이 없습니다. train_model()을 먼저 실행하세요!\")\n",
    "        return None\n",
    "\n",
    "print(\"예측 및 평가 함수 정의 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. 전체 파이프라인 실행\n",
    "\n",
    "Transfer Learning 프로젝트의 전체 과정을 단계별로 실행합니다.\n",
    "\n",
    "### 실행 순서:\n",
    "1. **특성 추출**: VGG19로 모든 이미지의 특성 추출 및 저장\n",
    "2. **모델 학습**: 추출된 특성으로 분류 모델 학습\n",
    "3. **성능 평가**: 테스트 데이터로 최종 성능 평가\n",
    "\n",
    "### 주의사항:\n",
    "- 특성 추출은 시간이 오래 걸릴 수 있습니다 (GPU 사용 권장)\n",
    "- 한 번 추출한 특성은 재사용 가능합니다\n",
    "- 각 단계는 독립적으로 실행 가능합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계: VGG19 특성 추출 및 저장\n",
    "print(\"🔄 1단계: VGG19 특성 추출 시작...\")\n",
    "print(\"※ 이 과정은 시간이 오래 걸릴 수 있습니다. (약 5-10분)\")\n",
    "\n",
    "# 특성 추출 실행\n",
    "save_features()\n",
    "\n",
    "print(\"✅ 1단계 완료: 모든 특성이 '개고양이특성.bin' 파일로 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계: 분류 모델 학습\n",
    "print(\"🔄 2단계: 분류 모델 학습 시작...\")\n",
    "\n",
    "# 모델 학습 실행\n",
    "model, history = train_model()\n",
    "\n",
    "if model is not None:\n",
    "    print(\"✅ 2단계 완료: 모델이 '특성추출.keras' 파일로 저장되었습니다!\")\n",
    "else:\n",
    "    print(\"❌ 2단계 실패: 특성 파일을 먼저 생성하세요!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3단계: 모델 예측 및 성능 평가\n",
    "print(\"🔄 3단계: 모델 성능 평가 시작...\")\n",
    "\n",
    "# 예측 및 평가 실행\n",
    "predictions = predict_and_evaluate()\n",
    "\n",
    "if predictions is not None:\n",
    "    print(\"✅ 3단계 완료: 모델 성능 평가가 완료되었습니다!\")\n",
    "else:\n",
    "    print(\"❌ 3단계 실패: 학습된 모델이 없습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. 추가 기능 및 시각화\n",
    "\n",
    "학습 과정과 결과를 시각화하고 분석합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    학습 과정의 정확도와 손실을 시각화\n",
    "    \n",
    "    Args:\n",
    "        history: model.fit()에서 반환된 History 객체\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        print(\"❌ 학습 기록이 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # 정확도 그래프\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 손실 그래프\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 학습 기록이 있다면 시각화\n",
    "try:\n",
    "    if 'history' in locals() and history is not None:\n",
    "        print(\"📈 학습 과정 시각화:\")\n",
    "        plot_training_history(history)\n",
    "    else:\n",
    "        print(\"ℹ️ 학습 기록을 시각화하려면 먼저 모델을 학습하세요.\")\n",
    "except:\n",
    "    print(\"ℹ️ 학습 기록을 시각화하려면 먼저 모델을 학습하세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. 결론 및 다음 단계\n",
    "\n",
    "### 완료된 작업:\n",
    "✅ **Transfer Learning 구현**: VGG19 사전학습 모델 활용  \n",
    "✅ **투스테이지 방식**: 특성 추출 후 분류 모델 학습  \n",
    "✅ **데이터 전처리**: 체계적인 Train/Val/Test 분할  \n",
    "✅ **모델 최적화**: ModelCheckpoint로 최적 모델 자동 저장  \n",
    "✅ **성능 평가**: 테스트 데이터로 정확도 측정  \n",
    "\n",
    "### Transfer Learning의 장점:\n",
    "1. **빠른 학습**: CNN 부분 재사용으로 학습 시간 단축\n",
    "2. **높은 성능**: 사전학습된 특성으로 좋은 성능 달성\n",
    "3. **적은 데이터**: 소규모 데이터셋으로도 효과적\n",
    "4. **효율성**: 제한된 컴퓨팅 자원으로도 학습 가능\n",
    "\n",
    "### 다음 단계 제안:\n",
    "1. **인라인 방식 구현**:\n",
    "   - VGG19를 전체 모델에 포함하여 end-to-end 학습\n",
    "   - 원본 이미지에 직접 데이터 증강 적용\n",
    "   - Fine-tuning으로 VGG19 상위 레이어도 함께 학습\n",
    "\n",
    "2. **모델 개선**:\n",
    "   - 다른 사전학습 모델 시도 (ResNet, EfficientNet, MobileNet)\n",
    "   - 하이퍼파라미터 최적화 (학습률, 배치 크기, 드롭아웃 비율)\n",
    "   - 교차 검증으로 더 신뢰성 있는 평가\n",
    "\n",
    "3. **고급 기법 적용**:\n",
    "   - Confusion Matrix 및 분류 보고서 생성\n",
    "   - Grad-CAM으로 모델 판단 근거 시각화\n",
    "   - 앙상블 모델로 성능 향상\n",
    "\n",
    "4. **실제 배포**:\n",
    "   - 웹 애플리케이션으로 실시간 이미지 분류\n",
    "   - REST API 서버 구축\n",
    "   - 모바일 앱 연동\n",
    "\n",
    "### 학습한 핵심 개념:\n",
    "- **Transfer Learning**: 사전학습 모델 재활용\n",
    "- **Feature Extraction**: CNN을 특성 추출기로 활용\n",
    "- **투스테이지 vs 인라인**: 두 가지 Transfer Learning 방식\n",
    "- **모델 저장/로드**: 최적 모델 관리 및 재사용\n",
    "- **이진 분류**: Sigmoid 활성화와 Binary Crossentropy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
