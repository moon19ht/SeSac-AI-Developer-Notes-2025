{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Transfer Learningì„ í™œìš©í•œ ê°œ-ê³ ì–‘ì´ ë¶„ë¥˜ í”„ë¡œì íŠ¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì‚¬ì „í•™ìŠµëœ VGG19 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°œì™€ ê³ ì–‘ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” Transfer Learning í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "- **ëª©í‘œ**: VGG19 ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ê°œ-ê³ ì–‘ì´ ì´ì§„ ë¶„ë¥˜\n",
    "- **ë°©ë²•**: Transfer Learning (íŠ¹ì„± ì¶”ì¶œ ë°©ì‹)\n",
    "- **ë°ì´í„°**: Cats and Dogs ë°ì´í„°ì…‹\n",
    "- **í”„ë ˆì„ì›Œí¬**: TensorFlow/Keras\n",
    "\n",
    "## Transfer Learningì´ë€?\n",
    "ì´ë¯¸ ìˆ˜ì‹­ë§Œ ì¥ì˜ ì´ë¯¸ì§€ë¡œ í•™ìŠµëœ ëª¨ë¸(VGG19)ì„ ì¬í™œìš©í•˜ì—¬:\n",
    "- **í•™ìŠµ ì‹œê°„ ë‹¨ì¶•**: CNN ë¶€ë¶„ì€ ë™ê²°í•˜ê³  ë¶„ë¥˜ì¸µë§Œ í•™ìŠµ\n",
    "- **ë†’ì€ ì„±ëŠ¥**: ì ì€ ë°ì´í„°ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±\n",
    "- **íš¨ìœ¨ì„±**: ì»´í“¨í„° ìì›ì´ ì ì–´ë„ í•™ìŠµ ê°€ëŠ¥\n",
    "\n",
    "## ë‘ ê°€ì§€ Transfer Learning ë°©ì‹\n",
    "\n",
    "### 1. íˆ¬ìŠ¤í…Œì´ì§€ ë°©ì‹ (Two-Stage)\n",
    "- **ë°©ë²•**: VGG19ì˜ íŠ¹ì§•ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ê³  numpy ë°°ì—´ë¡œ ì €ì¥ â†’ ë¶„ë¥˜ í•™ìŠµ\n",
    "- **ì¥ì **: í›ˆë ¨ ì†ë„ê°€ ë§¤ìš° ë¹ ë¦„\n",
    "- **ë‹¨ì **: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ, ë°ì´í„° ì¦ê°• ì œí•œì \n",
    "\n",
    "### 2. ì¸ë¼ì¸ ë°©ì‹ (Inline)  \n",
    "- **ë°©ë²•**: VGG19 íŠ¹ì§• ì¶”ì¶œë¶€ë¥¼ ì „ì²´ ëª¨ë¸ì— í¬í•¨í•˜ì—¬ í•™ìŠµ\n",
    "- **ì¥ì **: ì›ë³¸ ì´ë¯¸ì§€ì— ë°ì´í„° ì¦ê°• ì§ì ‘ ì ìš©, ê³¼ì í•© ë°©ì§€\n",
    "- **ë‹¨ì **: íˆ¬ìŠ¤í…Œì´ì§€ë³´ë‹¤ ì†ë„ ëŠë¦¼\n",
    "\n",
    "**ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” íˆ¬ìŠ¤í…Œì´ì§€ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë”¥ëŸ¬ë‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras import models, layers\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gdown  # êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œìš©\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"Keras ë²„ì „: {keras.__version__}\")\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ì œê±° (ì„ íƒì‚¬í•­)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… GPU/CPU ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f\"âœ… GPU({tf.test.gpu_device_name()}) ì‚¬ìš© ì„¤ì • ì™„ë£Œ\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        DEVICE = '/GPU:0'\n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # GPU ì‚¬ìš© ë°©ì§€\n",
    "    DEVICE = '/CPU:0'\n",
    "    print(\"âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"í˜„ì¬ ì‚¬ìš© ë””ë°”ì´ìŠ¤:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• \n",
    "\n",
    "Cats and Dogs ë°ì´í„°ì…‹ì„ Train/Validation/Testë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ë°ì´í„° êµ¬ì¡°:\n",
    "- **ì›ë³¸**: `cats_and_dogs/train/` (ì´ 25,000ì¥)\n",
    "- **ë¶„í•  í›„**: \n",
    "  - Train: 0~999ë²ˆ ì´ë¯¸ì§€ (ê° í´ë˜ìŠ¤ 1,000ì¥)\n",
    "  - Validation: 1000~1499ë²ˆ ì´ë¯¸ì§€ (ê° í´ë˜ìŠ¤ 500ì¥)\n",
    "  - Test: 1500~1999ë²ˆ ì´ë¯¸ì§€ (ê° í´ë˜ìŠ¤ 500ì¥)\n",
    "\n",
    "### í´ë” êµ¬ì¡°:\n",
    "```\n",
    "cats_and_dogs_small/\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ cat/\n",
    "â”‚   â””â”€â”€ dog/\n",
    "â”œâ”€â”€ validation/\n",
    "â”‚   â”œâ”€â”€ cat/\n",
    "â”‚   â””â”€â”€ dog/\n",
    "â””â”€â”€ test/\n",
    "    â”œâ”€â”€ cat/\n",
    "    â””â”€â”€ dog/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "original_dir = pathlib.Path(\"../../data/cats_and_dogs/train\")\n",
    "new_base_dir = pathlib.Path(\"../../data/cats_and_dogs/cats_and_dogs_small\")\n",
    "\n",
    "print(\"ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ì›ë³¸ ë°ì´í„°: {original_dir}\")\n",
    "print(f\"ë¶„í•  ì €ì¥: {new_base_dir}\")\n",
    "\n",
    "# ì„ íƒì‚¬í•­: êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "# gdown.download(id='18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd', output='dogs-vs-cats.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subset(subset_name, start_index, end_index):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ë²”ìœ„ì˜ ì´ë¯¸ì§€ë“¤ì„ ì§€ì •ëœ í´ë”ë¡œ ë³µì‚¬\n",
    "    \n",
    "    Args:\n",
    "        subset_name: 'train', 'validation', 'test'\n",
    "        start_index: ì‹œì‘ ì¸ë±ìŠ¤\n",
    "        end_index: ë ì¸ë±ìŠ¤ (í¬í•¨ë˜ì§€ ì•ŠìŒ)\n",
    "    \"\"\"\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        # ëª©ì ì§€ í´ë” ìƒì„±\n",
    "        dir_path = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì˜ˆ: cat.0.jpg, cat.1.jpg, ...)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        \n",
    "        # íŒŒì¼ ë³µì‚¬\n",
    "        for fname in fnames:\n",
    "            src_path = original_dir / fname\n",
    "            dst_path = dir_path / fname\n",
    "            if src_path.exists():  # ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ë³µì‚¬\n",
    "                shutil.copyfile(src_path, dst_path)\n",
    "        \n",
    "        print(f\"âœ… {subset_name}/{category}: {len(fnames)}ê°œ ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ\")\n",
    "\n",
    "print(\"ë°ì´í„° ë¶„í•  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„í•  ì‹¤í–‰\n",
    "print(\"ğŸ”„ ë°ì´í„° ë¶„í•  ì‹œì‘...\")\n",
    "\n",
    "# Train ì„¸íŠ¸: ê° í´ë˜ìŠ¤ 1,000ì¥ (0~999)\n",
    "make_subset(\"train\", 0, 1000)\n",
    "\n",
    "# Validation ì„¸íŠ¸: ê° í´ë˜ìŠ¤ 500ì¥ (1000~1499)\n",
    "make_subset(\"validation\", 1000, 1500)\n",
    "\n",
    "# Test ì„¸íŠ¸: ê° í´ë˜ìŠ¤ 500ì¥ (1500~1999)\n",
    "make_subset(\"test\", 1500, 2000)\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ë°ì´í„° ë¶„í•  ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ì´ ë°ì´í„°: Train({1000*2}), Validation({500*2}), Test({500*2})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¡œë” ìƒì„±\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16,\n",
    "    label_mode='binary'  # ì´ì§„ ë¶„ë¥˜ (0: cat, 1: dog)\n",
    ")\n",
    "\n",
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=16,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ì…‹ ë¡œë” ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ Train: {train_ds}\")\n",
    "print(f\"ğŸ“ Validation: {validation_ds}\")\n",
    "print(f\"ğŸ“ Test: {test_ds}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ì´ë¦„ í™•ì¸\n",
    "class_names = train_ds.class_names\n",
    "print(f\"ğŸ·ï¸ í´ë˜ìŠ¤: {class_names}\")  # ['cat', 'dog']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. VGG19 ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ë° íŠ¹ì„± ì¶”ì¶œ\n",
    "\n",
    "ImageNetìœ¼ë¡œ ì‚¬ì „í•™ìŠµëœ VGG19 ëª¨ë¸ì„ ë¡œë“œí•˜ê³  íŠ¹ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "### VGG19 ëª¨ë¸ íŠ¹ì§•:\n",
    "- **ImageNet**: 1400ë§Œ ì¥ì˜ ì´ë¯¸ì§€, 1000ê°œ í´ë˜ìŠ¤ë¡œ í•™ìŠµë¨\n",
    "- **êµ¬ì¡°**: 16ê°œ Conv ë ˆì´ì–´ + 3ê°œ FC ë ˆì´ì–´ = ì´ 19ê°œ ë ˆì´ì–´\n",
    "- **íŠ¹ì„± ì¶”ì¶œ**: ë§ˆì§€ë§‰ Conv ë¸”ë¡(block5_pool)ì—ì„œ íŠ¹ì„± ë²¡í„° ì¶”ì¶œ\n",
    "- **ì¶œë ¥ í¬ê¸°**: (5, 5, 512) â†’ ê° ì´ë¯¸ì§€ë‹¹ 12,800ì°¨ì› íŠ¹ì„± ë²¡í„°\n",
    "\n",
    "### Transfer Learning ê³¼ì •:\n",
    "1. VGG19 ëª¨ë¸ ë¡œë“œ (ê°€ì¤‘ì¹˜ ë™ê²°)\n",
    "2. ê° ì´ë¯¸ì§€ë¥¼ VGG19ì— í†µê³¼ì‹œì¼œ íŠ¹ì„± ì¶”ì¶œ\n",
    "3. ì¶”ì¶œëœ íŠ¹ì„±ìœ¼ë¡œ ìƒˆë¡œìš´ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ\n",
    "conv_base = keras.applications.vgg19.VGG19(\n",
    "    weights=\"imagenet\",        # ImageNet ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
    "    include_top=False,         # ìµœìƒìœ„ ë¶„ë¥˜ì¸µ ì œì™¸ (CNN ë¶€ë¶„ë§Œ ì‚¬ìš©)\n",
    "    input_shape=(180, 180, 3)  # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° ì§€ì •\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ VGG19 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ëª¨ë¸ ì…ë ¥ í¬ê¸°: {conv_base.input_shape}\")\n",
    "print(f\"ğŸ“Š ëª¨ë¸ ì¶œë ¥ í¬ê¸°: {conv_base.output_shape}\")\n",
    "\n",
    "# VGG19 ëª¨ë¸ êµ¬ì¡° ìš”ì•½\n",
    "conv_base.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(dataset):\n",
    "    \"\"\"\n",
    "    VGG19 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì„±ê³¼ ë¼ë²¨ì„ ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        dataset: tf.data.Dataset ê°ì²´\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (features, labels) numpy ë°°ì—´\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"ğŸ”„ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataset):\n",
    "        # VGG19 ì „ì²˜ë¦¬ (ImageNet ì •ê·œí™”)\n",
    "        preprocessed_images = keras.applications.vgg19.preprocess_input(images)\n",
    "        \n",
    "        # VGG19ìœ¼ë¡œ íŠ¹ì„± ì¶”ì¶œ\n",
    "        features = conv_base.predict(preprocessed_images, verbose=0)\n",
    "        \n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        # ì§„í–‰ìƒí™© ì¶œë ¥\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"   ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì™„ë£Œ...\")\n",
    "    \n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "print(\"íŠ¹ì„± ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features():\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ê³  íŒŒì¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“¥ ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì„± ì¶”ì¶œ ì‹œì‘...\")\n",
    "    \n",
    "    # ê° ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì„± ì¶”ì¶œ\n",
    "    print(\"\\n1ï¸âƒ£ Train ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ\")\n",
    "    train_features, train_labels = get_features_and_labels(train_ds)\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£ Validation ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ\")\n",
    "    validation_features, validation_labels = get_features_and_labels(validation_ds)\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£ Test ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ\")\n",
    "    test_features, test_labels = get_features_and_labels(test_ds)\n",
    "    \n",
    "    # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ê¸°\n",
    "    data = [\n",
    "        train_features, train_labels,\n",
    "        validation_features, validation_labels,\n",
    "        test_features, test_labels\n",
    "    ]\n",
    "    \n",
    "    # pickleë¡œ ì €ì¥\n",
    "    with open(\"ê°œê³ ì–‘ì´íŠ¹ì„±.bin\", \"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "    \n",
    "    print(\"\\nâœ… íŠ¹ì„± ì¶”ì¶œ ë° ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ ì €ì¥ íŒŒì¼: ê°œê³ ì–‘ì´íŠ¹ì„±.bin\")\n",
    "    print(f\"ğŸ“Š íŠ¹ì„± í¬ê¸°: {train_features.shape}\")\n",
    "\n",
    "\n",
    "def load_features():\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ íŠ¹ì„± íŒŒì¼ì„ ë¡œë“œ\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_features, train_labels, val_features, val_labels, test_features, test_labels)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"ê°œê³ ì–‘ì´íŠ¹ì„±.bin\", \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        print(\"âœ… ì €ì¥ëœ íŠ¹ì„± íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\")\n",
    "        return data[0], data[1], data[2], data[3], data[4], data[5]\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ íŠ¹ì„± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. save_features()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "        return None\n",
    "\n",
    "print(\"íŠ¹ì„± ì €ì¥/ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ\n",
    "\n",
    "VGG19ì—ì„œ ì¶”ì¶œí•œ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê°œ-ê³ ì–‘ì´ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ëª¨ë¸ êµ¬ì¡°:\n",
    "1. **ì…ë ¥**: VGG19 íŠ¹ì„± (5, 5, 512)\n",
    "2. **ë°ì´í„° ì¦ê°•**: RandomFlip, RandomRotation, RandomZoom\n",
    "3. **ë ˆì´ì–´ë“¤**:\n",
    "   - Flatten: íŠ¹ì„±ì„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "   - Dense(256): ì²« ë²ˆì§¸ ì™„ì „ì—°ê²°ì¸µ\n",
    "   - Dense(128): ë‘ ë²ˆì§¸ ì™„ì „ì—°ê²°ì¸µ  \n",
    "   - Dropout(0.5): ê³¼ì í•© ë°©ì§€\n",
    "   - Dense(1, sigmoid): ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ\n",
    "\n",
    "### í•™ìŠµ ì„¤ì •:\n",
    "- **ì˜µí‹°ë§ˆì´ì €**: RMSprop\n",
    "- **ì†ì‹¤ í•¨ìˆ˜**: Binary Crossentropy\n",
    "- **í‰ê°€ ì§€í‘œ**: Accuracy\n",
    "- **ì½œë°±**: ModelCheckpoint (ìµœì  ëª¨ë¸ ìë™ ì €ì¥)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model():\n",
    "    \"\"\"\n",
    "    VGG19 íŠ¹ì„±ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ë¶„ë¥˜ ëª¨ë¸ ìƒì„±\n",
    "    \n",
    "    Returns:\n",
    "        keras.Model: ì»´íŒŒì¼ëœ ë¶„ë¥˜ ëª¨ë¸\n",
    "    \"\"\"\n",
    "    # ë°ì´í„° ì¦ê°• ë ˆì´ì–´ ì •ì˜\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),     # ìˆ˜í‰ ë’¤ì§‘ê¸°\n",
    "        layers.RandomRotation(0.1),         # Â±10% íšŒì „ (36ë„)\n",
    "        layers.RandomZoom(0.1),             # Â±10% í™•ëŒ€/ì¶•ì†Œ\n",
    "    ])\n",
    "    \n",
    "    # ëª¨ë¸ êµ¬ì¡° ì •ì˜\n",
    "    inputs = keras.Input(shape=(5, 5, 512))  # VGG19 ì¶œë ¥ í¬ê¸°\n",
    "    x = data_augmentation(inputs)\n",
    "    x = layers.Flatten()(x)                  # 12,800 ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)               # ê³¼ì í•© ë°©ì§€\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  # ì´ì§„ ë¶„ë¥˜\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # ëª¨ë¸ ì»´íŒŒì¼\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ë¶„ë¥˜ ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "    return model\n",
    "\n",
    "print(\"ë¶„ë¥˜ ëª¨ë¸ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    # ì €ì¥ëœ íŠ¹ì„± ë¡œë“œ\n",
    "    features_data = load_features()\n",
    "    if features_data is None:\n",
    "        print(\"âŒ íŠ¹ì„± íŒŒì¼ì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”!\")\n",
    "        return None\n",
    "    \n",
    "    train_features, train_labels, validation_features, validation_labels, test_features, test_labels = features_data\n",
    "    \n",
    "    print(f\"ğŸ“Š ë°ì´í„° í¬ê¸°:\")\n",
    "    print(f\"   Train: {train_features.shape} / {train_labels.shape}\")\n",
    "    print(f\"   Validation: {validation_features.shape} / {validation_labels.shape}\")\n",
    "    print(f\"   Test: {test_features.shape} / {test_labels.shape}\")\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    model = create_classification_model()\n",
    "    model.summary()\n",
    "    \n",
    "    # ì½œë°± ì„¤ì • (ìµœì  ëª¨ë¸ ìë™ ì €ì¥)\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"íŠ¹ì„±ì¶”ì¶œ.keras\",\n",
    "            save_best_only=True,        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ ì €ì¥\n",
    "            monitor=\"val_loss\",         # ê²€ì¦ ì†ì‹¤ ê¸°ì¤€\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    \n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    history = model.fit(\n",
    "        train_features, train_labels,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(validation_features, validation_labels),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ìµœì¢… ì„±ëŠ¥ ì¶œë ¥\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥:\")\n",
    "    print(f\"   í›ˆë ¨ ì •í™•ë„: {final_train_acc:.4f}\")\n",
    "    print(f\"   ê²€ì¦ ì •í™•ë„: {final_val_acc:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. ëª¨ë¸ ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í‰ê°€ ë°©ë²•:\n",
    "1. **ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ**: ìµœì  ì„±ëŠ¥ ëª¨ë¸(`íŠ¹ì„±ì¶”ì¶œ.keras`) ë¡œë“œ\n",
    "2. **í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡**: í…ŒìŠ¤íŠ¸ íŠ¹ì„±ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "3. **ì´ì§„ ë¶„ë¥˜ ë³€í™˜**: í™•ë¥ ê°’(0~1)ì„ í´ë˜ìŠ¤(0: cat, 1: dog)ë¡œ ë³€í™˜\n",
    "4. **ì •í™•ë„ ê³„ì‚°**: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ\n",
    "\n",
    "### ì˜ˆì¸¡ ì„ê³„ê°’:\n",
    "- **0.5 ì´ìƒ**: ê°œ(dog) í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜\n",
    "- **0.5 ë¯¸ë§Œ**: ê³ ì–‘ì´(cat) í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate():\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
    "        model = keras.models.load_model(\"íŠ¹ì„±ì¶”ì¶œ.keras\")\n",
    "        print(\"âœ… ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "        \n",
    "        # ì €ì¥ëœ íŠ¹ì„± ë¡œë“œ\n",
    "        features_data = load_features()\n",
    "        if features_data is None:\n",
    "            return\n",
    "        \n",
    "        train_features, train_labels, validation_features, validation_labels, test_features, test_labels = features_data\n",
    "        \n",
    "        print(f\"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
    "        print(f\"   í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_features.shape}\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "        test_predictions = model.predict(test_features, verbose=0)\n",
    "        \n",
    "        # í™•ë¥ ê°’ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜ (0.5 ê¸°ì¤€)\n",
    "        test_pred_classes = (test_predictions > 0.5).astype(\"int\").flatten()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 20ê°œ):\")\n",
    "        print(f\"   ì˜ˆì¸¡ê°’: {test_pred_classes[:20]}\")\n",
    "        print(f\"   ì‹¤ì œê°’: {test_labels[:20].astype(int)}\")\n",
    "        \n",
    "        # ì •í™•ë„ ê³„ì‚°\n",
    "        correct_predictions = np.sum(test_pred_classes == test_labels.astype(int))\n",
    "        total_predictions = len(test_labels)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:\")\n",
    "        print(f\"   ì „ì²´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {total_predictions}ê°œ\")\n",
    "        print(f\"   ì •í™•íˆ ì˜ˆì¸¡í•œ ìˆ˜: {correct_predictions}ê°œ\")\n",
    "        print(f\"   í‹€ë¦¬ê²Œ ì˜ˆì¸¡í•œ ìˆ˜: {total_predictions - correct_predictions}ê°œ\")\n",
    "        print(f\"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ë³„ ì„¸ë¶€ ë¶„ì„\n",
    "        cat_indices = test_labels == 0\n",
    "        dog_indices = test_labels == 1\n",
    "        \n",
    "        cat_accuracy = np.sum(test_pred_classes[cat_indices] == test_labels[cat_indices]) / np.sum(cat_indices)\n",
    "        dog_accuracy = np.sum(test_pred_classes[dog_indices] == test_labels[dog_indices]) / np.sum(dog_indices)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì •í™•ë„:\")\n",
    "        print(f\"   ê³ ì–‘ì´(Cat) ì •í™•ë„: {cat_accuracy:.4f} ({cat_accuracy*100:.2f}%)\")\n",
    "        print(f\"   ê°œ(Dog) ì •í™•ë„: {dog_accuracy:.4f} ({dog_accuracy*100:.2f}%)\")\n",
    "        \n",
    "        return test_predictions, test_pred_classes\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”!\")\n",
    "        return None\n",
    "\n",
    "print(\"ì˜ˆì¸¡ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "\n",
    "Transfer Learning í”„ë¡œì íŠ¸ì˜ ì „ì²´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì‹¤í–‰ ìˆœì„œ:\n",
    "1. **íŠ¹ì„± ì¶”ì¶œ**: VGG19ë¡œ ëª¨ë“  ì´ë¯¸ì§€ì˜ íŠ¹ì„± ì¶”ì¶œ ë° ì €ì¥\n",
    "2. **ëª¨ë¸ í•™ìŠµ**: ì¶”ì¶œëœ íŠ¹ì„±ìœ¼ë¡œ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "3. **ì„±ëŠ¥ í‰ê°€**: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "### ì£¼ì˜ì‚¬í•­:\n",
    "- íŠ¹ì„± ì¶”ì¶œì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (GPU ì‚¬ìš© ê¶Œì¥)\n",
    "- í•œ ë²ˆ ì¶”ì¶œí•œ íŠ¹ì„±ì€ ì¬ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
    "- ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ë‹¨ê³„: VGG19 íŠ¹ì„± ì¶”ì¶œ ë° ì €ì¥\n",
    "print(\"ğŸ”„ 1ë‹¨ê³„: VGG19 íŠ¹ì„± ì¶”ì¶œ ì‹œì‘...\")\n",
    "print(\"â€» ì´ ê³¼ì •ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì•½ 5-10ë¶„)\")\n",
    "\n",
    "# íŠ¹ì„± ì¶”ì¶œ ì‹¤í–‰\n",
    "save_features()\n",
    "\n",
    "print(\"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ëª¨ë“  íŠ¹ì„±ì´ 'ê°œê³ ì–‘ì´íŠ¹ì„±.bin' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ë‹¨ê³„: ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ğŸ”„ 2ë‹¨ê³„: ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "model, history = train_model()\n",
    "\n",
    "if model is not None:\n",
    "    print(\"âœ… 2ë‹¨ê³„ ì™„ë£Œ: ëª¨ë¸ì´ 'íŠ¹ì„±ì¶”ì¶œ.keras' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âŒ 2ë‹¨ê³„ ì‹¤íŒ¨: íŠ¹ì„± íŒŒì¼ì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ë‹¨ê³„: ëª¨ë¸ ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€\n",
    "print(\"ğŸ”„ 3ë‹¨ê³„: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\")\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€ ì‹¤í–‰\n",
    "predictions = predict_and_evaluate()\n",
    "\n",
    "if predictions is not None:\n",
    "    print(\"âœ… 3ë‹¨ê³„ ì™„ë£Œ: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âŒ 3ë‹¨ê³„ ì‹¤íŒ¨: í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. ì¶”ê°€ ê¸°ëŠ¥ ë° ì‹œê°í™”\n",
    "\n",
    "í•™ìŠµ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ê³¼ì •ì˜ ì •í™•ë„ì™€ ì†ì‹¤ì„ ì‹œê°í™”\n",
    "    \n",
    "    Args:\n",
    "        history: model.fit()ì—ì„œ ë°˜í™˜ëœ History ê°ì²´\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        print(\"âŒ í•™ìŠµ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # ì •í™•ë„ ê·¸ë˜í”„\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # ì†ì‹¤ ê·¸ë˜í”„\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡ì´ ìˆë‹¤ë©´ ì‹œê°í™”\n",
    "try:\n",
    "    if 'history' in locals() and history is not None:\n",
    "        print(\"ğŸ“ˆ í•™ìŠµ ê³¼ì • ì‹œê°í™”:\")\n",
    "        plot_training_history(history)\n",
    "    else:\n",
    "        print(\"â„¹ï¸ í•™ìŠµ ê¸°ë¡ì„ ì‹œê°í™”í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.\")\n",
    "except:\n",
    "    print(\"â„¹ï¸ í•™ìŠµ ê¸°ë¡ì„ ì‹œê°í™”í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ì™„ë£Œëœ ì‘ì—…:\n",
    "âœ… **Transfer Learning êµ¬í˜„**: VGG19 ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©  \n",
    "âœ… **íˆ¬ìŠ¤í…Œì´ì§€ ë°©ì‹**: íŠ¹ì„± ì¶”ì¶œ í›„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ  \n",
    "âœ… **ë°ì´í„° ì „ì²˜ë¦¬**: ì²´ê³„ì ì¸ Train/Val/Test ë¶„í•   \n",
    "âœ… **ëª¨ë¸ ìµœì í™”**: ModelCheckpointë¡œ ìµœì  ëª¨ë¸ ìë™ ì €ì¥  \n",
    "âœ… **ì„±ëŠ¥ í‰ê°€**: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì •í™•ë„ ì¸¡ì •  \n",
    "\n",
    "### Transfer Learningì˜ ì¥ì :\n",
    "1. **ë¹ ë¥¸ í•™ìŠµ**: CNN ë¶€ë¶„ ì¬ì‚¬ìš©ìœ¼ë¡œ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•\n",
    "2. **ë†’ì€ ì„±ëŠ¥**: ì‚¬ì „í•™ìŠµëœ íŠ¹ì„±ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±\n",
    "3. **ì ì€ ë°ì´í„°**: ì†Œê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œë„ íš¨ê³¼ì \n",
    "4. **íš¨ìœ¨ì„±**: ì œí•œëœ ì»´í“¨íŒ… ìì›ìœ¼ë¡œë„ í•™ìŠµ ê°€ëŠ¥\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\n",
    "1. **ì¸ë¼ì¸ ë°©ì‹ êµ¬í˜„**:\n",
    "   - VGG19ë¥¼ ì „ì²´ ëª¨ë¸ì— í¬í•¨í•˜ì—¬ end-to-end í•™ìŠµ\n",
    "   - ì›ë³¸ ì´ë¯¸ì§€ì— ì§ì ‘ ë°ì´í„° ì¦ê°• ì ìš©\n",
    "   - Fine-tuningìœ¼ë¡œ VGG19 ìƒìœ„ ë ˆì´ì–´ë„ í•¨ê»˜ í•™ìŠµ\n",
    "\n",
    "2. **ëª¨ë¸ ê°œì„ **:\n",
    "   - ë‹¤ë¥¸ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‹œë„ (ResNet, EfficientNet, MobileNet)\n",
    "   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨)\n",
    "   - êµì°¨ ê²€ì¦ìœ¼ë¡œ ë” ì‹ ë¢°ì„± ìˆëŠ” í‰ê°€\n",
    "\n",
    "3. **ê³ ê¸‰ ê¸°ë²• ì ìš©**:\n",
    "   - Confusion Matrix ë° ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±\n",
    "   - Grad-CAMìœ¼ë¡œ ëª¨ë¸ íŒë‹¨ ê·¼ê±° ì‹œê°í™”\n",
    "   - ì•™ìƒë¸” ëª¨ë¸ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
    "\n",
    "4. **ì‹¤ì œ ë°°í¬**:\n",
    "   - ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ì´ë¯¸ì§€ ë¶„ë¥˜\n",
    "   - REST API ì„œë²„ êµ¬ì¶•\n",
    "   - ëª¨ë°”ì¼ ì•± ì—°ë™\n",
    "\n",
    "### í•™ìŠµí•œ í•µì‹¬ ê°œë…:\n",
    "- **Transfer Learning**: ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì¬í™œìš©\n",
    "- **Feature Extraction**: CNNì„ íŠ¹ì„± ì¶”ì¶œê¸°ë¡œ í™œìš©\n",
    "- **íˆ¬ìŠ¤í…Œì´ì§€ vs ì¸ë¼ì¸**: ë‘ ê°€ì§€ Transfer Learning ë°©ì‹\n",
    "- **ëª¨ë¸ ì €ì¥/ë¡œë“œ**: ìµœì  ëª¨ë¸ ê´€ë¦¬ ë° ì¬ì‚¬ìš©\n",
    "- **ì´ì§„ ë¶„ë¥˜**: Sigmoid í™œì„±í™”ì™€ Binary Crossentropy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
