{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ê½ƒ ì´ë¯¸ì§€ ë¶„ë¥˜ ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ ê½ƒ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” CNN ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "## í”„ë¡œì íŠ¸ ê°œìš”\n",
        "- **ëª©í‘œ**: 5ê°€ì§€ ê½ƒ ì¢…ë¥˜(daisy, dandelion, rose, sunflower, tulip)ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ ê°œë°œ\n",
        "- **ë°©ë²•**: Convolutional Neural Network (CNN) ì‚¬ìš©\n",
        "- **ë°ì´í„°**: ê½ƒ ì´ë¯¸ì§€ ë°ì´í„°ì…‹\n",
        "- **í”„ë ˆì„ì›Œí¬**: TensorFlow/Keras\n",
        "\n",
        "## í”„ë¡œì íŠ¸ êµ¬ì¡°\n",
        "1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •\n",
        "2. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "3. ë°ì´í„°ì…‹ êµ¬ì„± ë° ë¶„í• \n",
        "4. ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•\n",
        "5. ëª¨ë¸ í•™ìŠµ ë° ì €ì¥\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •\n",
        "\n",
        "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë”¥ëŸ¬ë‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "from keras.preprocessing import image\n",
        "from keras.models import load_model \n",
        "from keras import models, layers\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import PIL.Image as pilimg \n",
        "\n",
        "# íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import os\n",
        "import shutil\n",
        "import random \n",
        "import imghdr\n",
        "import pickle\n",
        "\n",
        "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
        "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
        "print(f\"Keras ë²„ì „: {keras.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "\n",
        "ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ì •ë¦¬í•˜ê³  ë¶„í• í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ì£¼ìš” ê¸°ëŠ¥:\n",
        "- **ì´ë¯¸ì§€ ë¦¬ë„¤ì´ë°**: í´ë˜ìŠ¤ë³„ë¡œ ì¼ê´€ëœ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½\n",
        "- **ë°ì´í„° ë¶„í• **: Train/Validation/Test ì„¸íŠ¸ë¡œ ë¶„í•  (50:25:25 ë¹„ìœ¨)\n",
        "- **ë””ë ‰í† ë¦¬ êµ¬ì„±**: ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í´ë” êµ¬ì¡° ìƒì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_images_in_class_folder(src_class_dir, class_name, dest_dir):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • í´ë˜ìŠ¤ í´ë”ì˜ ì´ë¯¸ì§€ë“¤ì„ 'class_name.ì¸ë±ìŠ¤.í™•ì¥ì' í˜•ì‹ìœ¼ë¡œ ë¦¬ë„¤ì´ë°\n",
        "    \n",
        "    Args:\n",
        "        src_class_dir: ì›ë³¸ í´ë˜ìŠ¤ í´ë” ê²½ë¡œ\n",
        "        class_name: í´ë˜ìŠ¤ëª… (ì˜ˆ: 'daisy')\n",
        "        dest_dir: ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ê°€ ì €ì¥ë  í´ë”\n",
        "    \"\"\"\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    # ì´ë¯¸ì§€ íŒŒì¼ë§Œ í•„í„°ë§\n",
        "    files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    files.sort()\n",
        "\n",
        "    for idx, fname in enumerate(files):\n",
        "        src_path = os.path.join(src_class_dir, fname)\n",
        "        ext = os.path.splitext(fname)[1].lower()\n",
        "        new_name = f\"{class_name}.{idx}{ext}\"\n",
        "        dst_path = os.path.join(dest_dir, new_name)\n",
        "        shutil.copyfile(src_path, dst_path)\n",
        "\n",
        "    print(f\"âœ… {class_name} ë¦¬ë„¤ì„ ì™„ë£Œ: {len(files)}ê°œ ì²˜ë¦¬ë¨.\")\n",
        "\n",
        "\n",
        "def rename_all_classes(original_root_dir, renamed_root_dir):\n",
        "    \"\"\"\n",
        "    ëª¨ë“  í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ë“¤ì„ ë¦¬ë„¤ì´ë°í•˜ì—¬ í•˜ë‚˜ì˜ í´ë”ì— í†µí•©\n",
        "    \n",
        "    Args:\n",
        "        original_root_dir: ì›ë³¸ ë°ì´í„°ì…‹ í´ë” (í´ë˜ìŠ¤ë³„ í•˜ìœ„ í´ë” í¬í•¨)\n",
        "        renamed_root_dir: ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ë“¤ì´ ì €ì¥ë  í´ë”\n",
        "    \"\"\"\n",
        "    classes = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
        "\n",
        "    # ê¸°ì¡´ í´ë”ê°€ ìˆìœ¼ë©´ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±\n",
        "    if os.path.exists(renamed_root_dir):\n",
        "        shutil.rmtree(renamed_root_dir)\n",
        "    os.makedirs(renamed_root_dir, exist_ok=True)\n",
        "\n",
        "    for class_name in classes:\n",
        "        src_class_dir = os.path.join(original_root_dir, class_name)\n",
        "        rename_images_in_class_folder(src_class_dir, class_name, renamed_root_dir)\n",
        "\n",
        "print(\"ì´ë¯¸ì§€ ë¦¬ë„¤ì´ë° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def copy_images_by_class(class_name, original_dataset_dir, dest_dirs, split_ratio=(0.5, 0.25, 0.25)):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ë“¤ì„ Train/Validation/Testë¡œ ë¶„í•  ë³µì‚¬\n",
        "    \n",
        "    Args:\n",
        "        class_name: í´ë˜ìŠ¤ëª…\n",
        "        original_dataset_dir: ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ë“¤ì´ ìˆëŠ” í´ë”\n",
        "        dest_dirs: [train_dir, val_dir, test_dir] ë¦¬ìŠ¤íŠ¸\n",
        "        split_ratio: ë¶„í•  ë¹„ìœ¨ (ê¸°ë³¸ê°’: 50:25:25)\n",
        "    \"\"\"\n",
        "    # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°\n",
        "    image_files = [\n",
        "        f for f in os.listdir(original_dataset_dir)\n",
        "        if f.startswith(f\"{class_name}.\") and f.lower().endswith(('.jpg', '.jpeg', '.png')) \n",
        "        and os.path.isfile(os.path.join(original_dataset_dir, f))\n",
        "    ]\n",
        "    image_files.sort()\n",
        "    random.shuffle(image_files)  # ëœë¤ ì…”í”Œ\n",
        "\n",
        "    # ë¶„í•  ì¸ë±ìŠ¤ ê³„ì‚°\n",
        "    total = len(image_files)\n",
        "    train_end = int(total * split_ratio[0])\n",
        "    val_end = train_end + int(total * split_ratio[1])\n",
        "\n",
        "    # ë¶„í• ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
        "    splits = [image_files[:train_end], image_files[train_end:val_end], image_files[val_end:]]\n",
        "\n",
        "    # ê° ì„¸íŠ¸ë¡œ íŒŒì¼ ë³µì‚¬\n",
        "    for split, dst_dir in zip(splits, dest_dirs):\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "        for fname in split:\n",
        "            src = os.path.join(original_dataset_dir, fname)\n",
        "            dst = os.path.join(dst_dir, fname)\n",
        "            shutil.copyfile(src, dst)\n",
        "\n",
        "\n",
        "def ImageCopy(renamed_dataset_dir, base_dir):\n",
        "    \"\"\"\n",
        "    ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ë“¤ì„ Train/Validation/Test í´ë”ë¡œ ë¶„í• í•˜ì—¬ ë³µì‚¬\n",
        "    \n",
        "    Args:\n",
        "        renamed_dataset_dir: ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ë“¤ì´ ìˆëŠ” í´ë”\n",
        "        base_dir: Train/Val/Test í´ë”ë“¤ì´ ìƒì„±ë  ê¸°ë³¸ ê²½ë¡œ\n",
        "    \"\"\"\n",
        "    categories = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
        "    sets = [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "    # ê¸°ì¡´ í´ë” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±\n",
        "    if os.path.exists(base_dir):\n",
        "        shutil.rmtree(base_dir)\n",
        "    for set_name in sets:\n",
        "        for category in categories:\n",
        "            os.makedirs(os.path.join(base_dir, set_name, category), exist_ok=True)\n",
        "\n",
        "    # í´ë” ê²½ë¡œ ì„¤ì •\n",
        "    train_dir = os.path.join(base_dir, \"train\")\n",
        "    val_dir = os.path.join(base_dir, \"validation\")\n",
        "    test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "    # ê° í´ë˜ìŠ¤ë³„ë¡œ ì´ë¯¸ì§€ ë¶„í•  ë³µì‚¬\n",
        "    for category in categories:\n",
        "        print(f\"ğŸ”„ {category} ë¶„í•  ì¤‘...\")\n",
        "        copy_images_by_class(\n",
        "            class_name=category,\n",
        "            original_dataset_dir=renamed_dataset_dir,\n",
        "            dest_dirs=[\n",
        "                os.path.join(train_dir, category),\n",
        "                os.path.join(val_dir, category),\n",
        "                os.path.join(test_dir, category)\n",
        "            ],\n",
        "            split_ratio=(0.5, 0.25, 0.25)\n",
        "        )\n",
        "\n",
        "    print(\"\\nâœ… ì´ë¯¸ì§€ ë¶„í•  ë³µì‚¬ ì™„ë£Œ!\\n\")\n",
        "\n",
        "    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
        "    for set_name in sets:\n",
        "        for category in categories:\n",
        "            dir_path = os.path.join(base_dir, set_name, category)\n",
        "            count = len(os.listdir(dir_path))\n",
        "            print(f\"ğŸ“ {set_name}/{category}: {count}ê°œ\")\n",
        "\n",
        "print(\"ë°ì´í„° ë¶„í•  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. ë°ì´í„°ì…‹ êµ¬ì„± ë° ë¶„í•  ì‹¤í–‰\n",
        "\n",
        "ì‹¤ì œë¡œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ì²˜ë¦¬ ê³¼ì •:\n",
        "1. **1ë‹¨ê³„**: í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ë“¤ì„ `class_name.index.ext` í˜•ì‹ìœ¼ë¡œ ë¦¬ë„¤ì´ë°\n",
        "2. **2ë‹¨ê³„**: ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ë¥¼ Train(50%) / Validation(25%) / Test(25%) ë¹„ìœ¨ë¡œ ë¶„í• \n",
        "\n",
        "### í´ë” êµ¬ì¡°:\n",
        "```\n",
        "flowers_small/\n",
        "â”œâ”€â”€ train/\n",
        "â”‚   â”œâ”€â”€ daisy/\n",
        "â”‚   â”œâ”€â”€ dandelion/\n",
        "â”‚   â”œâ”€â”€ rose/\n",
        "â”‚   â”œâ”€â”€ sunflower/\n",
        "â”‚   â””â”€â”€ tulip/\n",
        "â”œâ”€â”€ validation/\n",
        "â”‚   â””â”€â”€ (ë™ì¼í•œ êµ¬ì¡°)\n",
        "â””â”€â”€ test/\n",
        "    â””â”€â”€ (ë™ì¼í•œ êµ¬ì¡°)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
        "original_dataset_dir = \"../../data/flowers\"           # ì›ë³¸ ë°ì´í„°ì…‹ í´ë” (í´ë˜ìŠ¤ë³„ í•˜ìœ„ í´ë” ìˆìŒ)\n",
        "renamed_root = \"../../data/flowers_renamed\"           # ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ë“¤ì´ ì €ì¥ë  ìœ„ì¹˜\n",
        "base_dir = \"../../data/flowers_small\"                 # ìµœì¢… ë¶„í• ëœ train/val/test í´ë” ìƒì„± ìœ„ì¹˜\n",
        "\n",
        "print(\"ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"ì›ë³¸ ë°ì´í„°: {original_dataset_dir}\")\n",
        "print(f\"ë¦¬ë„¤ì´ë° ì €ì¥: {renamed_root}\")\n",
        "print(f\"ë¶„í•  ì €ì¥: {base_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1ë‹¨ê³„: í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ë“¤ì„ daisy.0.jpg í˜•ì‹ìœ¼ë¡œ ë¦¬ë„¤ì´ë° + í†µí•©\n",
        "print(\"ğŸ”„ 1ë‹¨ê³„: ì´ë¯¸ì§€ ë¦¬ë„¤ì´ë° ì‹œì‘...\")\n",
        "rename_all_classes(original_dataset_dir, renamed_root)\n",
        "print(\"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ëª¨ë“  ì´ë¯¸ì§€ê°€ ë¦¬ë„¤ì´ë°ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2ë‹¨ê³„: ë¦¬ë„¤ì´ë°ëœ ì´ë¯¸ì§€ë¥¼ 50:25:25 ë¹„ìœ¨ë¡œ train/validation/test ë¶„í•  ë³µì‚¬\n",
        "print(\"ğŸ”„ 2ë‹¨ê³„: ë°ì´í„° ë¶„í•  ì‹œì‘...\")\n",
        "ImageCopy(renamed_root, base_dir)\n",
        "print(\"âœ… 2ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ\n",
        "\n",
        "CNN(Convolutional Neural Network) ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "### ëª¨ë¸ êµ¬ì¡°:\n",
        "1. **ë°ì´í„° ì¦ê°•(Data Augmentation)**: RandomFlip, RandomRotation, RandomZoom\n",
        "2. **ì „ì²˜ë¦¬**: Rescaling (0-1 ì •ê·œí™”)\n",
        "3. **CNN ë ˆì´ì–´ë“¤**: \n",
        "   - Conv2D (32, 64, 32 í•„í„°) + MaxPooling2D\n",
        "   - Flatten + Dropout (0.5)\n",
        "   - Dense (128, 64 ìœ ë‹›) + ì¶œë ¥ì¸µ (5 í´ë˜ìŠ¤)\n",
        "\n",
        "### í•™ìŠµ ì„¤ì •:\n",
        "- **ì˜µí‹°ë§ˆì´ì €**: Adam\n",
        "- **ì†ì‹¤ í•¨ìˆ˜**: Sparse Categorical Crossentropy\n",
        "- **í‰ê°€ ì§€í‘œ**: Accuracy\n",
        "- **ì—í¬í¬**: 30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¶„í• ëœ ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "validation_dir = os.path.join(base_dir, \"validation\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "print(\"ğŸ“ í›ˆë ¨ ë°ì´í„° ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"Train: {train_dir}\")\n",
        "print(f\"Validation: {validation_dir}\")\n",
        "print(f\"Test: {test_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ì¦ê°• ë ˆì´ì–´ ì •ì˜\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\", input_shape=(180, 180, 3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "print(\"âœ… ë°ì´í„° ì¦ê°• ë ˆì´ì–´ ì •ì˜ ì™„ë£Œ!\")\n",
        "print(\"- RandomFlip: ìˆ˜í‰ ë’¤ì§‘ê¸°\")\n",
        "print(\"- RandomRotation: Â±10% íšŒì „\")\n",
        "print(\"- RandomZoom: Â±10% í™•ëŒ€/ì¶•ì†Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN ëª¨ë¸ êµ¬ì¶•\n",
        "model = models.Sequential()\n",
        "\n",
        "# ì „ì²˜ë¦¬ ë° ë°ì´í„° ì¦ê°•\n",
        "model.add(layers.Rescaling(1./255))  # í”½ì…€ ê°’ì„ 0-1ë¡œ ì •ê·œí™”\n",
        "model.add(data_augmentation)\n",
        "\n",
        "# í•©ì„±ê³± ë ˆì´ì–´ë“¤\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "\n",
        "# ì™„ì „ì—°ê²° ë ˆì´ì–´ë“¤\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))  # ê³¼ì í•© ë°©ì§€\n",
        "model.add(layers.Dense(128, activation=\"relu\"))\n",
        "model.add(layers.Dense(64, activation=\"relu\"))\n",
        "model.add(layers.Dense(5, activation=\"softmax\"))  # 5ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜\n",
        "\n",
        "# ëª¨ë¸ ì»´íŒŒì¼\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",  # ì •ìˆ˜ ë¼ë²¨ìš© ë‹¤ì¤‘ ë¶„ë¥˜\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(\"âœ… CNN ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split=0.2,  # í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ì¶”ê°€ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©\n",
        "    seed=123,\n",
        "    subset=\"training\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±\n",
        "validation_ds = keras.utils.image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split=0.2,\n",
        "    seed=123,\n",
        "    subset=\"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "print(\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"í›ˆë ¨ ë°ì´í„°ì…‹: {train_ds}\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„°ì…‹: {validation_ds}\")\n",
        "\n",
        "# í´ë˜ìŠ¤ ì´ë¦„ í™•ì¸\n",
        "class_names = train_ds.class_names\n",
        "print(f\"í´ë˜ìŠ¤: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. ëª¨ë¸ í•™ìŠµ ë° ì €ì¥\n",
        "\n",
        "ì‹¤ì œë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  ì €ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "### í•™ìŠµ ê³¼ì •:\n",
        "- **ì—í¬í¬**: 30íšŒ ë°˜ë³µ í•™ìŠµ\n",
        "- **ë°ì´í„°**: ì¦ê°•ëœ í›ˆë ¨ ë°ì´í„° + ê²€ì¦ ë°ì´í„°\n",
        "- **ê²°ê³¼**: ê° ì—í¬í¬ë§ˆë‹¤ í›ˆë ¨/ê²€ì¦ ì •í™•ë„ì™€ ì†ì‹¤ ì¶œë ¥\n",
        "- **ì €ì¥**: í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ì„ `flowers_model.keras` íŒŒì¼ë¡œ ì €ì¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
        "print(\"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
        "print(\"â€» 30 ì—í¬í¬ í•™ìŠµì—ëŠ” ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds, \n",
        "    validation_data=validation_ds,\n",
        "    epochs=30,\n",
        "    verbose=1  # í•™ìŠµ ê³¼ì • ì¶œë ¥\n",
        ")\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# ëª¨ë¸ ì €ì¥\n",
        "model_save_path = \"flowers_model.keras\"\n",
        "model.save(model_save_path)\n",
        "print(f\"ğŸ’¾ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_save_path}\")\n",
        "\n",
        "# ìµœì¢… ì„±ëŠ¥ ì¶œë ¥\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "print(f\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥:\")\n",
        "print(f\"- í›ˆë ¨ ì •í™•ë„: {final_train_acc:.4f}\")\n",
        "print(f\"- ê²€ì¦ ì •í™•ë„: {final_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "### ì™„ë£Œëœ ì‘ì—…:\n",
        "âœ… **ë°ì´í„° ì „ì²˜ë¦¬**: ì´ë¯¸ì§€ ë¦¬ë„¤ì´ë° ë° Train/Val/Test ë¶„í•   \n",
        "âœ… **ëª¨ë¸ êµ¬ì¶•**: CNN ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° ë°ì´í„° ì¦ê°• ì ìš©  \n",
        "âœ… **ëª¨ë¸ í•™ìŠµ**: 30 ì—í¬í¬ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€  \n",
        "âœ… **ëª¨ë¸ ì €ì¥**: `flowers_model.keras` íŒŒì¼ë¡œ ì €ì¥  \n",
        "\n",
        "### ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\n",
        "1. **ì„±ëŠ¥ ê°œì„ **:\n",
        "   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ì—í¬í¬ ìˆ˜)\n",
        "   - ë” ë³µì¡í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‹œë„ (ResNet, EfficientNet ë“±)\n",
        "   - ë” ë‹¤ì–‘í•œ ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©\n",
        "\n",
        "2. **ëª¨ë¸ í‰ê°€**:\n",
        "   - Test ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
        "   - Confusion Matrix ìƒì„±\n",
        "   - ì˜ëª» ë¶„ë¥˜ëœ ì´ë¯¸ì§€ ë¶„ì„\n",
        "\n",
        "3. **ëª¨ë¸ í™œìš©**:\n",
        "   - ìƒˆë¡œìš´ ê½ƒ ì´ë¯¸ì§€ ì˜ˆì¸¡ í•¨ìˆ˜ êµ¬í˜„\n",
        "   - ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë˜ëŠ” APIë¡œ ë°°í¬\n",
        "   - ì‹¤ì‹œê°„ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
        "\n",
        "### í•™ìŠµí•œ ì£¼ìš” ê°œë…:\n",
        "- **CNN ì•„í‚¤í…ì²˜**: Conv2D, MaxPooling2D, Dense ë ˆì´ì–´\n",
        "- **ë°ì´í„° ì¦ê°•**: ê³¼ì í•© ë°©ì§€ ë° ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
        "- **ë°ì´í„° ì „ì²˜ë¦¬**: ì²´ê³„ì ì¸ ë°ì´í„° ê´€ë¦¬ ë° ë¶„í• \n",
        "- **ëª¨ë¸ ì €ì¥/ë¡œë“œ**: í•™ìŠµëœ ëª¨ë¸ì˜ ì¬ì‚¬ìš©\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
