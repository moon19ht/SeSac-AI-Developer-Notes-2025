{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import load_model \n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random \n",
    "import PIL.Image as pilimg \n",
    "import imghdr\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import keras \n",
    "import os\n",
    "import shutil # 디렉토리 만들기, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc424e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_images_in_class_folder(src_class_dir, class_name, dest_dir):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    files.sort()\n",
    "\n",
    "    for idx, fname in enumerate(files):\n",
    "        src_path = os.path.join(src_class_dir, fname)\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "        new_name = f\"{class_name}.{idx}{ext}\"\n",
    "        dst_path = os.path.join(dest_dir, new_name)\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "    print(f\"✅ {class_name} 리네임 완료: {len(files)}개 처리됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d686de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_all_classes(original_root_dir, renamed_root_dir):\n",
    "    classes = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "\n",
    "    if os.path.exists(renamed_root_dir):\n",
    "        shutil.rmtree(renamed_root_dir)\n",
    "    os.makedirs(renamed_root_dir, exist_ok=True)\n",
    "\n",
    "    for class_name in classes:\n",
    "        src_class_dir = os.path.join(original_root_dir, class_name)\n",
    "        rename_images_in_class_folder(src_class_dir, class_name, renamed_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images_by_class(class_name, original_dataset_dir, dest_dirs, split_ratio=(0.5, 0.25, 0.25)):\n",
    "    image_files = [\n",
    "        f for f in os.listdir(original_dataset_dir)\n",
    "        if f.startswith(f\"{class_name}.\") and f.lower().endswith(('.jpg', '.jpeg', '.png')) and os.path.isfile(os.path.join(original_dataset_dir, f))\n",
    "    ]\n",
    "    image_files.sort()\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    total = len(image_files)\n",
    "    train_end = int(total * split_ratio[0])\n",
    "    val_end = train_end + int(total * split_ratio[1])\n",
    "\n",
    "    splits = [image_files[:train_end], image_files[train_end:val_end], image_files[val_end:]]\n",
    "\n",
    "    for split, dst_dir in zip(splits, dest_dirs):\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        for fname in split:\n",
    "            src = os.path.join(original_dataset_dir, fname)\n",
    "            dst = os.path.join(dst_dir, fname)\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageCopy(renamed_dataset_dir, base_dir):\n",
    "    categories = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            os.makedirs(os.path.join(base_dir, set_name, category), exist_ok=True)\n",
    "\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    val_dir = os.path.join(base_dir, \"validation\")\n",
    "    test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"🔄 {category} 분할 중...\")\n",
    "        copy_images_by_class(\n",
    "            class_name=category,\n",
    "            original_dataset_dir=renamed_dataset_dir,\n",
    "            dest_dirs=[\n",
    "                os.path.join(train_dir, category),\n",
    "                os.path.join(val_dir, category),\n",
    "                os.path.join(test_dir, category)\n",
    "            ],\n",
    "            split_ratio=(0.5, 0.25, 0.25)\n",
    "        )\n",
    "\n",
    "    print(\"\\n✅ 이미지 분할 복사 완료!\\n\")\n",
    "\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            dir_path = os.path.join(base_dir, set_name, category)\n",
    "            count = len(os.listdir(dir_path))\n",
    "            print(f\"📁 {set_name}/{category}: {count}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a75fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터셋 폴더 (클래스별 하위 폴더 있음)\n",
    "original_dataset_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "\n",
    "# 리네임된 이미지들이 저장될 위치\n",
    "renamed_root = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_renamed\"\n",
    "\n",
    "# 최종 분할된 train/val/test 폴더 생성 위치\n",
    "base_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_small\"\n",
    "\n",
    "# 1단계: 클래스별 이미지들을 daisy.0.jpg 형식으로 리네임 + 통합\n",
    "rename_all_classes(original_dataset_dir, renamed_root)\n",
    "\n",
    "# 2단계: 리네임된 이미지를 2:1:1 비율로 train/validation/test 분할 복사\n",
    "ImageCopy(renamed_root, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0278a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplearning():\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\", input_shape=(180, 180, 3)),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ])\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Rescaling(1./255))\n",
    "    model.add(data_augmentation)\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(5, activation=\"softmax\"))  # 👈 클래스 수 = 5\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",        # 👈 정수 라벨용 다중 분류\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    train_ds = keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=0.2,\n",
    "        seed=123,\n",
    "        subset=\"training\",\n",
    "        image_size=(180, 180),\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    validation_ds = keras.utils.image_dataset_from_directory(\n",
    "        validation_dir,\n",
    "        validation_split=0.2,\n",
    "        seed=123,\n",
    "        subset=\"validation\",\n",
    "        image_size=(180, 180),\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    model.fit(train_ds, \n",
    "              validation_data=validation_ds,\n",
    "              epochs=30)\n",
    "\n",
    "    model.save(\"flowers_model.keras\")\n",
    "\n",
    "deeplearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ba3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pickle \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "# 이미지 전처리 유틸리티 모듈\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import numpy as np \n",
    "import os \n",
    "import random \n",
    "import PIL.Image as pilimg \n",
    "import imghdr\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import keras \n",
    "\n",
    "def study():\n",
    "    batch_size=32 #한번에 불러올 이미지 개수 \n",
    "    img_height = 180 #내가 지정한 크기로 이미지를 가져온다  \n",
    "    img_width  = 180 \n",
    "\n",
    "    #데이터 증강을 위한 파라미터 지정하기 \n",
    "    data_augmentation = keras.Sequential(\n",
    "            [\n",
    "                    layers.RandomFlip(\"horizontal\"),\n",
    "                    layers.RandomRotation(0.1),\n",
    "                    layers.RandomZoom(0.1),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    model = models.Sequential() \n",
    "    model.add(data_augmentation) \n",
    "    #증강에 대한 파라미터를 주면 1에포크마다 데이터를 조금씩 변형을 가해서 가져간다\n",
    "    #과대적합을 막기 위해서 \n",
    "    model.add(layers.Rescaling(1./255))  #1/255 파이썬 3에서는 결과가 실수 2에서는 정수\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(layers.Flatten()) #CNN과 완전연결망을 연결한다.\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',  #라벨원한코딩안하려고 \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    train_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "    train_ds = keras.preprocessing.image_dataset_from_directory( \n",
    "        train_dir, \n",
    "        validation_split=0.2,\n",
    "        subset='training', \n",
    "        seed=1234,  \n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_ds = keras.preprocessing.image_dataset_from_directory( \n",
    "        train_dir, \n",
    "        validation_split=0.2,\n",
    "        subset='validation', #전체 데이터를 20% 나눠서 검증셋으로  \n",
    "        seed=1234,  \n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    history = model.fit(train_ds, \n",
    "              epochs=5,\n",
    "              validation_data = val_ds)\n",
    "    #모델을 저장하자 \n",
    "    model.save(\"flowers_model.keras\")\n",
    "    with open(\"flowers_hist.hist\", \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    return history.history  # 💡 수정된 부분\n",
    "\n",
    "\n",
    "def drawChart(history):  # ← 인자 받도록 수정\n",
    "    if not history:\n",
    "        print(\"⚠️ 학습 기록이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    acc = history[\"accuracy\"]\n",
    "    val_acc = history[\"val_accuracy\"]\n",
    "    loss = history[\"loss\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "    plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "    plt.title(\"Training and validation accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict():\n",
    "    # 모델 불러오기\n",
    "    model = load_model(\"flowers_model.keras\")\n",
    "\n",
    "    test_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_small/test\"\n",
    "    test_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=1234,\n",
    "        image_size=(180, 180),\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    print(\"✅ 예측 시작...\")\n",
    "\n",
    "    match_cnt = 0\n",
    "    total_cnt = 0\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        output = model.predict(images)\n",
    "        preds = np.argmax(output, axis=1)\n",
    "        match_cnt += np.sum(preds == labels.numpy())\n",
    "        total_cnt += len(labels)\n",
    "\n",
    "    print(\"✅ 예측 완료\")\n",
    "    print(\"총 샘플 수:\", total_cnt)\n",
    "    print(\"일치 수:\", match_cnt)\n",
    "    print(\"정확도: {:.2f}%\".format((match_cnt / total_cnt) * 100))\n",
    "\n",
    "\n",
    "def main():\n",
    "    history_data = None  # history 저장용 변수\n",
    "\n",
    "    while(True):\n",
    "        print(\"1.기본학습\")\n",
    "        print(\"2.차트\")\n",
    "        print(\"3.예측\")\n",
    "        print(\"4.평가하기\")\n",
    "        sel = input(\"선택 : \")\n",
    "        if sel == \"1\":\n",
    "            history_data = study()\n",
    "        elif sel == \"2\":\n",
    "            try:\n",
    "                with open(\"flowers_hist.hist\", \"rb\") as f:\n",
    "                    history_data = pickle.load(f)\n",
    "                drawChart(history_data)\n",
    "            except Exception as e:\n",
    "                print(\"❌ 차트를 불러오는 데 실패했습니다:\", e)\n",
    "        elif sel == \"3\":\n",
    "            predict()\n",
    "        elif sel == \"4\":\n",
    "            pass\n",
    "        else:\n",
    "            return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
