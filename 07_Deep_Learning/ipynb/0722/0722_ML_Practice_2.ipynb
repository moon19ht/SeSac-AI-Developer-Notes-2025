{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import load_model \n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random \n",
    "import PIL.Image as pilimg \n",
    "import imghdr\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import keras \n",
    "import os\n",
    "import shutil # ë””ë ‰í† ë¦¬ ë§Œë“¤ê¸°, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc424e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_images_in_class_folder(src_class_dir, class_name, dest_dir):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    files.sort()\n",
    "\n",
    "    for idx, fname in enumerate(files):\n",
    "        src_path = os.path.join(src_class_dir, fname)\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "        new_name = f\"{class_name}.{idx}{ext}\"\n",
    "        dst_path = os.path.join(dest_dir, new_name)\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "    print(f\"âœ… {class_name} ë¦¬ë„¤ì„ ì™„ë£Œ: {len(files)}ê°œ ì²˜ë¦¬ë¨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d686de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_all_classes(original_root_dir, renamed_root_dir):\n",
    "    classes = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "\n",
    "    if os.path.exists(renamed_root_dir):\n",
    "        shutil.rmtree(renamed_root_dir)\n",
    "    os.makedirs(renamed_root_dir, exist_ok=True)\n",
    "\n",
    "    for class_name in classes:\n",
    "        src_class_dir = os.path.join(original_root_dir, class_name)\n",
    "        rename_images_in_class_folder(src_class_dir, class_name, renamed_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images_by_class(class_name, original_dataset_dir, dest_dirs, split_ratio=(0.5, 0.25, 0.25)):\n",
    "    image_files = [\n",
    "        f for f in os.listdir(original_dataset_dir)\n",
    "        if f.startswith(f\"{class_name}.\") and f.lower().endswith(('.jpg', '.jpeg', '.png')) and os.path.isfile(os.path.join(original_dataset_dir, f))\n",
    "    ]\n",
    "    image_files.sort()\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    total = len(image_files)\n",
    "    train_end = int(total * split_ratio[0])\n",
    "    val_end = train_end + int(total * split_ratio[1])\n",
    "\n",
    "    splits = [image_files[:train_end], image_files[train_end:val_end], image_files[val_end:]]\n",
    "\n",
    "    for split, dst_dir in zip(splits, dest_dirs):\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        for fname in split:\n",
    "            src = os.path.join(original_dataset_dir, fname)\n",
    "            dst = os.path.join(dst_dir, fname)\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageCopy(renamed_dataset_dir, base_dir):\n",
    "    categories = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            os.makedirs(os.path.join(base_dir, set_name, category), exist_ok=True)\n",
    "\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    val_dir = os.path.join(base_dir, \"validation\")\n",
    "    test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"ğŸ”„ {category} ë¶„í•  ì¤‘...\")\n",
    "        copy_images_by_class(\n",
    "            class_name=category,\n",
    "            original_dataset_dir=renamed_dataset_dir,\n",
    "            dest_dirs=[\n",
    "                os.path.join(train_dir, category),\n",
    "                os.path.join(val_dir, category),\n",
    "                os.path.join(test_dir, category)\n",
    "            ],\n",
    "            split_ratio=(0.5, 0.25, 0.25)\n",
    "        )\n",
    "\n",
    "    print(\"\\nâœ… ì´ë¯¸ì§€ ë¶„í•  ë³µì‚¬ ì™„ë£Œ!\\n\")\n",
    "\n",
    "    for set_name in sets:\n",
    "        for category in categories:\n",
    "            dir_path = os.path.join(base_dir, set_name, category)\n",
    "            count = len(os.listdir(dir_path))\n",
    "            print(f\"ğŸ“ {set_name}/{category}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a75fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ ë°ì´í„°ì…‹ í´ë” (í´ë˜ìŠ¤ë³„ í•˜ìœ„ í´ë” ìˆìŒ)\n",
    "original_dataset_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "\n",
    "# ë¦¬ë„¤ì„ëœ ì´ë¯¸ì§€ë“¤ì´ ì €ì¥ë  ìœ„ì¹˜\n",
    "renamed_root = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_renamed\"\n",
    "\n",
    "# ìµœì¢… ë¶„í• ëœ train/val/test í´ë” ìƒì„± ìœ„ì¹˜\n",
    "base_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_small\"\n",
    "\n",
    "# 1ë‹¨ê³„: í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ë“¤ì„ daisy.0.jpg í˜•ì‹ìœ¼ë¡œ ë¦¬ë„¤ì„ + í†µí•©\n",
    "rename_all_classes(original_dataset_dir, renamed_root)\n",
    "\n",
    "# 2ë‹¨ê³„: ë¦¬ë„¤ì„ëœ ì´ë¯¸ì§€ë¥¼ 2:1:1 ë¹„ìœ¨ë¡œ train/validation/test ë¶„í•  ë³µì‚¬\n",
    "ImageCopy(renamed_root, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0278a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplearning():\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\", input_shape=(180, 180, 3)),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ])\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Rescaling(1./255))\n",
    "    model.add(data_augmentation)\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(5, activation=\"softmax\"))  # ğŸ‘ˆ í´ë˜ìŠ¤ ìˆ˜ = 5\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",        # ğŸ‘ˆ ì •ìˆ˜ ë¼ë²¨ìš© ë‹¤ì¤‘ ë¶„ë¥˜\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    train_ds = keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=0.2,\n",
    "        seed=123,\n",
    "        subset=\"training\",\n",
    "        image_size=(180, 180),\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    validation_ds = keras.utils.image_dataset_from_directory(\n",
    "        validation_dir,\n",
    "        validation_split=0.2,\n",
    "        seed=123,\n",
    "        subset=\"validation\",\n",
    "        image_size=(180, 180),\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    model.fit(train_ds, \n",
    "              validation_data=validation_ds,\n",
    "              epochs=30)\n",
    "\n",
    "    model.save(\"flowers_model.keras\")\n",
    "\n",
    "deeplearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ba3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pickle \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import numpy as np \n",
    "import os \n",
    "import random \n",
    "import PIL.Image as pilimg \n",
    "import imghdr\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import keras \n",
    "\n",
    "def study():\n",
    "    batch_size=32 #í•œë²ˆì— ë¶ˆëŸ¬ì˜¬ ì´ë¯¸ì§€ ê°œìˆ˜ \n",
    "    img_height = 180 #ë‚´ê°€ ì§€ì •í•œ í¬ê¸°ë¡œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜¨ë‹¤  \n",
    "    img_width  = 180 \n",
    "\n",
    "    #ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì§€ì •í•˜ê¸° \n",
    "    data_augmentation = keras.Sequential(\n",
    "            [\n",
    "                    layers.RandomFlip(\"horizontal\"),\n",
    "                    layers.RandomRotation(0.1),\n",
    "                    layers.RandomZoom(0.1),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    model = models.Sequential() \n",
    "    model.add(data_augmentation) \n",
    "    #ì¦ê°•ì— ëŒ€í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì£¼ë©´ 1ì—í¬í¬ë§ˆë‹¤ ë°ì´í„°ë¥¼ ì¡°ê¸ˆì”© ë³€í˜•ì„ ê°€í•´ì„œ ê°€ì ¸ê°„ë‹¤\n",
    "    #ê³¼ëŒ€ì í•©ì„ ë§‰ê¸° ìœ„í•´ì„œ \n",
    "    model.add(layers.Rescaling(1./255))  #1/255 íŒŒì´ì¬ 3ì—ì„œëŠ” ê²°ê³¼ê°€ ì‹¤ìˆ˜ 2ì—ì„œëŠ” ì •ìˆ˜\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(layers.Flatten()) #CNNê³¼ ì™„ì „ì—°ê²°ë§ì„ ì—°ê²°í•œë‹¤.\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',  #ë¼ë²¨ì›í•œì½”ë”©ì•ˆí•˜ë ¤ê³  \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    train_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers\"\n",
    "    train_ds = keras.preprocessing.image_dataset_from_directory( \n",
    "        train_dir, \n",
    "        validation_split=0.2,\n",
    "        subset='training', \n",
    "        seed=1234,  \n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_ds = keras.preprocessing.image_dataset_from_directory( \n",
    "        train_dir, \n",
    "        validation_split=0.2,\n",
    "        subset='validation', #ì „ì²´ ë°ì´í„°ë¥¼ 20% ë‚˜ëˆ ì„œ ê²€ì¦ì…‹ìœ¼ë¡œ  \n",
    "        seed=1234,  \n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    history = model.fit(train_ds, \n",
    "              epochs=5,\n",
    "              validation_data = val_ds)\n",
    "    #ëª¨ë¸ì„ ì €ì¥í•˜ì \n",
    "    model.save(\"flowers_model.keras\")\n",
    "    with open(\"flowers_hist.hist\", \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    return history.history  # ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "\n",
    "\n",
    "def drawChart(history):  # â† ì¸ì ë°›ë„ë¡ ìˆ˜ì •\n",
    "    if not history:\n",
    "        print(\"âš ï¸ í•™ìŠµ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    acc = history[\"accuracy\"]\n",
    "    val_acc = history[\"val_accuracy\"]\n",
    "    loss = history[\"loss\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "    plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "    plt.title(\"Training and validation accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict():\n",
    "    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    model = load_model(\"flowers_model.keras\")\n",
    "\n",
    "    test_dir = \"C:/Users/Admin/Documents/GitHub/ML-DL-study/deeplearning_workspace/data/flowers_small/test\"\n",
    "    test_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=1234,\n",
    "        image_size=(180, 180),\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    print(\"âœ… ì˜ˆì¸¡ ì‹œì‘...\")\n",
    "\n",
    "    match_cnt = 0\n",
    "    total_cnt = 0\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        output = model.predict(images)\n",
    "        preds = np.argmax(output, axis=1)\n",
    "        match_cnt += np.sum(preds == labels.numpy())\n",
    "        total_cnt += len(labels)\n",
    "\n",
    "    print(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "    print(\"ì´ ìƒ˜í”Œ ìˆ˜:\", total_cnt)\n",
    "    print(\"ì¼ì¹˜ ìˆ˜:\", match_cnt)\n",
    "    print(\"ì •í™•ë„: {:.2f}%\".format((match_cnt / total_cnt) * 100))\n",
    "\n",
    "\n",
    "def main():\n",
    "    history_data = None  # history ì €ì¥ìš© ë³€ìˆ˜\n",
    "\n",
    "    while(True):\n",
    "        print(\"1.ê¸°ë³¸í•™ìŠµ\")\n",
    "        print(\"2.ì°¨íŠ¸\")\n",
    "        print(\"3.ì˜ˆì¸¡\")\n",
    "        print(\"4.í‰ê°€í•˜ê¸°\")\n",
    "        sel = input(\"ì„ íƒ : \")\n",
    "        if sel == \"1\":\n",
    "            history_data = study()\n",
    "        elif sel == \"2\":\n",
    "            try:\n",
    "                with open(\"flowers_hist.hist\", \"rb\") as f:\n",
    "                    history_data = pickle.load(f)\n",
    "                drawChart(history_data)\n",
    "            except Exception as e:\n",
    "                print(\"âŒ ì°¨íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:\", e)\n",
    "        elif sel == \"3\":\n",
    "            predict()\n",
    "        elif sel == \"4\":\n",
    "            pass\n",
    "        else:\n",
    "            return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
