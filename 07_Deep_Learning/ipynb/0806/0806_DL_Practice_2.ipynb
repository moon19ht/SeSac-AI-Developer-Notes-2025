{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST 이미지 분류 모델\n",
    "\n",
    "## 프로젝트 개요\n",
    "- **목표**: Fashion-MNIST 데이터셋을 사용한 의류 이미지 10-class 분류\n",
    "- **모델**: Convolutional Neural Network (CNN)\n",
    "- **데이터**: 28x28 픽셀 흑백 의류 이미지\n",
    "- **클래스**: 10가지 의류 카테고리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 의존성 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.15.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pickle\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 전역 설정 및 상수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 경로: fashion_mnist_model.keras\n",
      "히스토리 저장 경로: fashion_mnist_history.bin\n",
      "분류 클래스: 10개\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 학습 히스토리 저장 경로 설정\n",
    "model_path = \"fashion_mnist_model.keras\"  # 훈련된 모델 저장 파일\n",
    "history_path = \"fashion_mnist_history.bin\"  # 학습 히스토리 저장 파일\n",
    "\n",
    "# Fashion-MNIST 클래스 라벨 정의\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"모델 저장 경로: {model_path}\")\n",
    "print(f\"히스토리 저장 경로: {history_path}\")\n",
    "print(f\"분류 클래스: {len(class_names)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 로딩\n",
    "\n",
    "Fashion-MNIST는 Keras에서 제공하는 표준 데이터셋입니다.\n",
    "- **훈련 데이터**: 60,000개 이미지\n",
    "- **테스트 데이터**: 10,000개 이미지  \n",
    "- **이미지 크기**: 28×28 픽셀 (흑백)\n",
    "- **클래스 수**: 10개 의류 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Fashion-MNIST 데이터셋을 로드하고 기본 정보를 출력합니다.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, y_train, X_test, y_test)\n",
    "    \"\"\"\n",
    "    # Keras에서 제공하는 Fashion-MNIST 데이터셋 로드\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "    \n",
    "    # 데이터셋 형태 확인\n",
    "    print(\"=== 데이터셋 정보 ===\")\n",
    "    print(f\"훈련 이미지 shape: {X_train.shape}\")\n",
    "    print(f\"훈련 레이블 shape: {y_train.shape}\")\n",
    "    print(f\"테스트 이미지 shape: {X_test.shape}\")\n",
    "    print(f\"테스트 레이블 shape: {y_test.shape}\")\n",
    "    \n",
    "    # 레이블 분포 확인\n",
    "    print(f\"\\n첫 10개 테스트 레이블: {y_test[:10]}\")\n",
    "    print(f\"픽셀 값 범위: {X_train.min()} ~ {X_train.max()}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리\n",
    "\n",
    "딥러닝 모델 학습을 위한 데이터 전처리 과정:\n",
    "1. **정규화**: 픽셀 값을 0-1 범위로 스케일링 (0-255 → 0-1)\n",
    "2. **차원 확장**: CNN을 위한 채널 차원 추가 (28, 28) → (28, 28, 1)\n",
    "3. **데이터 타입 변환**: float32로 변환하여 메모리 효율성 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    이미지 데이터를 딥러닝 모델에 적합하도록 전처리합니다.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: 훈련 데이터\n",
    "        X_test, y_test: 테스트 데이터\n",
    "        \n",
    "    Returns:\n",
    "        tuple: 전처리된 (X_train, y_train, X_test, y_test)\n",
    "    \"\"\"\n",
    "    print(\"=== 데이터 전처리 시작 ===\")\n",
    "    \n",
    "    # 1. 정규화: 픽셀 값을 0-1 범위로 스케일링 \n",
    "    X_train = X_train.astype(\"float32\") / 255.0\n",
    "    X_test = X_test.astype(\"float32\") / 255.0\n",
    "    print(f\"정규화 완료: 픽셀 값 범위 {X_train.min():.1f} ~ {X_train.max():.1f}\")\n",
    "    \n",
    "    # 2. CNN을 위한 채널 차원 추가 (28, 28) → (28, 28, 1)\n",
    "    X_train = np.expand_dims(X_train, -1)\n",
    "    X_test = np.expand_dims(X_test, -1)\n",
    "    \n",
    "    print(f\"차원 확장 완료:\")\n",
    "    print(f\"  - 훈련 데이터: {X_train.shape}\")\n",
    "    print(f\"  - 테스트 데이터: {X_test.shape}\")\n",
    "    \n",
    "    # 레이블은 sparse_categorical_crossentropy 사용으로 원핫인코딩 불필요\n",
    "    print(\"레이블: sparse_categorical_crossentropy 사용으로 원핫인코딩 생략\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CNN 모델 아키텍처 구성\n",
    "\n",
    "Fashion-MNIST 분류를 위한 Convolutional Neural Network 모델 설계:\n",
    "\n",
    "### 모델 구조\n",
    "1. **Convolutional Layers**: 특징 추출\n",
    "   - Conv2D(32) + ReLU + MaxPooling2D\n",
    "   - Conv2D(64) + ReLU + MaxPooling2D\n",
    "2. **Fully Connected Layers**: 분류\n",
    "   - Flatten → Dense(64) → Dense(32) → Dense(10)\n",
    "3. **출력층**: Softmax 활성화 (10개 클래스 확률)\n",
    "\n",
    "### 데이터 증강 (옵션)\n",
    "- RandomFlip, RandomRotation, RandomZoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(use_data_augmentation=False):\n",
    "    \"\"\"\n",
    "    Fashion-MNIST 분류용 CNN 모델을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        use_data_augmentation (bool): 데이터 증강 사용 여부\n",
    "        \n",
    "    Returns:\n",
    "        keras.Model: 컴파일된 CNN 모델\n",
    "    \"\"\"\n",
    "    print(\"=== CNN 모델 구성 ===\")\n",
    "    \n",
    "    # 데이터 증강 레이어 (선택적)\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2)\n",
    "    ], name=\"data_augmentation\")\n",
    "    \n",
    "    # CNN 모델 아키텍처 구성\n",
    "    model_layers = []\n",
    "    \n",
    "    # 데이터 증강 추가 (옵션)\n",
    "    if use_data_augmentation:\n",
    "        model_layers.append(data_augmentation)\n",
    "        print(\"데이터 증강 레이어 추가됨\")\n",
    "    \n",
    "    # Convolutional 블록 1\n",
    "    model_layers.extend([\n",
    "        layers.Conv2D(32, kernel_size=(3,3), activation=\"relu\", \n",
    "                     input_shape=(28,28,1), name=\"conv2d_1\"),\n",
    "        layers.MaxPooling2D(pool_size=(2,2), name=\"maxpool_1\"),\n",
    "    ])\n",
    "    \n",
    "    # Convolutional 블록 2  \n",
    "    model_layers.extend([\n",
    "        layers.Conv2D(64, kernel_size=(3,3), activation=\"relu\", name=\"conv2d_2\"),\n",
    "        layers.MaxPooling2D(pool_size=(2,2), name=\"maxpool_2\"),\n",
    "    ])\n",
    "    \n",
    "    # Fully Connected 블록\n",
    "    model_layers.extend([\n",
    "        layers.Flatten(name=\"flatten\"),\n",
    "        layers.Dense(64, activation='relu', name=\"dense_1\"),\n",
    "        layers.Dense(32, activation='relu', name=\"dense_2\"), \n",
    "        layers.Dense(10, activation='softmax', name=\"output\")\n",
    "    ])\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = keras.Sequential(model_layers, name=\"fashion_mnist_cnn\")\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"모델 컴파일 완료\")\n",
    "    print(f\"데이터 증강: {'사용' if use_data_augmentation else '미사용'}\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 학습 및 훈련\n",
    "\n",
    "### 학습 설정\n",
    "- **Optimizer**: Adam \n",
    "- **Loss Function**: Sparse Categorical Crossentropy\n",
    "- **Metrics**: Accuracy\n",
    "- **Validation Split**: 20% (훈련 데이터의 20%를 검증용으로 사용)\n",
    "- **Early Stopping**: 검증 손실이 5 에포크 동안 개선되지 않으면 조기 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=100):\n",
    "    \"\"\"\n",
    "    모델을 훈련하고 결과를 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: 훈련할 Keras 모델\n",
    "        X_train, y_train: 훈련 데이터\n",
    "        X_test, y_test: 테스트 데이터  \n",
    "        epochs (int): 최대 훈련 에포크 수\n",
    "        \n",
    "    Returns:\n",
    "        keras.callbacks.History: 학습 히스토리\n",
    "    \"\"\"\n",
    "    print(\"=== 모델 학습 시작 ===\")\n",
    "    \n",
    "    # 콜백 설정: 조기 종료\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"학습 설정:\")\n",
    "    print(f\"  - 최대 에포크: {epochs}\")\n",
    "    print(f\"  - 검증 분할: 20%\")\n",
    "    print(f\"  - 조기 종료: val_loss 5 에포크 patience\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 모델 및 학습 히스토리 저장\n",
    "    print(f\"\\n=== 모델 저장 ===\")\n",
    "    model.save(model_path)\n",
    "    print(f\"모델 저장 완료: {model_path}\")\n",
    "    \n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"히스토리 저장 완료: {history_path}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 전체 파이프라인 실행\n",
    "\n",
    "데이터 로딩부터 모델 훈련까지의 전체 과정을 순차적으로 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Fashion-MNIST CNN 모델 훈련 파이프라인 시작\n",
      "============================================================\n",
      "=== 데이터셋 정보 ===\n",
      "훈련 이미지 shape: (60000, 28, 28)\n",
      "훈련 레이블 shape: (60000,)\n",
      "테스트 이미지 shape: (10000, 28, 28)\n",
      "테스트 레이블 shape: (10000,)\n",
      "\n",
      "첫 10개 테스트 레이블: [9 2 1 1 6 1 4 6 5 7]\n",
      "픽셀 값 범위: 0 ~ 255\n",
      "=== 데이터 전처리 시작 ===\n",
      "정규화 완료: 픽셀 값 범위 0.0 ~ 1.0\n",
      "차원 확장 완료:\n",
      "  - 훈련 데이터: (60000, 28, 28, 1)\n",
      "  - 테스트 데이터: (10000, 28, 28, 1)\n",
      "레이블: sparse_categorical_crossentropy 사용으로 원핫인코딩 생략\n",
      "=== CNN 모델 구성 ===\n",
      "모델 컴파일 완료\n",
      "데이터 증강: 미사용\n",
      "Model: \"fashion_mnist_cnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " maxpool_1 (MaxPooling2D)    (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " maxpool_2 (MaxPooling2D)    (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                102464    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123690 (483.16 KB)\n",
      "Trainable params: 123690 (483.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "=== 모델 학습 시작 ===\n",
      "학습 설정:\n",
      "  - 최대 에포크: 100\n",
      "  - 검증 분할: 20%\n",
      "  - 조기 종료: val_loss 5 에포크 patience\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 13s 8ms/step - loss: 0.5189 - accuracy: 0.8106 - val_loss: 0.3910 - val_accuracy: 0.8579\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.3346 - accuracy: 0.8781 - val_loss: 0.3107 - val_accuracy: 0.8888\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.2913 - accuracy: 0.8936 - val_loss: 0.3240 - val_accuracy: 0.8852\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.2579 - accuracy: 0.9051 - val_loss: 0.2899 - val_accuracy: 0.8957\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.2323 - accuracy: 0.9148 - val_loss: 0.2632 - val_accuracy: 0.9072\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.2123 - accuracy: 0.9218 - val_loss: 0.2513 - val_accuracy: 0.9104\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1923 - accuracy: 0.9287 - val_loss: 0.2584 - val_accuracy: 0.9088\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1736 - accuracy: 0.9353 - val_loss: 0.2570 - val_accuracy: 0.9089\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.1595 - accuracy: 0.9409 - val_loss: 0.2583 - val_accuracy: 0.9136\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.1450 - accuracy: 0.9457 - val_loss: 0.2612 - val_accuracy: 0.9122\n",
      "Epoch 11/100\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.9485Restoring model weights from the end of the best epoch: 6.\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.1347 - accuracy: 0.9485 - val_loss: 0.3176 - val_accuracy: 0.9040\n",
      "Epoch 11: early stopping\n",
      "\n",
      "=== 모델 저장 ===\n",
      "모델 저장 완료: fashion_mnist_model.keras\n",
      "히스토리 저장 완료: fashion_mnist_history.bin\n",
      "============================================================\n",
      "모델 훈련 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 전체 머신러닝 파이프라인 실행\n",
    "print(\"=\" * 60)\n",
    "print(\"Fashion-MNIST CNN 모델 훈련 파이프라인 시작\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. 데이터 로딩\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# 2. 데이터 전처리  \n",
    "X_train, y_train, X_test, y_test = preprocessing(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 3. 모델 생성\n",
    "model = getModel(use_data_augmentation=False)  # 데이터 증강 비활성화\n",
    "\n",
    "# 4. 모델 훈련\n",
    "history = train_model(model, X_train, y_train, X_test, y_test, epochs=100)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"모델 훈련 완료!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 모델 평가 및 성능 분석\n",
    "\n",
    "훈련된 모델의 성능을 다양한 지표로 평가합니다:\n",
    "\n",
    "### 평가 지표\n",
    "1. **기본 지표**: 테스트셋 손실 및 정확도\n",
    "2. **혼동 행렬**: 클래스별 분류 성능 시각화\n",
    "3. **분류 보고서**: 정밀도, 재현율, F1-점수 등 상세 지표\n",
    "4. **클래스별 성능**: 각 의류 카테고리별 분류 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test, y_test):\n",
    "    \"\"\"\n",
    "    저장된 모델을 로드하여 테스트 데이터로 성능을 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        X_test: 테스트 이미지 데이터\n",
    "        y_test: 테스트 레이블 데이터\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"모델 평가 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 저장된 모델과 히스토리 로드\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print(f\"모델 로드 완료: {model_path}\")\n",
    "    \n",
    "    with open(history_path, 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    print(f\"히스토리 로드 완료: {history_path}\")\n",
    "    \n",
    "    # === 1. 예측 수행 ===\n",
    "    print(\"\\n=== 예측 수행 ===\")\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)  # 예측 확률\n",
    "    print(f\"예측 확률 shape: {y_pred_proba.shape}\")\n",
    "    print(f\"첫 번째 샘플 예측 확률: {y_pred_proba[0]}\")\n",
    "    \n",
    "    # 확률에서 클래스 인덱스로 변환\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    print(f\"\\n예측 클래스 (처음 10개): {y_pred[:10]}\")\n",
    "    print(f\"실제 클래스 (처음 10개): {y_test[:10]}\")\n",
    "    \n",
    "    # === 2. 기본 성능 지표 ===\n",
    "    print(\"\\n=== 기본 성능 지표 ===\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"테스트 손실 (Loss): {loss:.4f}\")\n",
    "    print(f\"테스트 정확도 (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # === 3. 혼동 행렬 ===\n",
    "    print(\"\\n=== 혼동 행렬 ===\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # === 4. 상세 분류 보고서 ===\n",
    "    print(\"\\n=== 분류 보고서 ===\")\n",
    "    report = classification_report(\n",
    "        y_test, y_pred, \n",
    "        target_names=class_names,\n",
    "        digits=4\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # === 5. 요약 통계 ===\n",
    "    print(\"\\n=== 성능 요약 ===\")\n",
    "    correct_predictions = np.sum(y_pred == y_test)\n",
    "    total_predictions = len(y_test)\n",
    "    print(f\"전체 테스트 샘플: {total_predictions:,}\")\n",
    "    print(f\"올바른 예측: {correct_predictions:,}\")\n",
    "    print(f\"잘못된 예측: {total_predictions - correct_predictions:,}\")\n",
    "    print(f\"최종 정확도: {correct_predictions/total_predictions:.4f}\")\n",
    "    \n",
    "    return model, history, y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 평가 실행\n",
    "\n",
    "저장된 모델을 로드하여 테스트 데이터셋으로 최종 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "모델 평가 시작\n",
      "============================================================\n",
      "모델 로드 완료: fashion_mnist_model.keras\n",
      "히스토리 로드 완료: fashion_mnist_history.bin\n",
      "\n",
      "=== 예측 수행 ===\n",
      "예측 확률 shape: (10000, 10)\n",
      "첫 번째 샘플 예측 확률: [1.9697832e-06 2.9713748e-08 6.0883741e-07 5.1347211e-08 4.6203857e-07\n",
      " 1.0105026e-03 2.0627771e-05 1.1649434e-03 1.1705238e-07 9.9780077e-01]\n",
      "\n",
      "예측 클래스 (처음 10개): [9 2 1 1 6 1 4 6 5 7]\n",
      "실제 클래스 (처음 10개): [9 2 1 1 6 1 4 6 5 7]\n",
      "\n",
      "=== 기본 성능 지표 ===\n",
      "테스트 손실 (Loss): 0.2636\n",
      "테스트 정확도 (Accuracy): 0.9067 (90.67%)\n",
      "\n",
      "=== 혼동 행렬 ===\n",
      "[[879   0  21  12   1   0  82   0   5   0]\n",
      " [  2 970   0  20   2   0   5   0   1   0]\n",
      " [ 15   1 872  10  38   0  62   0   2   0]\n",
      " [ 22   2   8 924  14   0  30   0   0   0]\n",
      " [  0   0  67  30 814   0  87   0   2   0]\n",
      " [  1   0   0   0   0 981   0  14   1   3]\n",
      " [132   0  77  24  40   0 716   0  11   0]\n",
      " [  0   0   0   0   0   6   0 976   3  15]\n",
      " [  2   0   3   5   0   2   5   1 982   0]\n",
      " [  1   0   0   0   0   7   0  37   2 953]]\n",
      "\n",
      "=== 분류 보고서 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top     0.8340    0.8790    0.8559      1000\n",
      "     Trouser     0.9969    0.9700    0.9833      1000\n",
      "    Pullover     0.8321    0.8720    0.8516      1000\n",
      "       Dress     0.9015    0.9240    0.9126      1000\n",
      "        Coat     0.8955    0.8140    0.8528      1000\n",
      "      Sandal     0.9849    0.9810    0.9830      1000\n",
      "       Shirt     0.7254    0.7160    0.7207      1000\n",
      "     Sneaker     0.9494    0.9760    0.9625      1000\n",
      "         Bag     0.9732    0.9820    0.9776      1000\n",
      "  Ankle boot     0.9815    0.9530    0.9670      1000\n",
      "\n",
      "    accuracy                         0.9067     10000\n",
      "   macro avg     0.9074    0.9067    0.9067     10000\n",
      "weighted avg     0.9074    0.9067    0.9067     10000\n",
      "\n",
      "\n",
      "=== 성능 요약 ===\n",
      "전체 테스트 샘플: 10,000\n",
      "올바른 예측: 9,067\n",
      "잘못된 예측: 933\n",
      "최종 정확도: 0.9067\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 실행\n",
    "model_eval, history_eval, predictions, prediction_probabilities = evaluate_model(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
