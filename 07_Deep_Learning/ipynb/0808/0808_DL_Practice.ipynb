{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 감성 분석 (Korean Sentiment Analysis)\n",
    "\n",
    "## 프로젝트 개요\n",
    "네이버 영화 리뷰 데이터셋(NSMC)을 사용하여 한국어 텍스트의 감성(긍정/부정)을 분류하는 딥러닝 모델을 구현합니다.\n",
    "\n",
    "## 사용 기술\n",
    "- **데이터셋**: NSMC (Naver Sentiment Movie Corpus)\n",
    "- **전처리**: KoNLPy의 Okt 형태소 분석기\n",
    "- **임베딩**: FastText 한국어 사전 훈련된 벡터\n",
    "- **모델**: Bidirectional LSTM\n",
    "- **프레임워크**: TensorFlow/Keras\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트\n",
    "\n",
    "필요한 라이브러리들을 임포트하고 환경을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 (주석 해제하여 실행)\n",
    "# !pip install Korpora konlpy gensim\n",
    "\n",
    "# 핵심 라이브러리 임포트\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import os, pathlib, shutil, random\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from gensim.models import KeyedVectors \n",
    "import pickle\n",
    "\n",
    "from Korpora import Korpora\n",
    "\n",
    "print(\"라이브러리 임포트 완료\")\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 로드 및 확인\n",
    "\n",
    "NSMC 데이터셋을 다운로드하고 구조를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSMC 데이터셋 다운로드\n",
    "Korpora.fetch(\"nsmc\")\n",
    "print(\"데이터셋 저장 위치: C:\\\\Users\\\\사용자계정\\\\Korpora\\\\nsmc\")\n",
    "\n",
    "# 데이터셋 로드\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "\n",
    "# 데이터셋 구조 확인\n",
    "print(\"\\n=== 데이터셋 샘플 확인 ===\")\n",
    "print(corpus.train[:3])\n",
    "\n",
    "print(f\"\\n=== 데이터셋 크기 ===\")\n",
    "print(f\"Train 데이터: {len(corpus.train)}개\")\n",
    "print(f\"Test 데이터: {len(corpus.test)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 폴더 구조 생성\n",
    "\n",
    "Keras의 `text_dataset_from_directory` 함수를 사용하기 위해 폴더 구조를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_korean_dataset(base_dir=\"korean_imdb\"):\n",
    "    \"\"\"\n",
    "    네이버 영화평 데이터를 train/val/test 폴더로 분리하여 저장하는 함수\n",
    "    각 폴더는 pos(긍정), neg(부정) 서브폴더를 가집니다.\n",
    "    \"\"\"\n",
    "    # 기존 폴더가 있으면 삭제\n",
    "    if os.path.exists(base_dir):\n",
    "        try:\n",
    "            shutil.rmtree(base_dir)\n",
    "            print(f\"기존 {base_dir} 폴더 삭제 완료\")\n",
    "        except OSError as e:\n",
    "            print(f\"폴더 삭제 오류: {e}\")\n",
    "    \n",
    "    # 서브디렉토리 생성\n",
    "    directories = [\n",
    "        \"train/pos\", \"train/neg\",\n",
    "        \"val/pos\", \"val/neg\", \n",
    "        \"test/pos\", \"test/neg\"\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(os.path.join(base_dir, directory), exist_ok=True)\n",
    "    \n",
    "    print(\"폴더 구조 생성 완료\")\n",
    "    \n",
    "    # 훈련 데이터 분리\n",
    "    pos_train_texts = []\n",
    "    neg_train_texts = []\n",
    "    \n",
    "    for i in range(len(corpus.train)):\n",
    "        if corpus.train[i].label == 1:\n",
    "            pos_train_texts.append(corpus.train[i].text)\n",
    "        else:\n",
    "            neg_train_texts.append(corpus.train[i].text)\n",
    "    \n",
    "    # 테스트 데이터 분리\n",
    "    pos_test_texts = []\n",
    "    neg_test_texts = []\n",
    "    \n",
    "    for i in range(len(corpus.test)):\n",
    "        if corpus.test[i].label == 1:\n",
    "            pos_test_texts.append(corpus.test[i].text)\n",
    "        else:\n",
    "            neg_test_texts.append(corpus.test[i].text)\n",
    "    \n",
    "    # 검증 데이터 분리 (훈련 데이터에서 1000개씩)\n",
    "    pos_val_texts = pos_train_texts[:1000]\n",
    "    neg_val_texts = neg_train_texts[:1000]\n",
    "    \n",
    "    pos_train_texts = pos_train_texts[1000:]\n",
    "    neg_train_texts = neg_train_texts[1000:]\n",
    "    \n",
    "    print(f\"데이터 분할 완료:\")\n",
    "    print(f\"  Train: 긍정 {len(pos_train_texts)}개, 부정 {len(neg_train_texts)}개\")\n",
    "    print(f\"  Val: 긍정 {len(pos_val_texts)}개, 부정 {len(neg_val_texts)}개\")\n",
    "    print(f\"  Test: 긍정 {len(pos_test_texts)}개, 부정 {len(neg_test_texts)}개\")\n",
    "    \n",
    "    # 파일로 저장\n",
    "    datasets = [\n",
    "        (pos_train_texts, \"train/pos\", \"pos\"),\n",
    "        (neg_train_texts, \"train/neg\", \"neg\"),\n",
    "        (pos_val_texts, \"val/pos\", \"pos\"),\n",
    "        (neg_val_texts, \"val/neg\", \"neg\"),\n",
    "        (pos_test_texts, \"test/pos\", \"pos\"),\n",
    "        (neg_test_texts, \"test/neg\", \"neg\")\n",
    "    ]\n",
    "    \n",
    "    for texts, folder, prefix in datasets:\n",
    "        for i, text in enumerate(texts):\n",
    "            file_path = os.path.join(base_dir, folder, f\"{prefix}_{i}.txt\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "    \n",
    "    print(\"파일 저장 완료\")\n",
    "    return base_dir\n",
    "\n",
    "# 데이터셋 생성 (주석 해제하여 실행)\n",
    "# korean_data_dir = create_korean_dataset()\n",
    "korean_data_dir = \"korean_imdb\"  # 이미 생성된 폴더 사용\n",
    "print(f\"데이터셋 디렉토리: {korean_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorFlow 데이터셋 로드\n",
    "\n",
    "Keras의 `text_dataset_from_directory`를 사용하여 데이터셋을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈 설정\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "    korean_data_dir + \"/train\", \n",
    "    batch_size=batch_size, \n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "    korean_data_dir + \"/val\", \n",
    "    batch_size=batch_size, \n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_ds_raw = keras.utils.text_dataset_from_directory(\n",
    "    korean_data_dir + \"/test\", \n",
    "    batch_size=batch_size, \n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "print(\"데이터셋 로드 완료\")\n",
    "print(f\"배치 사이즈: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 한국어 텍스트 전처리\n",
    "\n",
    "한국어 특성을 고려한 텍스트 전처리 파이프라인을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    한국어 텍스트 정제 함수\n",
    "    - UTF-8 디코딩\n",
    "    - 소문자 변환\n",
    "    - 한글, 영문, 숫자, 공백만 유지\n",
    "    \"\"\"\n",
    "    # TensorFlow 텐서를 Python 문자열로 디코딩\n",
    "    text = text.decode(\"utf-8\")\n",
    "    text = text.lower()\n",
    "    # 한글, 영문, 숫자, 공백만 유지\n",
    "    text = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def python_korean_preprocess(text_tensor):\n",
    "    \"\"\"\n",
    "    한국어 전처리 함수\n",
    "    - 텍스트 정제\n",
    "    - 형태소 분석 및 토큰화\n",
    "    \"\"\"\n",
    "    processed_text = []\n",
    "    \n",
    "    for text_bytes in text_tensor.numpy():\n",
    "        # 텍스트 정제\n",
    "        cleaned_text = clean_text(text_bytes)\n",
    "        # 형태소 분석 및 공백으로 연결\n",
    "        morphed_text = \" \".join(okt.morphs(cleaned_text))\n",
    "        processed_text.append(morphed_text)\n",
    "    \n",
    "    return tf.constant(processed_text, dtype=tf.string)\n",
    "\n",
    "def tf_korean_preprocess_fn(texts, labels):\n",
    "    \"\"\"\n",
    "    TensorFlow 데이터셋용 전처리 래퍼 함수\n",
    "    Python 함수를 TensorFlow 그래프에 통합\n",
    "    \"\"\"\n",
    "    processed_texts = tf.py_function(\n",
    "        func=python_korean_preprocess,\n",
    "        inp=[texts],\n",
    "        Tout=tf.string\n",
    "    )\n",
    "    # 명시적 shape 설정\n",
    "    processed_texts.set_shape(texts.get_shape())\n",
    "    return processed_texts, labels\n",
    "\n",
    "print(\"전처리 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 텍스트 벡터화 설정\n",
    "\n",
    "TextVectorization 레이어를 설정하여 텍스트를 수치 데이터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화 파라미터 설정\n",
    "max_tokens = 10000      # 어휘사전 크기 (상위 10,000개 단어)\n",
    "output_sequence = 20    # 문장 길이 제한 (20개 토큰)\n",
    "\n",
    "# TextVectorization 레이어 생성\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",                    # 정수 인덱스로 출력\n",
    "    output_sequence_length=output_sequence,\n",
    "    standardize=None,                     # 별도 표준화 진행\n",
    "    split=\"whitespace\"                    # 공백 기준 토큰 분리\n",
    ")\n",
    "\n",
    "print(f\"벡터화 설정 완료:\")\n",
    "print(f\"  최대 토큰 수: {max_tokens}\")\n",
    "print(f\"  시퀀스 길이: {output_sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 데이터 전처리 파이프라인 적용\n",
    "\n",
    "모든 데이터셋에 전처리를 적용하고 어휘사전을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터셋에 전처리 적용\n",
    "print(\"데이터 전처리 시작...\")\n",
    "\n",
    "train_ds_processed = train_ds_raw.map(\n",
    "    tf_korean_preprocess_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "val_ds_processed = val_ds_raw.map(\n",
    "    tf_korean_preprocess_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "test_ds_processed = test_ds_raw.map(\n",
    "    tf_korean_preprocess_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"전처리 완료\")\n",
    "\n",
    "# 어휘사전 구축 (훈련 데이터만 사용)\n",
    "print(\"어휘사전 구축 중...\")\n",
    "vectorizer.adapt(train_ds_processed.map(lambda x, y: x))\n",
    "print(\"어휘사전 구축 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 텍스트 벡터화 및 데이터 최적화\n",
    "\n",
    "전처리된 텍스트를 벡터로 변환하고 성능 최적화를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_fn(texts, labels):\n",
    "    \"\"\"텍스트를 벡터로 변환하는 함수\"\"\"\n",
    "    return vectorizer(texts), labels\n",
    "\n",
    "# 텍스트 벡터화 적용\n",
    "train_ds_vectorized = train_ds_processed.map(\n",
    "    vectorize_text_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "val_ds_vectorized = val_ds_processed.map(\n",
    "    vectorize_text_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "test_ds_vectorized = test_ds_processed.map(\n",
    "    vectorize_text_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# 성능 최적화: 캐시 및 프리패치\n",
    "train_ds_vectorized = train_ds_vectorized.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds_vectorized = val_ds_vectorized.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds_vectorized = test_ds_vectorized.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"벡터화 및 최적화 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 데이터 확인 및 검증\n",
    "\n",
    "처리된 데이터의 구조와 내용을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘사전 확인\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "print(f\"어휘사전 크기: {len(vocabulary)}\")\n",
    "print(f\"상위 20개 단어: {vocabulary[:20]}\")\n",
    "\n",
    "# 벡터화된 데이터 확인\n",
    "for text_batch, label_batch in train_ds_vectorized.take(1):\n",
    "    print(f\"\\n텍스트 배치 shape: {text_batch.shape}\")\n",
    "    print(f\"라벨 배치 shape: {label_batch.shape}\")\n",
    "    print(f\"첫 번째 샘플 벡터 (처음 10개): {text_batch[0, :10].numpy()}\")\n",
    "    \n",
    "    # 역변환하여 확인\n",
    "    decoded = \" \".join(\n",
    "        vocabulary[idx] for idx in text_batch[0, :10].numpy() if idx > 1\n",
    "    )\n",
    "    print(f\"역변환 결과: {decoded}\")\n",
    "\n",
    "print(\"\\n데이터 확인 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 사전 훈련된 임베딩 로드\n",
    "\n",
    "FastText 한국어 사전 훈련된 벡터를 로드하여 임베딩 행렬을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 Word2Vec 벡터 로드\n",
    "korean_word2vec_path = \"cc.ko.300.vec\"\n",
    "\n",
    "try:\n",
    "    # 메모리 절약을 위해 상위 50,000개만 로드\n",
    "    korean_word_vector = KeyedVectors.load_word2vec_format(\n",
    "        korean_word2vec_path, \n",
    "        binary=False, \n",
    "        encoding=\"utf-8\", \n",
    "        limit=50000\n",
    "    )\n",
    "    \n",
    "    print(f\"사전 훈련된 벡터 로드 완료\")\n",
    "    print(f\"벡터 차원: {korean_word_vector.vector_size}\")\n",
    "    \n",
    "    embedding_dim = korean_word_vector.vector_size\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"경고: {korean_word2vec_path} 파일을 찾을 수 없습니다.\")\n",
    "    print(\"임베딩 차원을 300으로 설정하고 랜덤 초기화를 사용합니다.\")\n",
    "    embedding_dim = 300\n",
    "    korean_word_vector = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 행렬 구축\n",
    "voca_size = vectorizer.vocabulary_size()\n",
    "embedding_matrix = np.zeros((voca_size, embedding_dim))\n",
    "\n",
    "if korean_word_vector is not None:\n",
    "    # 사전 훈련된 벡터로 초기화\n",
    "    found_words = 0\n",
    "    \n",
    "    for i, word in enumerate(vocabulary):\n",
    "        if i < voca_size:\n",
    "            try:\n",
    "                embedding_vector = korean_word_vector[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                found_words += 1\n",
    "            except KeyError:\n",
    "                # 벡터가 없는 단어는 0으로 유지\n",
    "                pass\n",
    "    \n",
    "    print(f\"임베딩 행렬 구축 완료\")\n",
    "    print(f\"전체 단어: {voca_size}, 사전 훈련된 벡터 활용: {found_words}개\")\n",
    "    print(f\"활용률: {found_words/voca_size*100:.1f}%\")\n",
    "else:\n",
    "    print(\"랜덤 초기화로 임베딩 행렬 생성\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 딥러닝 모델 구축\n",
    "\n",
    "Bidirectional LSTM 기반의 감성 분류 모델을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 아키텍처 정의\n",
    "inputs = keras.Input(shape=(None,), dtype=tf.int64)\n",
    "\n",
    "# 임베딩 레이어\n",
    "if korean_word_vector is not None:\n",
    "    # 사전 훈련된 임베딩 사용\n",
    "    x = layers.Embedding(\n",
    "        input_dim=voca_size,\n",
    "        output_dim=embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,  # 사전 훈련된 가중치 고정\n",
    "        mask_zero=True    # 패딩 토큰(0) 마스킹\n",
    "    )(inputs)\n",
    "else:\n",
    "    # 랜덤 초기화 임베딩\n",
    "    x = layers.Embedding(\n",
    "        input_dim=voca_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True\n",
    "    )(inputs)\n",
    "\n",
    "# Bidirectional LSTM 레이어\n",
    "x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "\n",
    "# 드롭아웃으로 과적합 방지\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# 출력 레이어 (이진 분류)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 구조 출력\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 모델 훈련\n",
    "\n",
    "구축된 모델을 훈련하고 최적의 가중치를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 설정\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"korean_rnn_model.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    train_ds_vectorized,\n",
    "    validation_data=val_ds_vectorized,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 훈련 기록 저장\n",
    "with open(\"korean_rnn_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "print(\"\\n모델 훈련 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 모델 평가\n",
    "\n",
    "테스트 데이터셋을 사용하여 모델 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "test_loss, test_accuracy = model.evaluate(test_ds_vectorized, verbose=0)\n",
    "\n",
    "print(f\"=== 모델 평가 결과 ===\")\n",
    "print(f\"테스트 손실: {test_loss:.4f}\")\n",
    "print(f\"테스트 정확도: {test_accuracy:.4f}\")\n",
    "\n",
    "# 훈련 과정 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='훈련 정확도')\n",
    "plt.plot(history.history['val_accuracy'], label='검증 정확도')\n",
    "plt.title('모델 정확도')\n",
    "plt.xlabel('에포크')\n",
    "plt.ylabel('정확도')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='훈련 손실')\n",
    "plt.plot(history.history['val_loss'], label='검증 손실')\n",
    "plt.title('모델 손실')\n",
    "plt.xlabel('에포크')\n",
    "plt.ylabel('손실')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 모델 예측 및 테스트\n",
    "\n",
    "훈련된 모델을 사용하여 새로운 한국어 텍스트의 감성을 예측해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(texts):\n",
    "    \"\"\"\n",
    "    텍스트 리스트의 감성을 예측하는 함수\n",
    "    \"\"\"\n",
    "    # 텍스트 전처리\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # 문자열을 바이트로 인코딩한 후 전처리\n",
    "        cleaned_text = clean_text(text.encode('utf-8'))\n",
    "        morphed_text = \" \".join(okt.morphs(cleaned_text))\n",
    "        processed_texts.append(morphed_text)\n",
    "    \n",
    "    # 벡터화\n",
    "    vectorized_texts = vectorizer(tf.constant(processed_texts, dtype=tf.string))\n",
    "    \n",
    "    # 예측\n",
    "    predictions = model.predict(vectorized_texts, verbose=0)\n",
    "    \n",
    "    return predictions.flatten()\n",
    "\n",
    "# 테스트 샘플\n",
    "test_samples = [\n",
    "    \"이 영화는 정말 감동적이었어요\",\n",
    "    \"영화 수준이 좀 초등학생용 같아요\", \n",
    "    \"개연성도 없고 시나리오 발로 썼냐\",\n",
    "    \"반드시 봐야할 영화입니다. 인생을 풍요롭게 하는 영화였어요\",\n",
    "    \"배우 하나로 이런 영화가 만들어질수 있다는게 믿기지 않습니다\",\n",
    "    \"정말 재미없고 지루한 영화였습니다\",\n",
    "    \"최고의 작품이에요! 강력 추천합니다\"\n",
    "]\n",
    "\n",
    "# 감성 예측\n",
    "probabilities = predict_sentiment(test_samples)\n",
    "\n",
    "print(\"=== 감성 분석 결과 ===\")\n",
    "print()\n",
    "\n",
    "for i, text in enumerate(test_samples):\n",
    "    prob = probabilities[i]\n",
    "    sentiment = \"긍정\" if prob >= 0.5 else \"부정\"\n",
    "    confidence = prob if prob >= 0.5 else (1 - prob)\n",
    "    \n",
    "    print(f\"텍스트: {text[:40]}{'...' if len(text) > 40 else ''}\")\n",
    "    print(f\"감성: {sentiment} (신뢰도: {confidence:.3f})\")\n",
    "    print(f\"긍정 확률: {prob:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 결론 및 개선 방향\n",
    "\n",
    "### 구현 결과\n",
    "- **데이터셋**: NSMC 15만개 영화 리뷰\n",
    "- **전처리**: KoNLPy Okt 형태소 분석기 활용\n",
    "- **임베딩**: FastText 사전 훈련된 300차원 벡터\n",
    "- **모델**: Bidirectional LSTM (32 units)\n",
    "- **성능**: 테스트 정확도 약 85-90%\n",
    "\n",
    "### 주요 특징\n",
    "1. **한국어 특화 전처리**: 형태소 분석을 통한 의미 단위 토큰화\n",
    "2. **사전 훈련된 임베딩**: 단어 간 의미적 관계 활용\n",
    "3. **양방향 LSTM**: 문맥의 앞뒤 정보 모두 활용\n",
    "4. **정규화 기법**: 드롭아웃을 통한 과적합 방지\n",
    "\n",
    "### 개선 방향\n",
    "1. **더 큰 모델**: Transformer 기반 모델 (BERT, KoBERT 등)\n",
    "2. **데이터 증강**: 역번역, 패러프레이징 등\n",
    "3. **앙상블 기법**: 여러 모델의 예측 결합\n",
    "4. **하이퍼파라미터 튜닝**: 그리드 서치, 베이지안 최적화\n",
    "5. **세부 도메인 특화**: 영화 외 다른 도메인 데이터 활용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}