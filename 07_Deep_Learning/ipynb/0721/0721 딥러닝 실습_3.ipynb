{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸŒ¸ ê½ƒ ë¶„ë¥˜ CNN ëª¨ë¸: Daisy vs Dandelion\n",
        "\n",
        "## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”\n",
        "\n",
        "### ğŸ¯ ëª©í‘œ\n",
        "- **ì£¼ëª©ì **: ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ê½ƒ ì´ë¯¸ì§€ ì´ì§„ ë¶„ë¥˜ ì‹œìŠ¤í…œ êµ¬í˜„\n",
        "- **ë¶„ë¥˜ ëŒ€ìƒ**: Daisy(ë°ì´ì§€) vs Dandelion(ë¯¼ë“¤ë ˆ)\n",
        "- **ê¸°ìˆ  ìŠ¤íƒ**: TensorFlow/Kerasë¥¼ í™œìš©í•œ CNN(Convolutional Neural Network)\n",
        "\n",
        "### ğŸ“Š í”„ë¡œì íŠ¸ êµ¬ì¡°\n",
        "1. **ë°ì´í„° ì¤€ë¹„**: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ì •ê·œí™”\n",
        "2. **ëª¨ë¸ ì„¤ê³„**: CNN ì•„í‚¤í…ì²˜ êµ¬ì„±\n",
        "3. **í•™ìŠµ ë° í‰ê°€**: ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ ì¸¡ì •\n",
        "4. **ê²°ê³¼ ë¶„ì„**: ì‹œê°í™” ë° ì„±ëŠ¥ ë¶„ì„\n",
        "\n",
        "### ğŸ”§ í•µì‹¬ ê¸°ëŠ¥\n",
        "- ì´ë¯¸ì§€ ë°ì´í„° ìë™ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "- ìµœì í™”ëœ CNN ëª¨ë¸ ì•„í‚¤í…ì²˜\n",
        "- ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§ ë° ì‹œê°í™”\n",
        "- ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€ ë° ë¶„ì„ ë„êµ¬\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ› ï¸ 1. í™˜ê²½ ì„¤ì •\n",
        "\n",
        "### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "ğŸš€ í™˜ê²½ ì„¤ì • ì™„ë£Œ\n",
            "==================================================\n",
            "ğŸ“Š TensorFlow version: 2.19.0\n",
            "ğŸ”¢ NumPy version: 2.1.3\n",
            "ğŸ“ˆ Matplotlib & Seaborn: ë¡œë“œ ì™„ë£Œ\n",
            "âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”\n",
        "import numpy as np \n",
        "import os \n",
        "import random \n",
        "import pandas as pd\n",
        "import PIL.Image as pilimg \n",
        "import imghdr\n",
        "\n",
        "# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# ë¨¸ì‹ ëŸ¬ë‹ ìœ í‹¸ë¦¬í‹°\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ì‹œê°í™”\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# í™˜ê²½ ì„¤ì •\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"ğŸš€ í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ“Š TensorFlow version: {tf.__version__}\")\n",
        "print(f\"ğŸ”¢ NumPy version: {np.__version__}\")\n",
        "print(\"ğŸ“ˆ Matplotlib & Seaborn: ë¡œë“œ ì™„ë£Œ\")\n",
        "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1.2 ë°ì´í„° ê²½ë¡œ ì„¤ì • ë° êµ¬ì¡° í™•ì¸\n",
        "\n",
        "ê½ƒ ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì €ì¥ëœ ê²½ë¡œë¥¼ ì„¤ì •í•˜ê³  ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ğŸ“ ì˜ˆìƒ ë°ì´í„° êµ¬ì¡°:**\n",
        "```markdown\n",
        "flowers/\n",
        "â”œâ”€â”€ train/\n",
        "â”‚   â”œâ”€â”€ daisy/      (ë°ì´ì§€ í›ˆë ¨ ì´ë¯¸ì§€)\n",
        "â”‚   â””â”€â”€ dandelion/  (ë¯¼ë“¤ë ˆ í›ˆë ¨ ì´ë¯¸ì§€)\n",
        "â””â”€â”€ test/\n",
        "    â”œâ”€â”€ daisy/      (ë°ì´ì§€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€)\n",
        "    â””â”€â”€ dandelion/  (ë¯¼ë“¤ë ˆ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "ğŸ“‚ ë°ì´í„° ê²½ë¡œ ì„¤ì • ì™„ë£Œ\n",
            "==================================================\n",
            "ğŸ¯ ì‚¬ìš© ê²½ë¡œ: ../../data/flowers\n",
            "\n",
            "ğŸ“ ë°ì´í„° êµ¬ì¡° ë° íŒŒì¼ ê°œìˆ˜:\n",
            "  ğŸ“‚ train/\n",
            "    ğŸŒ¸ daisy: 529ê°œ ì´ë¯¸ì§€\n",
            "    ğŸŒ¸ dandelion: 746ê°œ ì´ë¯¸ì§€\n",
            "  ğŸ“‚ test/\n",
            "    ğŸŒ¸ daisy: 77ê°œ ì´ë¯¸ì§€\n",
            "    ğŸŒ¸ dandelion: 105ê°œ ì´ë¯¸ì§€\n",
            "\n",
            "ğŸ“Š ì´ ì´ë¯¸ì§€ ê°œìˆ˜: 1457ê°œ\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° ê²½ë¡œ ì„¤ì • (ì—¬ëŸ¬ ì˜µì…˜ ì œê³µ)\n",
        "possible_paths = [\n",
        "    \"../../data/flowers\",  # ìƒëŒ€ ê²½ë¡œ (ê¶Œì¥)\n",
        "    \"../../../data/flowers\",  # ë‹¤ë¥¸ ìƒëŒ€ ê²½ë¡œ\n",
        "    \"C:/Users/ryan9/ë¬¸ì„œ/GitHub/SeSac-AI-Developer-Notes-2025/07_Deep_Learning/data/flowers\"  # ì ˆëŒ€ ê²½ë¡œ\n",
        "]\n",
        "\n",
        "base_path = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        base_path = path\n",
        "        break\n",
        "\n",
        "if base_path:\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ“‚ ë°ì´í„° ê²½ë¡œ ì„¤ì • ì™„ë£Œ\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"ğŸ¯ ì‚¬ìš© ê²½ë¡œ: {base_path}\")\n",
        "    print()\n",
        "    \n",
        "    # í´ë” êµ¬ì¡° ë° ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
        "    print(\"ğŸ“ ë°ì´í„° êµ¬ì¡° ë° íŒŒì¼ ê°œìˆ˜:\")\n",
        "    total_images = 0\n",
        "    \n",
        "    for split in ['train', 'test']:\n",
        "        split_path = os.path.join(base_path, split)\n",
        "        if os.path.exists(split_path):\n",
        "            print(f\"  ğŸ“‚ {split}/\")\n",
        "            for flower in ['daisy', 'dandelion']:\n",
        "                flower_path = os.path.join(split_path, flower)\n",
        "                if os.path.exists(flower_path):\n",
        "                    count = len([f for f in os.listdir(flower_path) \n",
        "                               if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))])\n",
        "                    total_images += count\n",
        "                    print(f\"    ğŸŒ¸ {flower}: {count}ê°œ ì´ë¯¸ì§€\")\n",
        "                else:\n",
        "                    print(f\"    âŒ {flower}: í´ë” ì—†ìŒ\")\n",
        "        else:\n",
        "            print(f\"  âŒ {split}/: í´ë” ì—†ìŒ\")\n",
        "    \n",
        "    print(f\"\\nğŸ“Š ì´ ì´ë¯¸ì§€ ê°œìˆ˜: {total_images}ê°œ\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
        "    for path in possible_paths:\n",
        "        print(f\"  - {path}\")\n",
        "    print(\"\\nğŸ’¡ íŒíŠ¸: ë…¸íŠ¸ë¶ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“Š 2. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "### 2.1 ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "\n",
        "ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì½ì–´ì„œ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  `.npz` íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ”„ ì£¼ìš” ì „ì²˜ë¦¬ ë‹¨ê³„:\n",
        "1. **ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬**: gif, png, jpeg, jpg í˜•ì‹ í™•ì¸\n",
        "2. **í¬ê¸° ì •ê·œí™”**: ëª¨ë“  ì´ë¯¸ì§€ë¥¼ 80Ã—80 í”½ì…€ë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n",
        "3. **ë ˆì´ë¸” ì¸ì½”ë”©**: daisy=0, dandelion=1\n",
        "4. **ë°ì´í„° ì§ë ¬í™”**: ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ .npz í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
        "\n",
        "#### âš™ï¸ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­:\n",
        "- **ì´ë¯¸ì§€ í¬ê¸°**: 80Ã—80Ã—3 (RGB ì±„ë„)\n",
        "- **ë°ì´í„° í˜•ì‹**: NumPy ë°°ì—´ â†’ NPZ ì••ì¶• í˜•ì‹\n",
        "- **ë©”ëª¨ë¦¬ ìµœì í™”**: ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def makeData(flower_name, label, isTrain=True):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ê½ƒ ì¢…ë¥˜ì˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ NumPy ë°°ì—´ë¡œ ë³€í™˜\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    flower_name : str\n",
        "        ê½ƒ ì´ë¦„ ('daisy' ë˜ëŠ” 'dandelion')\n",
        "    label : int\n",
        "        ë ˆì´ë¸” ê°’ (daisy=0, dandelion=1)\n",
        "    isTrain : bool\n",
        "        Trueë©´ train í´ë”, Falseë©´ test í´ë”ì—ì„œ ë°ì´í„° ë¡œë“œ\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    int : ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ìˆ˜\n",
        "    \"\"\"\n",
        "    # ê²½ë¡œ ì„¤ì •\n",
        "    if base_path is None:\n",
        "        print(\"âŒ base_pathê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ê²½ë¡œë¥¼ ì„¤ì •í•˜ì„¸ìš”.\")\n",
        "        return 0\n",
        "        \n",
        "    subset = \"train\" if isTrain else \"test\"\n",
        "    path = os.path.join(base_path, subset, flower_name)\n",
        "    \n",
        "    print(f\"ğŸ”„ ì²˜ë¦¬ ì¤‘: {subset}/{flower_name}\")\n",
        "    print(f\"ğŸ“‚ ê²½ë¡œ: {path}\")\n",
        "    \n",
        "    if not os.path.exists(path):\n",
        "        print(f\"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}\")\n",
        "        return 0\n",
        "    \n",
        "    data = []\n",
        "    labels = []\n",
        "    error_count = 0\n",
        "    \n",
        "    # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "    image_files = [f for f in os.listdir(path) \n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
        "    total_files = len(image_files)\n",
        "    \n",
        "    print(f\"ğŸ“Š ì´ {total_files}ê°œ ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬\")\n",
        "    \n",
        "    # ì´ë¯¸ì§€ ì²˜ë¦¬\n",
        "    for i, filename in enumerate(image_files, 1):\n",
        "        try:\n",
        "            # ì§„í–‰ë¥  í‘œì‹œ\n",
        "            if i % 100 == 0 or i == total_files:\n",
        "                progress = (i / total_files) * 100\n",
        "                print(f\"ğŸ“ˆ ì§„í–‰ë¥ : {i}/{total_files} ({progress:.1f}%)\")\n",
        "            \n",
        "            file_path = os.path.join(path, filename)\n",
        "            \n",
        "            # ì´ë¯¸ì§€ í˜•ì‹ í™•ì¸\n",
        "            kind = imghdr.what(file_path)\n",
        "            if kind not in [\"gif\", \"png\", \"jpeg\", \"jpg\"]:\n",
        "                continue\n",
        "                \n",
        "            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "            with pilimg.open(file_path) as img:\n",
        "                # RGB ë³€í™˜ (RGBAë‚˜ ë‹¤ë¥¸ í˜•ì‹ ì²˜ë¦¬)\n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "                \n",
        "                # í¬ê¸° ì¡°ì •\n",
        "                resize_img = img.resize((80, 80), pilimg.Resampling.LANCZOS)\n",
        "                pixel = np.array(resize_img)\n",
        "                \n",
        "                # í˜•íƒœ ê²€ì¦\n",
        "                if pixel.shape == (80, 80, 3):\n",
        "                    data.append(pixel)\n",
        "                    labels.append(label)\n",
        "                    \n",
        "        except Exception as e:\n",
        "            error_count += 1\n",
        "            if error_count <= 5:  # ì²˜ìŒ 5ê°œ ì˜¤ë¥˜ë§Œ ì¶œë ¥\n",
        "                print(f\"âš ï¸  {filename} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "            elif error_count == 6:\n",
        "                print(\"âš ï¸  ì¶”ê°€ ì˜¤ë¥˜ë“¤ì€ ë¡œê·¸ì—ì„œ ìƒëµë©ë‹ˆë‹¤...\")\n",
        "    \n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    savefileName = f\"C:/Users/ryan9/ë¬¸ì„œ/GitHub/SeSac-AI-Developer-Notes-2025/07_Deep_Learning/data/npz/imagedata{label}_{subset}.npz\"\n",
        "    \n",
        "    if data:\n",
        "        np.savez(savefileName, data=np.array(data), targets=np.array(labels))\n",
        "        print(f\"âœ… ì €ì¥ ì™„ë£Œ: {savefileName}\")\n",
        "        print(f\"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {len(data)}ê°œ ì´ë¯¸ì§€ ì„±ê³µ, {error_count}ê°œ ì˜¤ë¥˜\")\n",
        "    else:\n",
        "        print(f\"âŒ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    \n",
        "    return len(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initData():\n",
        "    \"\"\"\n",
        "    ëª¨ë“  ê½ƒ ì¢…ë¥˜ì˜ train/test ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ .npz íŒŒì¼ë¡œ ì €ì¥\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    bool : ì„±ê³µ ì—¬ë¶€\n",
        "    \"\"\"\n",
        "    flowers = [\"daisy\", \"dandelion\"]\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸš€ ë°ì´í„° ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤ ì‹œì‘\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # base_path í™•ì¸\n",
        "    if base_path is None:\n",
        "        print(\"âŒ ë°ì´í„° ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "        return False\n",
        "    \n",
        "    # ê¸°ì¡´ íŒŒì¼ í™•ì¸\n",
        "    existing_files = [f for f in os.listdir('.') \n",
        "                     if f.endswith('.npz') and f.startswith('imagedata')]\n",
        "    \n",
        "    if existing_files:\n",
        "        print(\"ğŸ“ ê¸°ì¡´ .npz íŒŒì¼ ë°œê²¬:\")\n",
        "        for file in existing_files:\n",
        "            size = os.path.getsize(file) / (1024*1024)\n",
        "            print(f\"  - {file} ({size:.2f} MB)\")\n",
        "        \n",
        "        response = input(\"\\në®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): \").lower()\n",
        "        if response != 'y':\n",
        "            print(\"ğŸš« ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "            return False\n",
        "        print()\n",
        "    \n",
        "    processing_results = []\n",
        "    total_processed = 0\n",
        "    start_time = pd.Timestamp.now()\n",
        "    \n",
        "    # ê° ê½ƒ ì¢…ë¥˜ë³„ ì²˜ë¦¬\n",
        "    for i, flower in enumerate(flowers):\n",
        "        print(f\"\\nğŸŒ¸ {flower.upper()} ì²˜ë¦¬ ì¤‘... (ë ˆì´ë¸”: {i})\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        flower_results = {}\n",
        "        \n",
        "        # Train ë°ì´í„° ì²˜ë¦¬\n",
        "        print(f\"ğŸ“‚ í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬\")\n",
        "        train_count = makeData(flower, i, True)\n",
        "        flower_results['train'] = train_count\n",
        "        \n",
        "        print(f\"\\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬\")\n",
        "        test_count = makeData(flower, i, False)\n",
        "        flower_results['test'] = test_count\n",
        "        \n",
        "        total_count = train_count + test_count\n",
        "        total_processed += total_count\n",
        "        \n",
        "        processing_results.append({\n",
        "            'flower': flower,\n",
        "            'label': i,\n",
        "            'train_count': train_count,\n",
        "            'test_count': test_count,\n",
        "            'total_count': total_count\n",
        "        })\n",
        "        \n",
        "        print(f\"âœ… {flower} ì™„ë£Œ: Train {train_count}ê°œ, Test {test_count}ê°œ, ì´ {total_count}ê°œ\")\n",
        "    \n",
        "    # ì²˜ë¦¬ ì™„ë£Œ ë° ê²°ê³¼ ìš”ì•½\n",
        "    end_time = pd.Timestamp.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ‰ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥\n",
        "    print(\"\\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:\")\n",
        "    result_df = pd.DataFrame(processing_results)\n",
        "    print(result_df.to_string(index=False))\n",
        "    \n",
        "    print(f\"\\nâ±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {duration:.1f}ì´ˆ\")\n",
        "    print(f\"ğŸ“ˆ ì´ ì²˜ë¦¬ ì´ë¯¸ì§€: {total_processed}ê°œ\")\n",
        "    print(f\"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {total_processed/duration:.1f}ê°œ/ì´ˆ\")\n",
        "    \n",
        "    # ìƒì„±ëœ íŒŒì¼ ëª©ë¡ ë° í¬ê¸° í™•ì¸\n",
        "    print(\"\\nğŸ“„ ìƒì„±ëœ .npz íŒŒì¼ë“¤:\")\n",
        "    npz_files = [f for f in os.listdir('.') \n",
        "                if f.endswith('.npz') and f.startswith('imagedata')]\n",
        "    \n",
        "    total_size = 0\n",
        "    for file in sorted(npz_files):\n",
        "        size = os.path.getsize(file) / (1024*1024)  # MB ë‹¨ìœ„\n",
        "        total_size += size\n",
        "        print(f\"  âœ… {file} ({size:.2f} MB)\")\n",
        "    \n",
        "    print(f\"\\nğŸ’¾ ì´ íŒŒì¼ í¬ê¸°: {total_size:.2f} MB\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return total_processed > 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.2 ë°ì´í„° ë¡œë”© ë° ì •ê·œí™” í•¨ìˆ˜\n",
        "\n",
        "ì €ì¥ëœ `.npz` íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ í•™ìŠµìš© ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData():\n",
        "    \"\"\"\n",
        "    ì €ì¥ëœ .npz íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ train/test ë°ì´í„°ì…‹ ë°˜í™˜\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (X_train, y_train, X_test, y_test)\n",
        "        - X_train, X_test : ì´ë¯¸ì§€ ë°ì´í„° (numpy.ndarray)\n",
        "        - y_train, y_test : ë ˆì´ë¸” ë°ì´í„° (numpy.ndarray)\n",
        "        ì‹¤íŒ¨ ì‹œ (None, None, None, None) ë°˜í™˜\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ“‚ ë°ì´í„° ë¡œë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # í•„ìš”í•œ íŒŒì¼ ëª©ë¡\n",
        "    required_files = [\n",
        "        \"imagedata0_train.npz\",  # daisy train\n",
        "        \"imagedata1_train.npz\",  # dandelion train  \n",
        "        \"imagedata0_test.npz\",   # daisy test\n",
        "        \"imagedata1_test.npz\"    # dandelion test\n",
        "    ]\n",
        "    \n",
        "    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "    if missing_files:\n",
        "        print(\"âŒ ëˆ„ë½ëœ ë°ì´í„° íŒŒì¼:\")\n",
        "        for file in missing_files:\n",
        "            print(f\"  - {file}\")\n",
        "        print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•: initData() í•¨ìˆ˜ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    print(\"âœ… ëª¨ë“  í•„ìš”í•œ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
        "    \n",
        "    datasets = {}\n",
        "    \n",
        "    # Train ë°ì´í„° ë¡œë”©\n",
        "    try:\n",
        "        print(\"\\nğŸ”„ í›ˆë ¨ ë°ì´í„° ë¡œë”©...\")\n",
        "        \n",
        "        # Daisy train ë°ì´í„°\n",
        "        with np.load(\"imagedata0_train.npz\") as f:\n",
        "            daisy_train_data = f[\"data\"]\n",
        "            daisy_train_labels = f[\"targets\"]\n",
        "        \n",
        "        # Dandelion train ë°ì´í„°  \n",
        "        with np.load(\"imagedata1_train.npz\") as f:\n",
        "            dandelion_train_data = f[\"data\"]\n",
        "            dandelion_train_labels = f[\"targets\"]\n",
        "        \n",
        "        # í›ˆë ¨ ë°ì´í„° ê²°í•©\n",
        "        X_train = np.concatenate([daisy_train_data, dandelion_train_data], axis=0)\n",
        "        y_train = np.concatenate([daisy_train_labels, dandelion_train_labels], axis=0)\n",
        "        \n",
        "        print(f\"  ğŸ“Š Daisy í›ˆë ¨ ì´ë¯¸ì§€: {len(daisy_train_data)}ê°œ\")\n",
        "        print(f\"  ğŸ“Š Dandelion í›ˆë ¨ ì´ë¯¸ì§€: {len(dandelion_train_data)}ê°œ\")\n",
        "        print(f\"  ğŸ“Š ì´ í›ˆë ¨ ë°ì´í„°: {X_train.shape}\")\n",
        "        \n",
        "        datasets['train'] = (X_train, y_train)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ í›ˆë ¨ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Test ë°ì´í„° ë¡œë”©\n",
        "    try:\n",
        "        print(\"\\nğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...\")\n",
        "        \n",
        "        # Daisy test ë°ì´í„°\n",
        "        with np.load(\"../../imagedata0_test.npz\") as f:\n",
        "            daisy_test_data = f[\"data\"]\n",
        "            daisy_test_labels = f[\"targets\"]\n",
        "        \n",
        "        # Dandelion test ë°ì´í„°\n",
        "        with np.load(\"../../imagedata1_test.npz\") as f:\n",
        "            dandelion_test_data = f[\"data\"]\n",
        "            dandelion_test_labels = f[\"targets\"]\n",
        "        \n",
        "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²°í•©\n",
        "        X_test = np.concatenate([daisy_test_data, dandelion_test_data], axis=0)\n",
        "        y_test = np.concatenate([daisy_test_labels, dandelion_test_labels], axis=0)\n",
        "        \n",
        "        print(f\"  ğŸ“Š Daisy í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(daisy_test_data)}ê°œ\")\n",
        "        print(f\"  ğŸ“Š Dandelion í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(dandelion_test_data)}ê°œ\")\n",
        "        print(f\"  ğŸ“Š ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}\")\n",
        "        \n",
        "        datasets['test'] = (X_test, y_test)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
        "        return X_train, y_train, None, None\n",
        "\n",
        "    # ë°ì´í„° ìš”ì•½ ë° ê²€ì¦\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š ë°ì´í„°ì…‹ ìš”ì•½ ë° ê²€ì¦\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    print(f\"ğŸ”¹ í›ˆë ¨ ë°ì´í„°: {X_train.shape} | ë ˆì´ë¸”: {y_train.shape}\")\n",
        "    print(f\"ğŸ”¹ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape} | ë ˆì´ë¸”: {y_test.shape}\")\n",
        "    print(f\"ğŸ”¹ ì´ë¯¸ì§€ í¬ê¸°: {X_train[0].shape}\")\n",
        "    print(f\"ğŸ”¹ í”½ì…€ ê°’ ë²”ìœ„: {X_train.min()} ~ {X_train.max()}\")\n",
        "    print(f\"ğŸ”¹ ë°ì´í„° íƒ€ì…: {X_train.dtype}\")\n",
        "    \n",
        "    # ë ˆì´ë¸” ë¶„í¬ í™•ì¸\n",
        "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ ë ˆì´ë¸” ë¶„í¬:\")\n",
        "    print(f\"  ğŸ“š í›ˆë ¨ ì„¸íŠ¸: Daisy({unique_train[0]})={counts_train[0]}ê°œ, Dandelion({unique_train[1]})={counts_train[1]}ê°œ\")\n",
        "    print(f\"  ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: Daisy({unique_test[0]})={counts_test[0]}ê°œ, Dandelion({unique_test[1]})={counts_test[1]}ê°œ\")\n",
        "    \n",
        "    # í´ë˜ìŠ¤ ê· í˜• í™•ì¸\n",
        "    train_balance = counts_train[0] / counts_train[1]\n",
        "    test_balance = counts_test[0] / counts_test[1]\n",
        "    print(f\"\\nâš–ï¸  í´ë˜ìŠ¤ ê· í˜•:\")\n",
        "    print(f\"  ğŸ“š í›ˆë ¨ ì„¸íŠ¸ ë¹„ìœ¨: {train_balance:.2f}\")\n",
        "    print(f\"  ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨: {test_balance:.2f}\")\n",
        "    \n",
        "    if 0.5 <= train_balance <= 2.0 and 0.5 <= test_balance <= 2.0:\n",
        "        print(\"  âœ… í´ë˜ìŠ¤ ê· í˜•ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(\"  âš ï¸  í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° ì¦ê°•ì„ ê³ ë ¤í•˜ì„¸ìš”.\")\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ§  3. CNN ëª¨ë¸ ì„¤ê³„\n",
        "\n",
        "### 3.1 ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„\n",
        "\n",
        "Convolutional Neural Networkë¥¼ êµ¬ì„±í•˜ì—¬ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°:\n",
        "```\n",
        "ì…ë ¥ ë ˆì´ì–´: (80, 80, 3)\n",
        "    â†“\n",
        "Conv2D(64 filters, 3Ã—3) + ReLU\n",
        "    â†“\n",
        "MaxPooling2D(2Ã—2)  # íŠ¹ì§•ë§µ í¬ê¸° ì¶•ì†Œ\n",
        "    â†“\n",
        "Conv2D(32 filters, 3Ã—3) + ReLU  \n",
        "    â†“\n",
        "Flatten  # 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
        "    â†“\n",
        "Dense(128) + ReLU\n",
        "    â†“\n",
        "Dense(32) + ReLU\n",
        "    â†“\n",
        "Dense(1) + Sigmoid  # ì´ì§„ ë¶„ë¥˜ìš© ì¶œë ¥\n",
        "```\n",
        "\n",
        "#### ğŸ“Š ì„¤ê³„ ì›ë¦¬:\n",
        "- **ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´**: ì´ë¯¸ì§€ì˜ ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ\n",
        "- **í’€ë§ ë ˆì´ì–´**: ì°¨ì› ì¶•ì†Œ ë° ê³¼ì í•© ë°©ì§€\n",
        "- **ì™„ì „ì—°ê²° ë ˆì´ì–´**: ë¶„ë¥˜ë¥¼ ìœ„í•œ ê³ ì°¨ì› íŠ¹ì§• í•™ìŠµ\n",
        "- **ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™”**: ì´ì§„ ë¶„ë¥˜ í™•ë¥  ì¶œë ¥ (0~1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createModel(input_shape=(80, 80, 3), summary=True):\n",
        "    \"\"\"\n",
        "    CNN ëª¨ë¸ì„ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•˜ì—¬ ë°˜í™˜\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_shape : tuple\n",
        "        ì…ë ¥ ì´ë¯¸ì§€ì˜ í˜•íƒœ (height, width, channels)\n",
        "    summary : bool\n",
        "        ëª¨ë¸ êµ¬ì¡° ìš”ì•½ ì¶œë ¥ ì—¬ë¶€\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tf.keras.Model : ì»´íŒŒì¼ëœ Keras ëª¨ë¸\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ—ï¸ CNN ëª¨ë¸ ìƒì„± ì‹œì‘\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜\n",
        "    network = models.Sequential([\n",
        "        # ì…ë ¥ ë ˆì´ì–´\n",
        "        layers.Input(shape=input_shape),\n",
        "        \n",
        "        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡\n",
        "        layers.Conv2D(\n",
        "            filters=64, \n",
        "            kernel_size=(3, 3), \n",
        "            activation='relu',\n",
        "            padding='same',\n",
        "            name='conv2d_1'\n",
        "        ),\n",
        "        layers.MaxPooling2D(\n",
        "            pool_size=(2, 2),\n",
        "            name='maxpool2d_1'\n",
        "        ),\n",
        "        \n",
        "        # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡\n",
        "        layers.Conv2D(\n",
        "            filters=32, \n",
        "            kernel_size=(3, 3), \n",
        "            activation='relu',\n",
        "            padding='same',\n",
        "            name='conv2d_2'\n",
        "        ),\n",
        "        layers.MaxPooling2D(\n",
        "            pool_size=(2, 2),\n",
        "            name='maxpool2d_2'\n",
        "        ),\n",
        "        \n",
        "        # ë¶„ë¥˜ê¸° ë¶€ë¶„\n",
        "        layers.Flatten(name='flatten'),\n",
        "        \n",
        "        # ì™„ì „ì—°ê²° ë ˆì´ì–´ë“¤\n",
        "        layers.Dense(\n",
        "            units=128, \n",
        "            activation='relu',\n",
        "            name='dense_1'\n",
        "        ),\n",
        "        layers.Dropout(0.3, name='dropout_1'),  # ê³¼ì í•© ë°©ì§€\n",
        "        \n",
        "        layers.Dense(\n",
        "            units=32, \n",
        "            activation='relu',\n",
        "            name='dense_2'\n",
        "        ),\n",
        "        layers.Dropout(0.2, name='dropout_2'),  # ê³¼ì í•© ë°©ì§€\n",
        "        \n",
        "        # ì¶œë ¥ ë ˆì´ì–´ (ì´ì§„ ë¶„ë¥˜)\n",
        "        layers.Dense(\n",
        "            units=1, \n",
        "            activation='sigmoid',\n",
        "            name='output'\n",
        "        )\n",
        "    ], name='FlowerClassifier')\n",
        "    \n",
        "    # ëª¨ë¸ ì»´íŒŒì¼\n",
        "    network.compile(\n",
        "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',  # ì´ì§„ë¶„ë¥˜ìš© ì†ì‹¤í•¨ìˆ˜\n",
        "        metrics=['accuracy', 'precision', 'recall']  # ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼ ì™„ë£Œ!\")\n",
        "    \n",
        "    # ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
        "    if summary:\n",
        "        print(\"\\nğŸ“‹ ëª¨ë¸ êµ¬ì¡° ìš”ì•½:\")\n",
        "        print(\"-\" * 50)\n",
        "        network.summary()\n",
        "        \n",
        "        # ì¶”ê°€ ëª¨ë¸ ì •ë³´\n",
        "        total_params = network.count_params()\n",
        "        trainable_params = sum([tf.keras.backend.count_params(w) for w in network.trainable_weights])\n",
        "        \n",
        "        print(f\"\\nğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´:\")\n",
        "        print(f\"  ğŸ”¹ ì´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
        "        print(f\"  ğŸ”¹ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}\")\n",
        "        print(f\"  ğŸ”¹ ê³ ì • íŒŒë¼ë¯¸í„°: {total_params - trainable_params:,}\")\n",
        "        \n",
        "        # ëª¨ë¸ í¬ê¸° ì¶”ì •\n",
        "        model_size_mb = (total_params * 4) / (1024 * 1024)  # float32 ê¸°ì¤€\n",
        "        print(f\"  ğŸ”¹ ì˜ˆìƒ ëª¨ë¸ í¬ê¸°: {model_size_mb:.2f} MB\")\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.2 ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ê·œí™”\n",
        "\n",
        "ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ”„ ì „ì²˜ë¦¬ ë‹¨ê³„:\n",
        "- **í”½ì…€ ê°’ ì •ê·œí™”**: 0~255 ë²”ìœ„ë¥¼ 0~1 ë²”ìœ„ë¡œ ë³€í™˜ (Ã·255)\n",
        "- **ë°ì´í„° ì…”í”Œë§**: í•™ìŠµ ë°ì´í„°ì˜ ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ê¸°\n",
        "- **ë°ì´í„° ê²€ì¦**: í˜•íƒœì™€ ê°’ ë²”ìœ„ í™•ì¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing(shuffle=True, random_state=42):\n",
        "    \"\"\"\n",
        "    ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ í•™ìŠµ ì¤€ë¹„\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    shuffle : bool\n",
        "        í›ˆë ¨ ë°ì´í„° ì…”í”Œë§ ì—¬ë¶€\n",
        "    random_state : int\n",
        "        ì…”í”Œë§ ì‹œ ì‚¬ìš©í•  ëœë¤ ì‹œë“œ\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "        ì •ê·œí™”ë˜ê³  ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹\n",
        "        ì‹¤íŒ¨ ì‹œ (None, None, None, None) ë°˜í™˜\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # 1. ë°ì´í„° ë¡œë”©\n",
        "    print(\"ğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©\")\n",
        "    X_train, y_train, X_test, y_test = loadData()\n",
        "    \n",
        "    if X_train is None:\n",
        "        print(\"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    print(\"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ\")\n",
        "    \n",
        "    # 2. ë°ì´í„° ê²€ì¦\n",
        "    print(f\"\\nğŸ” 2ë‹¨ê³„: ë°ì´í„° ê²€ì¦\")\n",
        "    print(f\"  ğŸ“Š í›ˆë ¨ ë°ì´í„°: {X_train.shape}, íƒ€ì…: {X_train.dtype}\")\n",
        "    print(f\"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}, íƒ€ì…: {X_test.dtype}\")\n",
        "    print(f\"  ğŸ“Š í”½ì…€ ê°’ ë²”ìœ„: {X_train.min()} ~ {X_train.max()}\")\n",
        "    \n",
        "    # ë°ì´í„° íƒ€ì… í™•ì¸\n",
        "    if X_train.dtype != np.uint8 and (X_train.min() < 0 or X_train.max() > 255):\n",
        "        print(\"âš ï¸  ë¹„ì •ìƒì ì¸ í”½ì…€ ê°’ ë²”ìœ„ ê°ì§€\")\n",
        "    else:\n",
        "        print(\"âœ… í”½ì…€ ê°’ ë²”ìœ„ ì •ìƒ\")\n",
        "    \n",
        "    # 3. ë°ì´í„° ì •ê·œí™”\n",
        "    print(f\"\\nâš¡ 3ë‹¨ê³„: í”½ì…€ ê°’ ì •ê·œí™” (0~255 â†’ 0~1)\")\n",
        "    X_train_scaled = X_train.astype(np.float32) / 255.0\n",
        "    X_test_scaled = X_test.astype(np.float32) / 255.0\n",
        "    \n",
        "    print(f\"  âœ… ì •ê·œí™” ì™„ë£Œ\")\n",
        "    print(f\"  ğŸ“Š í›ˆë ¨ ë°ì´í„° ë²”ìœ„: {X_train_scaled.min():.6f} ~ {X_train_scaled.max():.6f}\")\n",
        "    print(f\"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²”ìœ„: {X_test_scaled.min():.6f} ~ {X_test_scaled.max():.6f}\")\n",
        "    \n",
        "    # 4. ë°ì´í„° ì…”í”Œë§ (í›ˆë ¨ ë°ì´í„°ë§Œ)\n",
        "    if shuffle:\n",
        "        print(f\"\\nğŸ”€ 4ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ì…”í”Œë§\")\n",
        "        np.random.seed(random_state)\n",
        "        shuffle_idx = np.random.permutation(len(X_train_scaled))\n",
        "        X_train_scaled = X_train_scaled[shuffle_idx]\n",
        "        y_train = y_train[shuffle_idx]\n",
        "        print(f\"  âœ… ì…”í”Œë§ ì™„ë£Œ (ì‹œë“œ: {random_state})\")\n",
        "    else:\n",
        "        print(f\"\\nâ­ï¸ 4ë‹¨ê³„: ì…”í”Œë§ ê±´ë„ˆë›°ê¸°\")\n",
        "    \n",
        "    # 5. ë ˆì´ë¸” ê²€ì¦ ë° ë¶„í¬ í™•ì¸\n",
        "    print(f\"\\nğŸ“ˆ 5ë‹¨ê³„: ë ˆì´ë¸” ë¶„ì„\")\n",
        "    \n",
        "    # ë ˆì´ë¸” í˜•íƒœ í™•ì¸\n",
        "    print(f\"  ğŸ“Š í›ˆë ¨ ë ˆì´ë¸”: {y_train.shape}, íƒ€ì…: {y_train.dtype}\")\n",
        "    print(f\"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”: {y_test.shape}, íƒ€ì…: {y_test.dtype}\")\n",
        "    \n",
        "    # ë ˆì´ë¸” ê°’ í™•ì¸\n",
        "    unique_train_labels = np.unique(y_train)\n",
        "    unique_test_labels = np.unique(y_test)\n",
        "    \n",
        "    print(f\"  ğŸ“Š í›ˆë ¨ ì„¸íŠ¸ ë ˆì´ë¸”: {unique_train_labels}\")\n",
        "    print(f\"  ğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë ˆì´ë¸”: {unique_test_labels}\")\n",
        "    \n",
        "    # ë ˆì´ë¸” ë¶„í¬\n",
        "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "    \n",
        "    print(f\"\\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "    train_total = len(y_train)\n",
        "    test_total = len(y_test)\n",
        "    \n",
        "    for label, count in zip(unique_train, counts_train):\n",
        "        class_name = \"Daisy\" if label == 0 else \"Dandelion\"\n",
        "        percentage = (count / train_total) * 100\n",
        "        print(f\"  ğŸ“š í›ˆë ¨ - {class_name}: {count}ê°œ ({percentage:.1f}%)\")\n",
        "    \n",
        "    for label, count in zip(unique_test, counts_test):\n",
        "        class_name = \"Daisy\" if label == 0 else \"Dandelion\"  \n",
        "        percentage = (count / test_total) * 100\n",
        "        print(f\"  ğŸ§ª í…ŒìŠ¤íŠ¸ - {class_name}: {count}ê°œ ({percentage:.1f}%)\")\n",
        "    \n",
        "    # 6. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n",
        "    print(f\"\\nğŸ’¾ 6ë‹¨ê³„: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\")\n",
        "    train_memory = X_train_scaled.nbytes / (1024**2)  # MB\n",
        "    test_memory = X_test_scaled.nbytes / (1024**2)   # MB\n",
        "    total_memory = train_memory + test_memory\n",
        "    \n",
        "    print(f\"  ğŸ“Š í›ˆë ¨ ë°ì´í„°: {train_memory:.2f} MB\")\n",
        "    print(f\"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_memory:.2f} MB\")\n",
        "    print(f\"  ğŸ“Š ì´ ë©”ëª¨ë¦¬: {total_memory:.2f} MB\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return X_train_scaled, y_train, X_test_scaled, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "ini"
        }
      },
      "source": [
        "## ğŸš€ 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
        "\n",
        "### 4.1 í†µí•© í•™ìŠµ íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "ì „ì²´ ë”¥ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤:\n",
        "\n",
        "#### ğŸ“‹ í•™ìŠµ í”„ë¡œì„¸ìŠ¤:\n",
        "1. **ë°ì´í„° ì „ì²˜ë¦¬**: ë¡œë”©, ì •ê·œí™”, ì…”í”Œë§\n",
        "2. **ëª¨ë¸ ìƒì„±**: CNN ì•„í‚¤í…ì²˜ êµ¬ì„± ë° ì»´íŒŒì¼  \n",
        "3. **ëª¨ë¸ í•™ìŠµ**: í›ˆë ¨ ë°ì´í„°ë¡œ ëª¨ë¸ í›ˆë ¨\n",
        "4. **ì„±ëŠ¥ í‰ê°€**: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€\n",
        "5. **ê²°ê³¼ ë¶„ì„**: í•™ìŠµ ê³¼ì • ë° ì„±ëŠ¥ ë¶„ì„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(epochs=10, batch_size=32, validation_split=0.0, save_model=False, model_name=\"flower_classifier\"):\n",
        "    \"\"\"\n",
        "    ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    epochs : int\n",
        "        í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)\n",
        "    batch_size : int\n",
        "        ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)\n",
        "    validation_split : float\n",
        "        í›ˆë ¨ ë°ì´í„°ì—ì„œ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• í•  ë¹„ìœ¨ (0.0~1.0)\n",
        "    save_model : bool\n",
        "        ëª¨ë¸ ì €ì¥ ì—¬ë¶€\n",
        "    model_name : str\n",
        "        ì €ì¥í•  ëª¨ë¸ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (model, history, results)\n",
        "        - model: í›ˆë ¨ëœ ëª¨ë¸\n",
        "        - history: í•™ìŠµ ê¸°ë¡\n",
        "        - results: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸŒ¸ ê½ƒ ë¶„ë¥˜ CNN ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    start_time = pd.Timestamp.now()\n",
        "    \n",
        "    # í•™ìŠµ ì„¤ì • ì¶œë ¥\n",
        "    print(f\"âš™ï¸ í•™ìŠµ ì„¤ì •:\")\n",
        "    print(f\"  ğŸ”¹ ì—í¬í¬ ìˆ˜: {epochs}\")\n",
        "    print(f\"  ğŸ”¹ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "    print(f\"  ğŸ”¹ ê²€ì¦ ë¶„í• : {validation_split}\")\n",
        "    print(f\"  ğŸ”¹ ëª¨ë¸ ì €ì¥: {'ì˜ˆ' if save_model else 'ì•„ë‹ˆìš”'}\")\n",
        "    if save_model:\n",
        "        print(f\"  ğŸ”¹ ëª¨ë¸ëª…: {model_name}\")\n",
        "    print()\n",
        "    \n",
        "    # 1. ë°ì´í„° ì „ì²˜ë¦¬\n",
        "    print(\"ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = preprocessing()\n",
        "    \n",
        "    if X_train is None:\n",
        "        print(\"âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨. ë¨¼ì € initData() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # 2. ëª¨ë¸ ìƒì„±\n",
        "    print(\"\\nğŸ§  2ë‹¨ê³„: ëª¨ë¸ ìƒì„±\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    model = createModel(summary=True)\n",
        "    \n",
        "    # 3. ì½œë°± ì„¤ì •\n",
        "    print(\"\\nâš™ï¸ 3ë‹¨ê³„: ì½œë°± ì„¤ì •\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=0.0001,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    if save_model:\n",
        "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=f\"{model_name}_best.h5\",\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            verbose=1\n",
        "        )\n",
        "        callbacks.append(checkpoint_callback)\n",
        "    \n",
        "    print(f\"  âœ… ì½œë°± ì„¤ì • ì™„ë£Œ: {len(callbacks)}ê°œ\")\n",
        "    for i, callback in enumerate(callbacks, 1):\n",
        "        print(f\"    {i}. {callback.__class__.__name__}\")\n",
        "    \n",
        "    # 4. ëª¨ë¸ í•™ìŠµ\n",
        "    print(f\"\\nğŸ¯ 4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
        "    \n",
        "    # ê²€ì¦ ë°ì´í„° ì„¤ì •\n",
        "    if validation_split > 0:\n",
        "        validation_data = None\n",
        "        print(f\"  ğŸ“Š í›ˆë ¨ ë°ì´í„°ì—ì„œ {validation_split*100:.1f}% ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• \")\n",
        "    else:\n",
        "        validation_data = (X_test, y_test)\n",
        "        validation_split = 0.0\n",
        "        print(f\"  ğŸ“Š ë³„ë„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©\")\n",
        "    \n",
        "    try:\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=validation_data,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        training_time = pd.Timestamp.now() - start_time\n",
        "        print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time.total_seconds():.1f}ì´ˆ)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # 5. ëª¨ë¸ í‰ê°€\n",
        "    print(f\"\\nğŸ“Š 5ë‹¨ê³„: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # í›ˆë ¨ ë°ì´í„° í‰ê°€\n",
        "    print(\"ğŸ” í›ˆë ¨ ë°ì´í„° í‰ê°€...\")\n",
        "    train_results = model.evaluate(X_train, y_train, verbose=0)\n",
        "    train_metrics = dict(zip(model.metrics_names, train_results))\n",
        "    \n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€\n",
        "    print(\"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€...\")\n",
        "    test_results = model.evaluate(X_test, y_test, verbose=0)\n",
        "    test_metrics = dict(zip(model.metrics_names, test_results))\n",
        "    \n",
        "    # ê²°ê³¼ ì •ë¦¬\n",
        "    results = {\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'training_time': training_time.total_seconds(),\n",
        "        'epochs_completed': len(history.history['loss']),\n",
        "        'best_epoch': np.argmax(history.history['val_accuracy']) + 1 if 'val_accuracy' in history.history else len(history.history['loss'])\n",
        "    }\n",
        "    \n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"\\nğŸ“ˆ ìµœì¢… ì„±ëŠ¥ ê²°ê³¼:\")\n",
        "    print(f\"  ğŸ“š í›ˆë ¨ ì„¸íŠ¸:\")\n",
        "    for metric, value in train_metrics.items():\n",
        "        print(f\"    ğŸ”¹ {metric}: {value:.4f}\")\n",
        "    \n",
        "    print(f\"  ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸:\")\n",
        "    for metric, value in test_metrics.items():\n",
        "        print(f\"    ğŸ”¹ {metric}: {value:.4f}\")\n",
        "    \n",
        "    # ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„\n",
        "    accuracy_gap = train_metrics.get('accuracy', 0) - test_metrics.get('accuracy', 0)\n",
        "    print(f\"\\nğŸ¯ ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„:\")\n",
        "    print(f\"  ğŸ”¹ ì •í™•ë„ ì°¨ì´: {accuracy_gap:.4f}\")\n",
        "    \n",
        "    if accuracy_gap > 0.15:\n",
        "        print(f\"  âš ï¸  ì‹¬ê°í•œ ê³¼ì í•© ê°ì§€\")\n",
        "        print(f\"      â†’ ì •ê·œí™” ê°•í™”, ë“œë¡­ì•„ì›ƒ ì¦ê°€, ë°ì´í„° ì¦ê°• ê³ ë ¤\")\n",
        "    elif accuracy_gap > 0.05:\n",
        "        print(f\"  âš ï¸  ê²½ë¯¸í•œ ê³¼ì í•© ê°ì§€\")\n",
        "        print(f\"      â†’ ì¡°ê¸° ì¢…ë£Œ, ì •ê·œí™” ì¶”ê°€ ê³ ë ¤\")\n",
        "    else:\n",
        "        print(f\"  âœ… ì–‘í˜¸í•œ ì¼ë°˜í™” ì„±ëŠ¥\")\n",
        "    \n",
        "    # 6. ëª¨ë¸ ì €ì¥\n",
        "    if save_model:\n",
        "        print(f\"\\nğŸ’¾ 6ë‹¨ê³„: ëª¨ë¸ ì €ì¥\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        try:\n",
        "            final_model_path = f\"{model_name}_final.h5\"\n",
        "            model.save(final_model_path)\n",
        "            print(f\"  âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}\")\n",
        "            \n",
        "            # ëª¨ë¸ ì •ë³´ ì €ì¥\n",
        "            model_info = {\n",
        "                'model_name': model_name,\n",
        "                'training_time': training_time.total_seconds(),\n",
        "                'epochs': epochs,\n",
        "                'batch_size': batch_size,\n",
        "                'final_metrics': test_metrics,\n",
        "                'save_time': pd.Timestamp.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            import json\n",
        "            info_path = f\"{model_name}_info.json\"\n",
        "            with open(info_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(model_info, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"  âœ… ëª¨ë¸ ì •ë³´ ì €ì¥: {info_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
        "    \n",
        "    # ìµœì¢… ìš”ì•½\n",
        "    total_time = pd.Timestamp.now() - start_time\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(f\"ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\")\n",
        "    print(f\"=\"*70)\n",
        "    print(f\"â±ï¸  ì´ ì†Œìš”ì‹œê°„: {total_time.total_seconds():.1f}ì´ˆ\")\n",
        "    print(f\"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_metrics.get('accuracy', 0)*100:.2f}%\")\n",
        "    print(f\"ğŸ“Š í•™ìŠµëœ ì—í¬í¬: {results['epochs_completed']}/{epochs}\")\n",
        "    print(f\"ğŸ† ìµœê³  ì„±ëŠ¥ ì—í¬í¬: {results['best_epoch']}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return model, history, results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“Š 5. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
        "\n",
        "### 5.1 í•™ìŠµ ê³¼ì • ì‹œê°í™” ë° ìƒì„¸ ë¶„ì„\n",
        "\n",
        "í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ë„êµ¬ë“¤ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í•™ìŠµ ê³¡ì„  ì‹œê°í™” í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ê°’ê³¼ ì •í™•ë„ ë³€í™”ë¥¼ ì‹œê°í™”\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    history : tf.keras.callbacks.History\n",
        "        ëª¨ë¸ í•™ìŠµ ê¸°ë¡\n",
        "    save_path : str, optional\n",
        "        ê·¸ë˜í”„ ì €ì¥ ê²½ë¡œ\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        print(\"âŒ í•™ìŠµ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "    \n",
        "    print(\"ğŸ“Š í•™ìŠµ ê³¡ì„  ì‹œê°í™” ì¤‘...\")\n",
        "    \n",
        "    # í•™ìŠµ ê¸°ë¡ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ\n",
        "    metrics = list(history.history.keys())\n",
        "    train_metrics = [m for m in metrics if not m.startswith('val_')]\n",
        "    val_metrics = [m for m in metrics if m.startswith('val_')]\n",
        "    \n",
        "    n_metrics = len(train_metrics)\n",
        "    fig, axes = plt.subplots(1, n_metrics, figsize=(6*n_metrics, 5))\n",
        "    \n",
        "    if n_metrics == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    \n",
        "    for i, metric in enumerate(train_metrics):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # í›ˆë ¨ ë©”íŠ¸ë¦­ í”Œë¡¯\n",
        "        epochs = range(1, len(history.history[metric]) + 1)\n",
        "        ax.plot(epochs, history.history[metric], \n",
        "               color=colors[0], marker='o', markersize=4, \n",
        "               label=f'Training {metric.title()}', linewidth=2)\n",
        "        \n",
        "        # ê²€ì¦ ë©”íŠ¸ë¦­ í”Œë¡¯ (ìˆëŠ” ê²½ìš°)\n",
        "        val_metric = f'val_{metric}'\n",
        "        if val_metric in history.history:\n",
        "            ax.plot(epochs, history.history[val_metric], \n",
        "                   color=colors[1], marker='s', markersize=4,\n",
        "                   label=f'Validation {metric.title()}', linewidth=2)\n",
        "        \n",
        "        # ê·¸ë˜í”„ ì„¤ì •\n",
        "        ax.set_title(f'{metric.title()} over Epochs', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Epoch', fontsize=12)\n",
        "        ax.set_ylabel(metric.title(), fontsize=12)\n",
        "        ax.legend(fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # ìµœì ê°’ í‘œì‹œ\n",
        "        if val_metric in history.history:\n",
        "            if 'loss' in metric:\n",
        "                best_epoch = np.argmin(history.history[val_metric]) + 1\n",
        "                best_value = min(history.history[val_metric])\n",
        "            else:\n",
        "                best_epoch = np.argmax(history.history[val_metric]) + 1\n",
        "                best_value = max(history.history[val_metric])\n",
        "            \n",
        "            ax.axvline(x=best_epoch, color=colors[2], linestyle='--', alpha=0.7)\n",
        "            ax.text(best_epoch, best_value, f'Best: {best_value:.3f}\\\\nEpoch: {best_epoch}',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=colors[2], alpha=0.3),\n",
        "                   fontsize=9, ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"  âœ… í•™ìŠµ ê³¡ì„  ì €ì¥: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # í•™ìŠµ ìš”ì•½ ì¶œë ¥\n",
        "    final_epoch = len(history.history['loss'])\n",
        "    print(f\"\\nğŸ“ˆ í•™ìŠµ ìš”ì•½:\")\n",
        "    print(f\"  ğŸ”¹ ì´ ì—í¬í¬: {final_epoch}\")\n",
        "    \n",
        "    for metric in train_metrics:\n",
        "        final_train = history.history[metric][-1]\n",
        "        val_metric = f'val_{metric}'\n",
        "        \n",
        "        if val_metric in history.history:\n",
        "            final_val = history.history[val_metric][-1]\n",
        "            print(f\"  ğŸ”¹ ìµœì¢… {metric}: í›ˆë ¨ {final_train:.4f}, ê²€ì¦ {final_val:.4f}\")\n",
        "            \n",
        "            if 'loss' in metric:\n",
        "                best_val = min(history.history[val_metric])\n",
        "                best_epoch = np.argmin(history.history[val_metric]) + 1\n",
        "            else:\n",
        "                best_val = max(history.history[val_metric])\n",
        "                best_epoch = np.argmax(history.history[val_metric]) + 1\n",
        "            \n",
        "            print(f\"  ğŸ”¹ ìµœê³  ê²€ì¦ {metric}: {best_val:.4f} (ì—í¬í¬ {best_epoch})\")\n",
        "        else:\n",
        "            print(f\"  ğŸ”¹ ìµœì¢… {metric}: {final_train:.4f}\")\n",
        "\n",
        "print(\"âœ… í•™ìŠµ ê³¡ì„  ì‹œê°í™” í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 5.2 ì¶”ê°€ ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬\n",
        "\n",
        "ëª¨ë¸ ì„±ëŠ¥ì„ ë” ìì„¸íˆ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ì¶”ê°€ ë„êµ¬ë“¤ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ê³ ê¸‰ ë¶„ì„ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
            "ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:\n",
            "   evaluate_model_detailed(model, X_test_scaled, y_test)\n",
            "   show_sample_predictions(model, X_test_scaled, y_test, num_samples=12)\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model_detailed(model, X_test, y_test, save_path=None):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ì˜ ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€ ë° í˜¼ë™í–‰ë ¬ ì‹œê°í™”\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tf.keras.Model\n",
        "        í‰ê°€í•  ëª¨ë¸\n",
        "    X_test : numpy.ndarray\n",
        "        í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë°ì´í„°\n",
        "    y_test : numpy.ndarray\n",
        "        í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”\n",
        "    save_path : str, optional\n",
        "        í˜¼ë™í–‰ë ¬ ì €ì¥ ê²½ë¡œ\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        print(\"âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return None, None\n",
        "    \n",
        "    print(\"ğŸ” ìƒì„¸ ëª¨ë¸ í‰ê°€ ì§„í–‰ ì¤‘...\")\n",
        "    \n",
        "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    y_pred_prob = model.predict(X_test, verbose=0)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # í˜¼ë™í–‰ë ¬\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
        "    class_names = ['Daisy', 'Dandelion']\n",
        "    target_names = [f'{name} ({i})' for i, name in enumerate(class_names)]\n",
        "    report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
        "    \n",
        "    print(\"ğŸ“Š ìƒì„¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_key = f'{class_name} ({i})'\n",
        "        metrics = report[class_key]\n",
        "        print(f\"ğŸŒ¸ {class_name}:\")\n",
        "        print(f\"   ì •ë°€ë„(Precision): {metrics['precision']:.4f}\")\n",
        "        print(f\"   ì¬í˜„ìœ¨(Recall):     {metrics['recall']:.4f}\")\n",
        "        print(f\"   F1-ì ìˆ˜:           {metrics['f1-score']:.4f}\")\n",
        "        print(f\"   ì§€ì› ìƒ˜í”Œ:         {int(metrics['support'])}\")\n",
        "        print()\n",
        "    \n",
        "    # ì „ì²´ ì„±ëŠ¥\n",
        "    print(f\"ğŸ“ˆ ì „ì²´ ì„±ëŠ¥:\")\n",
        "    print(f\"   ì •í™•ë„:            {report['accuracy']:.4f}\")\n",
        "    print(f\"   ë§¤í¬ë¡œ í‰ê·  F1:    {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"   ê°€ì¤‘ í‰ê·  F1:      {report['weighted avg']['f1-score']:.4f}\")\n",
        "    \n",
        "    # í˜¼ë™í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    \n",
        "    # ì •ê·œí™”ëœ í˜¼ë™í–‰ë ¬ ê³„ì‚°\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    # íˆíŠ¸ë§µ ìƒì„±\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                square=True, linewidths=0.5)\n",
        "    \n",
        "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    \n",
        "    # ì •í™•ë„ í‘œì‹œ\n",
        "    total_samples = cm.sum()\n",
        "    correct_predictions = np.diag(cm).sum()\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    \n",
        "    plt.figtext(0.02, 0.02, f'Total Accuracy: {accuracy:.4f} ({correct_predictions}/{total_samples})',\n",
        "                fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.5))\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"  âœ… í˜¼ë™í–‰ë ¬ ì €ì¥: {save_path}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return y_pred, y_pred_prob\n",
        "\n",
        "def show_sample_predictions(model, X_test, y_test, num_samples=8, save_path=None):\n",
        "    \"\"\"\n",
        "    ëœë¤ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°í™”\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tf.keras.Model\n",
        "        ì˜ˆì¸¡ì— ì‚¬ìš©í•  ëª¨ë¸\n",
        "    X_test : numpy.ndarray\n",
        "        í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë°ì´í„°\n",
        "    y_test : numpy.ndarray\n",
        "        í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”\n",
        "    num_samples : int\n",
        "        í‘œì‹œí•  ìƒ˜í”Œ ìˆ˜\n",
        "    save_path : str, optional\n",
        "        ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        print(\"âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"ğŸ–¼ï¸ {num_samples}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡ ì‹œê°í™” ì¤‘...\")\n",
        "    \n",
        "    # ëœë¤ ìƒ˜í”Œ ì„ íƒ\n",
        "    np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´\n",
        "    indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
        "    \n",
        "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    predictions = model.predict(X_test[indices], verbose=0)\n",
        "    \n",
        "    # ì‹œê°í™” ì„¤ì •\n",
        "    rows = 2\n",
        "    cols = num_samples // rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    class_names = ['Daisy', 'Dandelion']\n",
        "    correct_count = 0\n",
        "    \n",
        "    for i, idx in enumerate(indices):\n",
        "        # ì´ë¯¸ì§€ í‘œì‹œ\n",
        "        axes[i].imshow(X_test[idx])\n",
        "        axes[i].axis('off')\n",
        "        \n",
        "        # ì˜ˆì¸¡ ê²°ê³¼\n",
        "        pred_prob = predictions[i][0]\n",
        "        pred_class = int(pred_prob > 0.5)\n",
        "        actual_class = int(y_test[idx])\n",
        "        \n",
        "        # ì •í™•ë„ ì¹´ìš´íŠ¸\n",
        "        if pred_class == actual_class:\n",
        "            correct_count += 1\n",
        "        \n",
        "        # ì‹ ë¢°ë„ ìƒ‰ìƒ ì„¤ì •\n",
        "        confidence = max(pred_prob, 1 - pred_prob)\n",
        "        if pred_class == actual_class:\n",
        "            if confidence > 0.8:\n",
        "                color = 'darkgreen'\n",
        "            else:\n",
        "                color = 'green'\n",
        "        else:\n",
        "            if confidence > 0.8:\n",
        "                color = 'darkred'\n",
        "            else:\n",
        "                color = 'red'\n",
        "        \n",
        "        # ì œëª© ì„¤ì •\n",
        "        title = f'ì˜ˆì¸¡: {class_names[pred_class]} ({pred_prob:.3f})\\\\nì‹¤ì œ: {class_names[actual_class]}'\n",
        "        axes[i].set_title(title, color=color, fontsize=11, fontweight='bold')\n",
        "        \n",
        "        # í…Œë‘ë¦¬ ìƒ‰ìƒ ì„¤ì •\n",
        "        for spine in axes[i].spines.values():\n",
        "            spine.set_color(color)\n",
        "            spine.set_linewidth(2)\n",
        "    \n",
        "    # ì „ì²´ ì œëª©\n",
        "    sample_accuracy = correct_count / num_samples\n",
        "    fig.suptitle(f'ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ì •í™•ë„: {sample_accuracy:.2%})', \n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"  âœ… ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"ğŸ“Š ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼:\")\n",
        "    print(f\"  ğŸ”¹ ì •í™• ì˜ˆì¸¡: {correct_count}/{num_samples} ({sample_accuracy:.2%})\")\n",
        "    print(f\"  ğŸ”¹ ì˜¤ë¥˜ ì˜ˆì¸¡: {num_samples - correct_count}/{num_samples}\")\n",
        "\n",
        "print(\"âœ… ê³ ê¸‰ ë¶„ì„ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:\")\n",
        "print(\"   evaluate_model_detailed(model, X_test_scaled, y_test)\")\n",
        "print(\"   show_sample_predictions(model, X_test_scaled, y_test, num_samples=12)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸš€ 6. ì‹¤í–‰ ê°€ì´ë“œ\n",
        "\n",
        "### 6.1 ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ë²•\n",
        "\n",
        "ì´ ì„¹ì…˜ì—ì„œëŠ” í”„ë¡œì íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
        "\n",
        "#### ğŸ“‹ ì‹¤í–‰ ìˆœì„œ:\n",
        "\n",
        "1. **ì²« ì‹¤í–‰ ì‹œ (ë°ì´í„° ìƒì„±)**\n",
        "2. **ëª¨ë¸ í•™ìŠµ ë° í‰ê°€**  \n",
        "3. **ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”**\n",
        "\n",
        "### 6.2 ì‹¤ì œ ì‹¤í–‰ ì½”ë“œ\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë“¤ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸŒ¸ ê½ƒ ë¶„ë¥˜ CNN í”„ë¡œì íŠ¸ ì‹¤í–‰ ê°€ì´ë“œ\n",
            "================================================================================\n",
            "\n",
            "ğŸ“‹ ì‹¤í–‰ ë‹¨ê³„:\n",
            "  1ï¸âƒ£ ì²« ì‹¤í–‰ ì‹œ: initData() - ì´ë¯¸ì§€ë¥¼ .npz íŒŒì¼ë¡œ ë³€í™˜ (ì‹œê°„ ì†Œìš”)\n",
            "  2ï¸âƒ£ ëª¨ë¸ í•™ìŠµ: main() - CNN ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€\n",
            "  3ï¸âƒ£ ê²°ê³¼ ë¶„ì„: ì‹œê°í™” í•¨ìˆ˜ë“¤ë¡œ ìƒì„¸ ë¶„ì„\n",
            "\n",
            "ğŸ’¡ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!\n",
            "================================================================================\n",
            "1ï¸âƒ£ ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘...\n",
            "============================================================\n",
            "ğŸš€ ë°ì´í„° ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "============================================================\n",
            "\n",
            "ğŸŒ¸ DAISY ì²˜ë¦¬ ì¤‘... (ë ˆì´ë¸”: 0)\n",
            "----------------------------------------\n",
            "ğŸ“‚ í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬\n",
            "ğŸ”„ ì²˜ë¦¬ ì¤‘: train/daisy\n",
            "ğŸ“‚ ê²½ë¡œ: ../../data/flowers\\train\\daisy\n",
            "ğŸ“Š ì´ 529ê°œ ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 100/529 (18.9%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 200/529 (37.8%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 300/529 (56.7%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 400/529 (75.6%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 500/529 (94.5%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 529/529 (100.0%)\n",
            "âœ… ì €ì¥ ì™„ë£Œ: ../../data/npz/imagedata0_train.npz\n",
            "ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: 529ê°œ ì´ë¯¸ì§€ ì„±ê³µ, 0ê°œ ì˜¤ë¥˜\n",
            "\n",
            "ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬\n",
            "ğŸ”„ ì²˜ë¦¬ ì¤‘: test/daisy\n",
            "ğŸ“‚ ê²½ë¡œ: ../../data/flowers\\test\\daisy\n",
            "ğŸ“Š ì´ 77ê°œ ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 77/77 (100.0%)\n",
            "âœ… ì €ì¥ ì™„ë£Œ: ../../data/npz/imagedata0_test.npz\n",
            "ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: 77ê°œ ì´ë¯¸ì§€ ì„±ê³µ, 0ê°œ ì˜¤ë¥˜\n",
            "âœ… daisy ì™„ë£Œ: Train 529ê°œ, Test 77ê°œ, ì´ 606ê°œ\n",
            "\n",
            "ğŸŒ¸ DANDELION ì²˜ë¦¬ ì¤‘... (ë ˆì´ë¸”: 1)\n",
            "----------------------------------------\n",
            "ğŸ“‚ í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬\n",
            "ğŸ”„ ì²˜ë¦¬ ì¤‘: train/dandelion\n",
            "ğŸ“‚ ê²½ë¡œ: ../../data/flowers\\train\\dandelion\n",
            "ğŸ“Š ì´ 746ê°œ ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 100/746 (13.4%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 200/746 (26.8%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 300/746 (40.2%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 400/746 (53.6%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 500/746 (67.0%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 600/746 (80.4%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 700/746 (93.8%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 746/746 (100.0%)\n",
            "âœ… ì €ì¥ ì™„ë£Œ: ../../data/npz/imagedata1_train.npz\n",
            "ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: 746ê°œ ì´ë¯¸ì§€ ì„±ê³µ, 0ê°œ ì˜¤ë¥˜\n",
            "\n",
            "ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬\n",
            "ğŸ”„ ì²˜ë¦¬ ì¤‘: test/dandelion\n",
            "ğŸ“‚ ê²½ë¡œ: ../../data/flowers\\test\\dandelion\n",
            "ğŸ“Š ì´ 105ê°œ ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 100/105 (95.2%)\n",
            "ğŸ“ˆ ì§„í–‰ë¥ : 105/105 (100.0%)\n",
            "âœ… ì €ì¥ ì™„ë£Œ: ../../data/npz/imagedata1_test.npz\n",
            "ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: 105ê°œ ì´ë¯¸ì§€ ì„±ê³µ, 0ê°œ ì˜¤ë¥˜\n",
            "âœ… dandelion ì™„ë£Œ: Train 746ê°œ, Test 105ê°œ, ì´ 851ê°œ\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:\n",
            "   flower  label  train_count  test_count  total_count\n",
            "    daisy      0          529          77          606\n",
            "dandelion      1          746         105          851\n",
            "\n",
            "â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: 7.7ì´ˆ\n",
            "ğŸ“ˆ ì´ ì²˜ë¦¬ ì´ë¯¸ì§€: 1457ê°œ\n",
            "âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: 190.2ê°œ/ì´ˆ\n",
            "\n",
            "ğŸ“„ ìƒì„±ëœ .npz íŒŒì¼ë“¤:\n",
            "\n",
            "ğŸ’¾ ì´ íŒŒì¼ í¬ê¸°: 0.00 MB\n",
            "============================================================\n",
            "âœ… ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ!\n",
            "\n",
            "2ï¸âƒ£ ë¹ ë¥¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸...\n",
            "======================================================================\n",
            "ğŸŒ¸ ê½ƒ ë¶„ë¥˜ CNN ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n",
            "======================================================================\n",
            "âš™ï¸ í•™ìŠµ ì„¤ì •:\n",
            "  ğŸ”¹ ì—í¬í¬ ìˆ˜: 3\n",
            "  ğŸ”¹ ë°°ì¹˜ í¬ê¸°: 16\n",
            "  ğŸ”¹ ê²€ì¦ ë¶„í• : 0.0\n",
            "  ğŸ”¹ ëª¨ë¸ ì €ì¥: ì•„ë‹ˆìš”\n",
            "\n",
            "ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "ğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©\n",
            "==================================================\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "âŒ ëˆ„ë½ëœ ë°ì´í„° íŒŒì¼:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "ğŸ’¡ í•´ê²° ë°©ë²•: initData() í•¨ìˆ˜ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
            "âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨\n",
            "âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨. ë¨¼ì € initData() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n",
            "\n",
            "3ï¸âƒ£ ì •ì‹ ëª¨ë¸ í•™ìŠµ...\n",
            "======================================================================\n",
            "ğŸŒ¸ ê½ƒ ë¶„ë¥˜ CNN ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n",
            "======================================================================\n",
            "âš™ï¸ í•™ìŠµ ì„¤ì •:\n",
            "  ğŸ”¹ ì—í¬í¬ ìˆ˜: 10\n",
            "  ğŸ”¹ ë°°ì¹˜ í¬ê¸°: 32\n",
            "  ğŸ”¹ ê²€ì¦ ë¶„í• : 0.0\n",
            "  ğŸ”¹ ëª¨ë¸ ì €ì¥: ì˜ˆ\n",
            "  ğŸ”¹ ëª¨ë¸ëª…: flower_cnn_v1\n",
            "\n",
            "ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "ğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©\n",
            "==================================================\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "âŒ ëˆ„ë½ëœ ë°ì´í„° íŒŒì¼:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "ğŸ’¡ í•´ê²° ë°©ë²•: initData() í•¨ìˆ˜ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
            "âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨\n",
            "âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨. ë¨¼ì € initData() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n",
            "\n",
            "4ï¸âƒ£ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”...\n",
            "==================================================\n",
            "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "ğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©\n",
            "==================================================\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "âŒ ëˆ„ë½ëœ ë°ì´í„° íŒŒì¼:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "ğŸ’¡ í•´ê²° ë°©ë²•: initData() í•¨ìˆ˜ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
            "âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨\n",
            "âŒ ë°ì´í„° ë˜ëŠ” ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
            "\n",
            "5ï¸âƒ£ ê°œë³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸...\n",
            "==================================================\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "âŒ ëˆ„ë½ëœ ë°ì´í„° íŒŒì¼:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "ğŸ’¡ í•´ê²° ë°©ë²•: initData() í•¨ìˆ˜ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
            "==================================================\n",
            "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "ğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©\n",
            "==================================================\n",
            "ğŸ“‚ ë°ì´í„° ë¡œë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
            "==================================================\n",
            "âŒ ëˆ„ë½ëœ ë°ì´í„° íŒŒì¼:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "ğŸ’¡ í•´ê²° ë°©ë²•: initData() í•¨ìˆ˜ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
            "âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨\n",
            "==================================================\n",
            "ğŸ—ï¸ CNN ëª¨ë¸ ìƒì„± ì‹œì‘\n",
            "==================================================\n",
            "âœ… ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼ ì™„ë£Œ!\n",
            "\n",
            "ğŸ“‹ ëª¨ë¸ êµ¬ì¡° ìš”ì•½:\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FlowerClassifier\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"FlowerClassifier\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ maxpool2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ maxpool2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,638,528</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚         \u001b[38;5;34m1,792\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ maxpool2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚        \u001b[38;5;34m18,464\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ maxpool2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚     \u001b[38;5;34m1,638,528\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m4,128\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,662,945</span> (6.34 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,662,945\u001b[0m (6.34 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,662,945</span> (6.34 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,662,945\u001b[0m (6.34 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´:\n",
            "  ğŸ”¹ ì´ íŒŒë¼ë¯¸í„°: 1,662,945\n",
            "  ğŸ”¹ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: 1,662,945\n",
            "  ğŸ”¹ ê³ ì • íŒŒë¼ë¯¸í„°: 0\n",
            "  ğŸ”¹ ì˜ˆìƒ ëª¨ë¸ í¬ê¸°: 6.34 MB\n",
            "==================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ ì‹¤í–‰ ì™„ë£Œ í›„ ë‹¤ìŒê³¼ ê°™ì€ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:\n",
            "  ğŸ“„ imagedata*.npz - ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ë°ì´í„°\n",
            "  ğŸ¤– *_final.h5 - í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼\n",
            "  ğŸ“‹ *_info.json - ëª¨ë¸ ì •ë³´\n",
            "  ğŸ“Š *.png - ë¶„ì„ ê²°ê³¼ ì´ë¯¸ì§€\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸš€ ì‹¤í–‰ ê°€ì´ë“œ - ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•˜ì„¸ìš”\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸŒ¸ ê½ƒ ë¶„ë¥˜ CNN í”„ë¡œì íŠ¸ ì‹¤í–‰ ê°€ì´ë“œ\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"ğŸ“‹ ì‹¤í–‰ ë‹¨ê³„:\")\n",
        "print(\"  1ï¸âƒ£ ì²« ì‹¤í–‰ ì‹œ: initData() - ì´ë¯¸ì§€ë¥¼ .npz íŒŒì¼ë¡œ ë³€í™˜ (ì‹œê°„ ì†Œìš”)\")\n",
        "print(\"  2ï¸âƒ£ ëª¨ë¸ í•™ìŠµ: main() - CNN ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€\")  \n",
        "print(\"  3ï¸âƒ£ ê²°ê³¼ ë¶„ì„: ì‹œê°í™” í•¨ìˆ˜ë“¤ë¡œ ìƒì„¸ ë¶„ì„\")\n",
        "print()\n",
        "print(\"ğŸ’¡ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1ë‹¨ê³„: ë°ì´í„° ì´ˆê¸°í™” (ì²« ì‹¤í–‰ ì‹œì—ë§Œ, ì•½ 5-10ë¶„ ì†Œìš”)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"1ï¸âƒ£ ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘...\")\n",
        "success = initData()\n",
        "if not success:\n",
        "    print(\"âŒ ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨\")\n",
        "else:\n",
        "    print(\"âœ… ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "\n",
        "# -----------------------------------------------------------------------------  \n",
        "# 2ë‹¨ê³„: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (3 ì—í¬í¬, ì•½ 1-2ë¶„)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n2ï¸âƒ£ ë¹ ë¥¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸...\")\n",
        "model, history, results = main(\n",
        "    epochs=3, \n",
        "    batch_size=16,\n",
        "    save_model=False\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3ë‹¨ê³„: ì •ì‹ í•™ìŠµ (10 ì—í¬í¬, ì•½ 5-10ë¶„)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n3ï¸âƒ£ ì •ì‹ ëª¨ë¸ í•™ìŠµ...\")\n",
        "model, history, results = main(\n",
        "    epochs=10,\n",
        "    batch_size=32, \n",
        "    save_model=True,\n",
        "    model_name=\"flower_cnn_v1\"\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n4ï¸âƒ£ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”...\")\n",
        "\n",
        "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ\n",
        "X_train, y_train, X_test, y_test = preprocessing()\n",
        "\n",
        "if X_test is not None and model is not None:\n",
        "    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
        "    plot_training_history(history, save_path=\"training_history.png\")\n",
        "    \n",
        "    # ìƒì„¸ ì„±ëŠ¥ í‰ê°€\n",
        "    y_pred, y_pred_prob = evaluate_model_detailed(\n",
        "        model, X_test, y_test, \n",
        "        save_path=\"confusion_matrix.png\"\n",
        "    )\n",
        "    \n",
        "    # ìƒ˜í”Œ ì˜ˆì¸¡ ì‹œê°í™”\n",
        "    show_sample_predictions(\n",
        "        model, X_test, y_test, \n",
        "        num_samples=12,\n",
        "        save_path=\"sample_predictions.png\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\nğŸ‰ ëª¨ë“  ë¶„ì„ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„° ë˜ëŠ” ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5ë‹¨ê³„: ê°œë³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n5ï¸âƒ£ ê°œë³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸...\")\n",
        "\n",
        "# ë°ì´í„°ë§Œ ë¡œë“œí•˜ì—¬ í™•ì¸\n",
        "X_train, y_train, X_test, y_test = loadData()\n",
        "\n",
        "# ì „ì²˜ë¦¬ë§Œ í…ŒìŠ¤íŠ¸\n",
        "X_train_scaled, y_train, X_test_scaled, y_test = preprocessing()\n",
        "\n",
        "# ëª¨ë¸ë§Œ ìƒì„±í•˜ì—¬ êµ¬ì¡° í™•ì¸\n",
        "test_model = createModel(summary=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ¯ ì‹¤í–‰ ì™„ë£Œ í›„ ë‹¤ìŒê³¼ ê°™ì€ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:\")\n",
        "print(\"  ğŸ“„ imagedata*.npz - ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ë°ì´í„°\")\n",
        "print(\"  ğŸ¤– *_final.h5 - í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼\")\n",
        "print(\"  ğŸ“‹ *_info.json - ëª¨ë¸ ì •ë³´\")\n",
        "print(\"  ğŸ“Š *.png - ë¶„ì„ ê²°ê³¼ ì´ë¯¸ì§€\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“‹ 7. í”„ë¡œì íŠ¸ ì •ë¦¬ ë° í™•ì¥ ë°©ì•ˆ\n",
        "\n",
        "### 7.1 í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥\n",
        "\n",
        "#### âœ… ì™„ì„±ëœ ê¸°ëŠ¥ë“¤:\n",
        "- **ë°ì´í„° íŒŒì´í”„ë¼ì¸**: ì´ë¯¸ì§€ ì „ì²˜ë¦¬, ì •ê·œí™”, ë°°ì¹˜ ì²˜ë¦¬\n",
        "- **CNN ëª¨ë¸**: Conv2D, MaxPooling, Dropoutì´ í¬í•¨ëœ ìµœì í™”ëœ êµ¬ì¡°\n",
        "- **í•™ìŠµ ì‹œìŠ¤í…œ**: ì¡°ê¸° ì¢…ë£Œ, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ë“± ê³ ê¸‰ ê¸°ëŠ¥\n",
        "- **í‰ê°€ ë„êµ¬**: í˜¼ë™í–‰ë ¬, ë¶„ë¥˜ ë¦¬í¬íŠ¸, ì‹œê°í™”\n",
        "- **ëª¨ë¸ ê´€ë¦¬**: ìë™ ì €ì¥, ë©”íƒ€ë°ì´í„° ê¸°ë¡\n",
        "\n",
        "#### ğŸ”§ ê¸°ìˆ ì  íŠ¹ì§•:\n",
        "- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: NPZ ì••ì¶• í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥\n",
        "- **í™•ì¥ì„±**: ëª¨ë“ˆí™”ëœ í•¨ìˆ˜ êµ¬ì¡°ë¡œ ì‰¬ìš´ ìˆ˜ì • ê°€ëŠ¥\n",
        "- **ì¬í˜„ì„±**: ëœë¤ ì‹œë“œ ê³ ì •ìœ¼ë¡œ ê²°ê³¼ ì¬í˜„ ë³´ì¥\n",
        "- **ì‚¬ìš©ì ì¹œí™”ì„±**: ìƒì„¸í•œ ì§„í–‰ë¥  í‘œì‹œ ë° ì˜¤ë¥˜ ì²˜ë¦¬\n",
        "\n",
        "### 7.2 ê°œì„  ë° í™•ì¥ ê°€ëŠ¥ ì‚¬í•­\n",
        "\n",
        "#### ğŸš€ ì„±ëŠ¥ í–¥ìƒ ë°©ì•ˆ:\n",
        "\n",
        "1. **ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°œì„ **:\n",
        "   ```python\n",
        "   # ë°°ì¹˜ ì •ê·œí™” ì¶”ê°€\n",
        "   layers.BatchNormalization()\n",
        "   \n",
        "   # ResNet ìŠ¤íƒ€ì¼ ì”ì°¨ ì—°ê²°\n",
        "   # VGG, ResNet ë“± ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í™œìš©\n",
        "   ```\n",
        "\n",
        "2. **ë°ì´í„° ì¦ê°•**:\n",
        "   ```python\n",
        "   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "   \n",
        "   datagen = ImageDataGenerator(\n",
        "       rotation_range=30,\n",
        "       width_shift_range=0.3,\n",
        "       height_shift_range=0.3,\n",
        "       horizontal_flip=True,\n",
        "       zoom_range=0.3,\n",
        "       brightness_range=[0.8, 1.2],\n",
        "       fill_mode='nearest'\n",
        "   )\n",
        "   ```\n",
        "\n",
        "3. **ê³ ê¸‰ ìµœì í™” ê¸°ë²•**:\n",
        "   - **Learning Rate Scheduling**: ì½”ì‚¬ì¸ ì–´ë‹ë§, ì›œì—…\n",
        "   - **ì •ê·œí™” ê¸°ë²•**: L1/L2 ì •ê·œí™”, ìŠ¤í™íŠ¸ëŸ´ ì •ê·œí™”\n",
        "   - **ì•™ìƒë¸” ê¸°ë²•**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ê²°ê³¼ ì¡°í•©\n",
        "\n",
        "#### ğŸ“Š í™•ì¥ ê°€ëŠ¥í•œ ê¸°ëŠ¥ë“¤:\n",
        "\n",
        "1. **ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜**: ë” ë§ì€ ê½ƒ ì¢…ë¥˜ ì¶”ê°€\n",
        "2. **ì‹¤ì‹œê°„ ë¶„ë¥˜**: ì›¹ìº ì„ í†µí•œ ì‹¤ì‹œê°„ ì˜ˆì¸¡\n",
        "3. **ëª¨ë°”ì¼ ë°°í¬**: TensorFlow Liteë¡œ ëª¨ë°”ì¼ ìµœì í™”\n",
        "4. **ì›¹ ì„œë¹„ìŠ¤**: Flask/FastAPIë¥¼ í†µí•œ API ì„œë²„ êµ¬ì¶•\n",
        "\n",
        "### 7.3 ì‹¤ì œ ìš´ì˜ ê³ ë ¤ì‚¬í•­\n",
        "\n",
        "#### ğŸ’¡ ë°°í¬ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n",
        "- [ ] ë‹¤ì–‘í•œ í•´ìƒë„ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸\n",
        "- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\n",
        "- [ ] ëª¨ë¸ ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬\n",
        "- [ ] ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (íë¦¿í•œ ì´ë¯¸ì§€, ë‹¤ë¥¸ ê°ì²´ ë“±)\n",
        "- [ ] A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì„±ëŠ¥ ê²€ì¦\n",
        "\n",
        "#### ğŸ”’ ë³´ì•ˆ ë° í’ˆì§ˆ ê´€ë¦¬:\n",
        "- **ëª¨ë¸ ë²„ì „ ê´€ë¦¬**: MLflow, DVC ë“± í™œìš©\n",
        "- **ë°ì´í„° ê²€ì¦**: ì…ë ¥ ë°ì´í„° í˜•ì‹ ë° í¬ê¸° ê²€ì¦\n",
        "- **ëª¨ë‹ˆí„°ë§**: ì˜ˆì¸¡ ì„±ëŠ¥ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§\n",
        "- **ë¡¤ë°± ì‹œìŠ¤í…œ**: ë¬¸ì œ ë°œìƒ ì‹œ ì´ì „ ë²„ì „ìœ¼ë¡œ ë³µêµ¬\n",
        "\n",
        "ì´ í”„ë¡œì íŠ¸ëŠ” ê¸°ë³¸ì ì¸ ì´ë¯¸ì§€ ë¶„ë¥˜ë¶€í„° ì‹¤ì œ ìš´ì˜ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œê¹Œì§€ì˜ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤. ğŸŒ¸âœ¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸ‰ ê½ƒ ë¶„ë¥˜ CNN í”„ë¡œì íŠ¸ ì™„ë£Œ!\n",
            "================================================================================\n",
            "\n",
            "ğŸ“‹ í”„ë¡œì íŠ¸ ìš”ì•½:\n",
            "  â€¢ ëª©í‘œ: Daisy vs Dandelion ì´ì§„ ë¶„ë¥˜\n",
            "  â€¢ ê¸°ìˆ : CNN, TensorFlow/Keras\n",
            "  â€¢ íŠ¹ì§•: ì™„ì „ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸\n",
            "\n",
            "ğŸ› ï¸ êµ¬í˜„ëœ ì£¼ìš” ê¸°ëŠ¥:\n",
            "  âœ“ ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ê·œí™”\n",
            "  âœ“ ìµœì í™”ëœ CNN ëª¨ë¸ ì•„í‚¤í…ì²˜\n",
            "  âœ“ ê³ ê¸‰ í•™ìŠµ ê¸°ë²• (ì¡°ê¸° ì¢…ë£Œ, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§)\n",
            "  âœ“ ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”\n",
            "  âœ“ ëª¨ë¸ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬\n",
            "\n",
            "ğŸš€ ë‹¤ìŒ í•™ìŠµ ë°©í–¥:\n",
            "  â€¢ ì „ì´ í•™ìŠµ (Transfer Learning)\n",
            "  â€¢ ê³ ê¸‰ ë°ì´í„° ì¦ê°• ê¸°ë²•\n",
            "  â€¢ ëª¨ë¸ ì•™ìƒë¸” ë° ìµœì í™”\n",
            "  â€¢ ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•\n",
            "  â€¢ ì›¹/ëª¨ë°”ì¼ ì•± ë°°í¬\n",
            "\n",
            "ğŸ“š ê´€ë ¨ í•™ìŠµ ìë£Œ:\n",
            "  â€¢ TensorFlow ê³µì‹ ë¬¸ì„œ: https://tensorflow.org\n",
            "  â€¢ Keras ê°€ì´ë“œ: https://keras.io/guides/\n",
            "  â€¢ ì»´í“¨í„° ë¹„ì „ íŠœí† ë¦¬ì–¼: https://tensorflow.org/tutorials/images\n",
            "\n",
            "================================================================================\n",
            "ê°ì‚¬í•©ë‹ˆë‹¤! ì¦ê±°ìš´ ë”¥ëŸ¬ë‹ ì—¬ì •ì´ ë˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤! ğŸŒ¸âœ¨\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# í”„ë¡œì íŠ¸ ì™„ë£Œ - ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ‰ ê½ƒ ë¶„ë¥˜ CNN í”„ë¡œì íŠ¸ ì™„ë£Œ!\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"ğŸ“‹ í”„ë¡œì íŠ¸ ìš”ì•½:\")\n",
        "print(\"  â€¢ ëª©í‘œ: Daisy vs Dandelion ì´ì§„ ë¶„ë¥˜\")\n",
        "print(\"  â€¢ ê¸°ìˆ : CNN, TensorFlow/Keras\") \n",
        "print(\"  â€¢ íŠ¹ì§•: ì™„ì „ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸\")\n",
        "print()\n",
        "print(\"ğŸ› ï¸ êµ¬í˜„ëœ ì£¼ìš” ê¸°ëŠ¥:\")\n",
        "print(\"  âœ“ ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ê·œí™”\")\n",
        "print(\"  âœ“ ìµœì í™”ëœ CNN ëª¨ë¸ ì•„í‚¤í…ì²˜\")\n",
        "print(\"  âœ“ ê³ ê¸‰ í•™ìŠµ ê¸°ë²• (ì¡°ê¸° ì¢…ë£Œ, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§)\")\n",
        "print(\"  âœ“ ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”\")\n",
        "print(\"  âœ“ ëª¨ë¸ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬\")\n",
        "print()\n",
        "print(\"ğŸš€ ë‹¤ìŒ í•™ìŠµ ë°©í–¥:\")\n",
        "print(\"  â€¢ ì „ì´ í•™ìŠµ (Transfer Learning)\")\n",
        "print(\"  â€¢ ê³ ê¸‰ ë°ì´í„° ì¦ê°• ê¸°ë²•\")\n",
        "print(\"  â€¢ ëª¨ë¸ ì•™ìƒë¸” ë° ìµœì í™”\")\n",
        "print(\"  â€¢ ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•\")\n",
        "print(\"  â€¢ ì›¹/ëª¨ë°”ì¼ ì•± ë°°í¬\")\n",
        "print()\n",
        "print(\"ğŸ“š ê´€ë ¨ í•™ìŠµ ìë£Œ:\")\n",
        "print(\"  â€¢ TensorFlow ê³µì‹ ë¬¸ì„œ: https://tensorflow.org\")\n",
        "print(\"  â€¢ Keras ê°€ì´ë“œ: https://keras.io/guides/\")\n",
        "print(\"  â€¢ ì»´í“¨í„° ë¹„ì „ íŠœí† ë¦¬ì–¼: https://tensorflow.org/tutorials/images\")\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"ê°ì‚¬í•©ë‹ˆë‹¤! ì¦ê±°ìš´ ë”¥ëŸ¬ë‹ ì—¬ì •ì´ ë˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤! ğŸŒ¸âœ¨\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
