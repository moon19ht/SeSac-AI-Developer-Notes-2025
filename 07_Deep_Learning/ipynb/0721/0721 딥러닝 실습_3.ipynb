{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🌸 꽃 분류 CNN 모델: Daisy vs Dandelion\n",
        "\n",
        "## 📋 프로젝트 개요\n",
        "\n",
        "### 🎯 목표\n",
        "- **주목적**: 딥러닝을 활용한 꽃 이미지 이진 분류 시스템 구현\n",
        "- **분류 대상**: Daisy(데이지) vs Dandelion(민들레)\n",
        "- **기술 스택**: TensorFlow/Keras를 활용한 CNN(Convolutional Neural Network)\n",
        "\n",
        "### 📊 프로젝트 구조\n",
        "1. **데이터 준비**: 이미지 전처리 및 정규화\n",
        "2. **모델 설계**: CNN 아키텍처 구성\n",
        "3. **학습 및 평가**: 모델 훈련 및 성능 측정\n",
        "4. **결과 분석**: 시각화 및 성능 분석\n",
        "\n",
        "### 🔧 핵심 기능\n",
        "- 이미지 데이터 자동 전처리 파이프라인\n",
        "- 최적화된 CNN 모델 아키텍처\n",
        "- 실시간 학습 모니터링 및 시각화\n",
        "- 상세한 성능 평가 및 분석 도구\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🛠️ 1. 환경 설정\n",
        "\n",
        "### 1.1 필요한 라이브러리 Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "🚀 환경 설정 완료\n",
            "==================================================\n",
            "📊 TensorFlow version: 2.19.0\n",
            "🔢 NumPy version: 2.1.3\n",
            "📈 Matplotlib & Seaborn: 로드 완료\n",
            "✅ 모든 라이브러리가 성공적으로 로드되었습니다!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# 데이터 처리 및 시각화\n",
        "import numpy as np \n",
        "import os \n",
        "import random \n",
        "import pandas as pd\n",
        "import PIL.Image as pilimg \n",
        "import imghdr\n",
        "\n",
        "# 딥러닝 프레임워크\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# 머신러닝 유틸리티\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 환경 설정\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"🚀 환경 설정 완료\")\n",
        "print(\"=\"*50)\n",
        "print(f\"📊 TensorFlow version: {tf.__version__}\")\n",
        "print(f\"🔢 NumPy version: {np.__version__}\")\n",
        "print(\"📈 Matplotlib & Seaborn: 로드 완료\")\n",
        "print(\"✅ 모든 라이브러리가 성공적으로 로드되었습니다!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1.2 데이터 경로 설정 및 구조 확인\n",
        "\n",
        "꽃 이미지 데이터가 저장된 경로를 설정하고 데이터 구조를 확인합니다.\n",
        "\n",
        "**📁 예상 데이터 구조:**\n",
        "```markdown\n",
        "flowers/\n",
        "├── train/\n",
        "│   ├── daisy/      (데이지 훈련 이미지)\n",
        "│   └── dandelion/  (민들레 훈련 이미지)\n",
        "└── test/\n",
        "    ├── daisy/      (데이지 테스트 이미지)\n",
        "    └── dandelion/  (민들레 테스트 이미지)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "📂 데이터 경로 설정 완료\n",
            "==================================================\n",
            "🎯 사용 경로: ../../data/flowers\n",
            "\n",
            "📁 데이터 구조 및 파일 개수:\n",
            "  📂 train/\n",
            "    🌸 daisy: 529개 이미지\n",
            "    🌸 dandelion: 746개 이미지\n",
            "  📂 test/\n",
            "    🌸 daisy: 77개 이미지\n",
            "    🌸 dandelion: 105개 이미지\n",
            "\n",
            "📊 총 이미지 개수: 1457개\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# 데이터 경로 설정 (여러 옵션 제공)\n",
        "possible_paths = [\n",
        "    \"../../data/flowers\",  # 상대 경로 (권장)\n",
        "    \"../../../data/flowers\",  # 다른 상대 경로\n",
        "    \"C:/Users/ryan9/문서/GitHub/SeSac-AI-Developer-Notes-2025/07_Deep_Learning/data/flowers\"  # 절대 경로\n",
        "]\n",
        "\n",
        "base_path = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        base_path = path\n",
        "        break\n",
        "\n",
        "if base_path:\n",
        "    print(\"=\"*50)\n",
        "    print(\"📂 데이터 경로 설정 완료\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"🎯 사용 경로: {base_path}\")\n",
        "    print()\n",
        "    \n",
        "    # 폴더 구조 및 데이터 개수 확인\n",
        "    print(\"📁 데이터 구조 및 파일 개수:\")\n",
        "    total_images = 0\n",
        "    \n",
        "    for split in ['train', 'test']:\n",
        "        split_path = os.path.join(base_path, split)\n",
        "        if os.path.exists(split_path):\n",
        "            print(f\"  📂 {split}/\")\n",
        "            for flower in ['daisy', 'dandelion']:\n",
        "                flower_path = os.path.join(split_path, flower)\n",
        "                if os.path.exists(flower_path):\n",
        "                    count = len([f for f in os.listdir(flower_path) \n",
        "                               if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))])\n",
        "                    total_images += count\n",
        "                    print(f\"    🌸 {flower}: {count}개 이미지\")\n",
        "                else:\n",
        "                    print(f\"    ❌ {flower}: 폴더 없음\")\n",
        "        else:\n",
        "            print(f\"  ❌ {split}/: 폴더 없음\")\n",
        "    \n",
        "    print(f\"\\n📊 총 이미지 개수: {total_images}개\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "else:\n",
        "    print(\"❌ 데이터 경로를 찾을 수 없습니다.\")\n",
        "    print(\"다음 경로들을 확인하세요:\")\n",
        "    for path in possible_paths:\n",
        "        print(f\"  - {path}\")\n",
        "    print(\"\\n💡 힌트: 노트북 파일이 올바른 위치에 있는지 확인하세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 2. 데이터 처리 파이프라인\n",
        "\n",
        "### 2.1 데이터 전처리 함수\n",
        "\n",
        "이미지 파일들을 읽어서 NumPy 배열로 변환하고 `.npz` 파일로 저장하는 함수들입니다.\n",
        "\n",
        "#### 🔄 주요 전처리 단계:\n",
        "1. **이미지 유효성 검사**: gif, png, jpeg, jpg 형식 확인\n",
        "2. **크기 정규화**: 모든 이미지를 80×80 픽셀로 리사이즈\n",
        "3. **레이블 인코딩**: daisy=0, dandelion=1\n",
        "4. **데이터 직렬화**: 처리된 데이터를 .npz 형식으로 저장\n",
        "\n",
        "#### ⚙️ 기술적 세부사항:\n",
        "- **이미지 크기**: 80×80×3 (RGB 채널)\n",
        "- **데이터 형식**: NumPy 배열 → NPZ 압축 형식\n",
        "- **메모리 최적화**: 배치 단위 처리로 메모리 효율성 확보\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def makeData(flower_name, label, isTrain=True):\n",
        "    \"\"\"\n",
        "    특정 꽃 종류의 이미지 데이터를 처리하여 NumPy 배열로 변환\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    flower_name : str\n",
        "        꽃 이름 ('daisy' 또는 'dandelion')\n",
        "    label : int\n",
        "        레이블 값 (daisy=0, dandelion=1)\n",
        "    isTrain : bool\n",
        "        True면 train 폴더, False면 test 폴더에서 데이터 로드\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    int : 처리된 이미지 수\n",
        "    \"\"\"\n",
        "    # 경로 설정\n",
        "    if base_path is None:\n",
        "        print(\"❌ base_path가 설정되지 않았습니다. 먼저 데이터 경로를 설정하세요.\")\n",
        "        return 0\n",
        "        \n",
        "    subset = \"train\" if isTrain else \"test\"\n",
        "    path = os.path.join(base_path, subset, flower_name)\n",
        "    \n",
        "    print(f\"🔄 처리 중: {subset}/{flower_name}\")\n",
        "    print(f\"📂 경로: {path}\")\n",
        "    \n",
        "    if not os.path.exists(path):\n",
        "        print(f\"❌ 경로가 존재하지 않습니다: {path}\")\n",
        "        return 0\n",
        "    \n",
        "    data = []\n",
        "    labels = []\n",
        "    error_count = 0\n",
        "    \n",
        "    # 이미지 파일 목록 가져오기\n",
        "    image_files = [f for f in os.listdir(path) \n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
        "    total_files = len(image_files)\n",
        "    \n",
        "    print(f\"📊 총 {total_files}개 이미지 파일 발견\")\n",
        "    \n",
        "    # 이미지 처리\n",
        "    for i, filename in enumerate(image_files, 1):\n",
        "        try:\n",
        "            # 진행률 표시\n",
        "            if i % 100 == 0 or i == total_files:\n",
        "                progress = (i / total_files) * 100\n",
        "                print(f\"📈 진행률: {i}/{total_files} ({progress:.1f}%)\")\n",
        "            \n",
        "            file_path = os.path.join(path, filename)\n",
        "            \n",
        "            # 이미지 형식 확인\n",
        "            kind = imghdr.what(file_path)\n",
        "            if kind not in [\"gif\", \"png\", \"jpeg\", \"jpg\"]:\n",
        "                continue\n",
        "                \n",
        "            # 이미지 로드 및 전처리\n",
        "            with pilimg.open(file_path) as img:\n",
        "                # RGB 변환 (RGBA나 다른 형식 처리)\n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "                \n",
        "                # 크기 조정\n",
        "                resize_img = img.resize((80, 80), pilimg.Resampling.LANCZOS)\n",
        "                pixel = np.array(resize_img)\n",
        "                \n",
        "                # 형태 검증\n",
        "                if pixel.shape == (80, 80, 3):\n",
        "                    data.append(pixel)\n",
        "                    labels.append(label)\n",
        "                    \n",
        "        except Exception as e:\n",
        "            error_count += 1\n",
        "            if error_count <= 5:  # 처음 5개 오류만 출력\n",
        "                print(f\"⚠️  {filename} 처리 오류: {e}\")\n",
        "            elif error_count == 6:\n",
        "                print(\"⚠️  추가 오류들은 로그에서 생략됩니다...\")\n",
        "    \n",
        "    # 결과 저장\n",
        "    savefileName = f\"C:/Users/ryan9/문서/GitHub/SeSac-AI-Developer-Notes-2025/07_Deep_Learning/data/npz/imagedata{label}_{subset}.npz\"\n",
        "    \n",
        "    if data:\n",
        "        np.savez(savefileName, data=np.array(data), targets=np.array(labels))\n",
        "        print(f\"✅ 저장 완료: {savefileName}\")\n",
        "        print(f\"📊 처리 결과: {len(data)}개 이미지 성공, {error_count}개 오류\")\n",
        "    else:\n",
        "        print(f\"❌ 처리된 이미지가 없습니다.\")\n",
        "    \n",
        "    return len(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initData():\n",
        "    \"\"\"\n",
        "    모든 꽃 종류의 train/test 데이터를 처리하여 .npz 파일로 저장\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    bool : 성공 여부\n",
        "    \"\"\"\n",
        "    flowers = [\"daisy\", \"dandelion\"]\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"🚀 데이터 초기화 프로세스 시작\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # base_path 확인\n",
        "    if base_path is None:\n",
        "        print(\"❌ 데이터 경로가 설정되지 않았습니다.\")\n",
        "        return False\n",
        "    \n",
        "    # 기존 파일 확인\n",
        "    existing_files = [f for f in os.listdir('.') \n",
        "                     if f.endswith('.npz') and f.startswith('imagedata')]\n",
        "    \n",
        "    if existing_files:\n",
        "        print(\"📁 기존 .npz 파일 발견:\")\n",
        "        for file in existing_files:\n",
        "            size = os.path.getsize(file) / (1024*1024)\n",
        "            print(f\"  - {file} ({size:.2f} MB)\")\n",
        "        \n",
        "        response = input(\"\\n덮어쓰시겠습니까? (y/N): \").lower()\n",
        "        if response != 'y':\n",
        "            print(\"🚫 작업이 취소되었습니다.\")\n",
        "            return False\n",
        "        print()\n",
        "    \n",
        "    processing_results = []\n",
        "    total_processed = 0\n",
        "    start_time = pd.Timestamp.now()\n",
        "    \n",
        "    # 각 꽃 종류별 처리\n",
        "    for i, flower in enumerate(flowers):\n",
        "        print(f\"\\n🌸 {flower.upper()} 처리 중... (레이블: {i})\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        flower_results = {}\n",
        "        \n",
        "        # Train 데이터 처리\n",
        "        print(f\"📂 훈련 데이터 처리\")\n",
        "        train_count = makeData(flower, i, True)\n",
        "        flower_results['train'] = train_count\n",
        "        \n",
        "        print(f\"\\n📂 테스트 데이터 처리\")\n",
        "        test_count = makeData(flower, i, False)\n",
        "        flower_results['test'] = test_count\n",
        "        \n",
        "        total_count = train_count + test_count\n",
        "        total_processed += total_count\n",
        "        \n",
        "        processing_results.append({\n",
        "            'flower': flower,\n",
        "            'label': i,\n",
        "            'train_count': train_count,\n",
        "            'test_count': test_count,\n",
        "            'total_count': total_count\n",
        "        })\n",
        "        \n",
        "        print(f\"✅ {flower} 완료: Train {train_count}개, Test {test_count}개, 총 {total_count}개\")\n",
        "    \n",
        "    # 처리 완료 및 결과 요약\n",
        "    end_time = pd.Timestamp.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 데이터 초기화 완료\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # 결과 테이블 출력\n",
        "    print(\"\\n📊 처리 결과 요약:\")\n",
        "    result_df = pd.DataFrame(processing_results)\n",
        "    print(result_df.to_string(index=False))\n",
        "    \n",
        "    print(f\"\\n⏱️  총 처리 시간: {duration:.1f}초\")\n",
        "    print(f\"📈 총 처리 이미지: {total_processed}개\")\n",
        "    print(f\"⚡ 평균 처리 속도: {total_processed/duration:.1f}개/초\")\n",
        "    \n",
        "    # 생성된 파일 목록 및 크기 확인\n",
        "    print(\"\\n📄 생성된 .npz 파일들:\")\n",
        "    npz_files = [f for f in os.listdir('.') \n",
        "                if f.endswith('.npz') and f.startswith('imagedata')]\n",
        "    \n",
        "    total_size = 0\n",
        "    for file in sorted(npz_files):\n",
        "        size = os.path.getsize(file) / (1024*1024)  # MB 단위\n",
        "        total_size += size\n",
        "        print(f\"  ✅ {file} ({size:.2f} MB)\")\n",
        "    \n",
        "    print(f\"\\n💾 총 파일 크기: {total_size:.2f} MB\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return total_processed > 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.2 데이터 로딩 및 정규화 함수\n",
        "\n",
        "저장된 `.npz` 파일들을 로드하여 학습용 데이터셋을 준비하는 함수들입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData():\n",
        "    \"\"\"\n",
        "    저장된 .npz 파일들을 로드하여 train/test 데이터셋 반환\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (X_train, y_train, X_test, y_test)\n",
        "        - X_train, X_test : 이미지 데이터 (numpy.ndarray)\n",
        "        - y_train, y_test : 레이블 데이터 (numpy.ndarray)\n",
        "        실패 시 (None, None, None, None) 반환\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"📂 데이터 로딩 프로세스 시작\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # 필요한 파일 목록\n",
        "    required_files = [\n",
        "        \"imagedata0_train.npz\",  # daisy train\n",
        "        \"imagedata1_train.npz\",  # dandelion train  \n",
        "        \"imagedata0_test.npz\",   # daisy test\n",
        "        \"imagedata1_test.npz\"    # dandelion test\n",
        "    ]\n",
        "    \n",
        "    # 파일 존재 여부 확인\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "    if missing_files:\n",
        "        print(\"❌ 누락된 데이터 파일:\")\n",
        "        for file in missing_files:\n",
        "            print(f\"  - {file}\")\n",
        "        print(\"\\n💡 해결 방법: initData() 함수를 먼저 실행하여 데이터를 생성하세요.\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    print(\"✅ 모든 필요한 파일이 존재합니다.\")\n",
        "    \n",
        "    datasets = {}\n",
        "    \n",
        "    # Train 데이터 로딩\n",
        "    try:\n",
        "        print(\"\\n🔄 훈련 데이터 로딩...\")\n",
        "        \n",
        "        # Daisy train 데이터\n",
        "        with np.load(\"imagedata0_train.npz\") as f:\n",
        "            daisy_train_data = f[\"data\"]\n",
        "            daisy_train_labels = f[\"targets\"]\n",
        "        \n",
        "        # Dandelion train 데이터  \n",
        "        with np.load(\"imagedata1_train.npz\") as f:\n",
        "            dandelion_train_data = f[\"data\"]\n",
        "            dandelion_train_labels = f[\"targets\"]\n",
        "        \n",
        "        # 훈련 데이터 결합\n",
        "        X_train = np.concatenate([daisy_train_data, dandelion_train_data], axis=0)\n",
        "        y_train = np.concatenate([daisy_train_labels, dandelion_train_labels], axis=0)\n",
        "        \n",
        "        print(f\"  📊 Daisy 훈련 이미지: {len(daisy_train_data)}개\")\n",
        "        print(f\"  📊 Dandelion 훈련 이미지: {len(dandelion_train_data)}개\")\n",
        "        print(f\"  📊 총 훈련 데이터: {X_train.shape}\")\n",
        "        \n",
        "        datasets['train'] = (X_train, y_train)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 훈련 데이터 로딩 실패: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Test 데이터 로딩\n",
        "    try:\n",
        "        print(\"\\n🔄 테스트 데이터 로딩...\")\n",
        "        \n",
        "        # Daisy test 데이터\n",
        "        with np.load(\"../../imagedata0_test.npz\") as f:\n",
        "            daisy_test_data = f[\"data\"]\n",
        "            daisy_test_labels = f[\"targets\"]\n",
        "        \n",
        "        # Dandelion test 데이터\n",
        "        with np.load(\"../../imagedata1_test.npz\") as f:\n",
        "            dandelion_test_data = f[\"data\"]\n",
        "            dandelion_test_labels = f[\"targets\"]\n",
        "        \n",
        "        # 테스트 데이터 결합\n",
        "        X_test = np.concatenate([daisy_test_data, dandelion_test_data], axis=0)\n",
        "        y_test = np.concatenate([daisy_test_labels, dandelion_test_labels], axis=0)\n",
        "        \n",
        "        print(f\"  📊 Daisy 테스트 이미지: {len(daisy_test_data)}개\")\n",
        "        print(f\"  📊 Dandelion 테스트 이미지: {len(dandelion_test_data)}개\")\n",
        "        print(f\"  📊 총 테스트 데이터: {X_test.shape}\")\n",
        "        \n",
        "        datasets['test'] = (X_test, y_test)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 테스트 데이터 로딩 실패: {e}\")\n",
        "        return X_train, y_train, None, None\n",
        "\n",
        "    # 데이터 요약 및 검증\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 데이터셋 요약 및 검증\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    print(f\"🔹 훈련 데이터: {X_train.shape} | 레이블: {y_train.shape}\")\n",
        "    print(f\"🔹 테스트 데이터: {X_test.shape} | 레이블: {y_test.shape}\")\n",
        "    print(f\"🔹 이미지 크기: {X_train[0].shape}\")\n",
        "    print(f\"🔹 픽셀 값 범위: {X_train.min()} ~ {X_train.max()}\")\n",
        "    print(f\"🔹 데이터 타입: {X_train.dtype}\")\n",
        "    \n",
        "    # 레이블 분포 확인\n",
        "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "    \n",
        "    print(f\"\\n📈 레이블 분포:\")\n",
        "    print(f\"  📚 훈련 세트: Daisy({unique_train[0]})={counts_train[0]}개, Dandelion({unique_train[1]})={counts_train[1]}개\")\n",
        "    print(f\"  🧪 테스트 세트: Daisy({unique_test[0]})={counts_test[0]}개, Dandelion({unique_test[1]})={counts_test[1]}개\")\n",
        "    \n",
        "    # 클래스 균형 확인\n",
        "    train_balance = counts_train[0] / counts_train[1]\n",
        "    test_balance = counts_test[0] / counts_test[1]\n",
        "    print(f\"\\n⚖️  클래스 균형:\")\n",
        "    print(f\"  📚 훈련 세트 비율: {train_balance:.2f}\")\n",
        "    print(f\"  🧪 테스트 세트 비율: {test_balance:.2f}\")\n",
        "    \n",
        "    if 0.5 <= train_balance <= 2.0 and 0.5 <= test_balance <= 2.0:\n",
        "        print(\"  ✅ 클래스 균형이 양호합니다.\")\n",
        "    else:\n",
        "        print(\"  ⚠️  클래스 불균형이 감지되었습니다. 데이터 증강을 고려하세요.\")\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🧠 3. CNN 모델 설계\n",
        "\n",
        "### 3.1 모델 아키텍처 설계\n",
        "\n",
        "Convolutional Neural Network를 구성하여 이진 분류 모델을 생성합니다.\n",
        "\n",
        "#### 🏗️ 모델 구조:\n",
        "```\n",
        "입력 레이어: (80, 80, 3)\n",
        "    ↓\n",
        "Conv2D(64 filters, 3×3) + ReLU\n",
        "    ↓\n",
        "MaxPooling2D(2×2)  # 특징맵 크기 축소\n",
        "    ↓\n",
        "Conv2D(32 filters, 3×3) + ReLU  \n",
        "    ↓\n",
        "Flatten  # 1차원으로 변환\n",
        "    ↓\n",
        "Dense(128) + ReLU\n",
        "    ↓\n",
        "Dense(32) + ReLU\n",
        "    ↓\n",
        "Dense(1) + Sigmoid  # 이진 분류용 출력\n",
        "```\n",
        "\n",
        "#### 📊 설계 원리:\n",
        "- **컨볼루션 레이어**: 이미지의 공간적 특징 추출\n",
        "- **풀링 레이어**: 차원 축소 및 과적합 방지\n",
        "- **완전연결 레이어**: 분류를 위한 고차원 특징 학습\n",
        "- **시그모이드 활성화**: 이진 분류 확률 출력 (0~1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createModel(input_shape=(80, 80, 3), summary=True):\n",
        "    \"\"\"\n",
        "    CNN 모델을 생성하고 컴파일하여 반환\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_shape : tuple\n",
        "        입력 이미지의 형태 (height, width, channels)\n",
        "    summary : bool\n",
        "        모델 구조 요약 출력 여부\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tf.keras.Model : 컴파일된 Keras 모델\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"🏗️ CNN 모델 생성 시작\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # 모델 아키텍처 정의\n",
        "    network = models.Sequential([\n",
        "        # 입력 레이어\n",
        "        layers.Input(shape=input_shape),\n",
        "        \n",
        "        # 첫 번째 컨볼루션 블록\n",
        "        layers.Conv2D(\n",
        "            filters=64, \n",
        "            kernel_size=(3, 3), \n",
        "            activation='relu',\n",
        "            padding='same',\n",
        "            name='conv2d_1'\n",
        "        ),\n",
        "        layers.MaxPooling2D(\n",
        "            pool_size=(2, 2),\n",
        "            name='maxpool2d_1'\n",
        "        ),\n",
        "        \n",
        "        # 두 번째 컨볼루션 블록\n",
        "        layers.Conv2D(\n",
        "            filters=32, \n",
        "            kernel_size=(3, 3), \n",
        "            activation='relu',\n",
        "            padding='same',\n",
        "            name='conv2d_2'\n",
        "        ),\n",
        "        layers.MaxPooling2D(\n",
        "            pool_size=(2, 2),\n",
        "            name='maxpool2d_2'\n",
        "        ),\n",
        "        \n",
        "        # 분류기 부분\n",
        "        layers.Flatten(name='flatten'),\n",
        "        \n",
        "        # 완전연결 레이어들\n",
        "        layers.Dense(\n",
        "            units=128, \n",
        "            activation='relu',\n",
        "            name='dense_1'\n",
        "        ),\n",
        "        layers.Dropout(0.3, name='dropout_1'),  # 과적합 방지\n",
        "        \n",
        "        layers.Dense(\n",
        "            units=32, \n",
        "            activation='relu',\n",
        "            name='dense_2'\n",
        "        ),\n",
        "        layers.Dropout(0.2, name='dropout_2'),  # 과적합 방지\n",
        "        \n",
        "        # 출력 레이어 (이진 분류)\n",
        "        layers.Dense(\n",
        "            units=1, \n",
        "            activation='sigmoid',\n",
        "            name='output'\n",
        "        )\n",
        "    ], name='FlowerClassifier')\n",
        "    \n",
        "    # 모델 컴파일\n",
        "    network.compile(\n",
        "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',  # 이진분류용 손실함수\n",
        "        metrics=['accuracy', 'precision', 'recall']  # 다양한 평가 지표\n",
        "    )\n",
        "    \n",
        "    print(\"✅ 모델 생성 및 컴파일 완료!\")\n",
        "    \n",
        "    # 모델 정보 출력\n",
        "    if summary:\n",
        "        print(\"\\n📋 모델 구조 요약:\")\n",
        "        print(\"-\" * 50)\n",
        "        network.summary()\n",
        "        \n",
        "        # 추가 모델 정보\n",
        "        total_params = network.count_params()\n",
        "        trainable_params = sum([tf.keras.backend.count_params(w) for w in network.trainable_weights])\n",
        "        \n",
        "        print(f\"\\n📊 모델 파라미터 정보:\")\n",
        "        print(f\"  🔹 총 파라미터: {total_params:,}\")\n",
        "        print(f\"  🔹 훈련 가능한 파라미터: {trainable_params:,}\")\n",
        "        print(f\"  🔹 고정 파라미터: {total_params - trainable_params:,}\")\n",
        "        \n",
        "        # 모델 크기 추정\n",
        "        model_size_mb = (total_params * 4) / (1024 * 1024)  # float32 기준\n",
        "        print(f\"  🔹 예상 모델 크기: {model_size_mb:.2f} MB\")\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.2 데이터 전처리 및 정규화\n",
        "\n",
        "모델 학습을 위해 이미지 데이터를 정규화하고 전처리합니다.\n",
        "\n",
        "#### 🔄 전처리 단계:\n",
        "- **픽셀 값 정규화**: 0~255 범위를 0~1 범위로 변환 (÷255)\n",
        "- **데이터 셔플링**: 학습 데이터의 순서를 무작위로 섞기\n",
        "- **데이터 검증**: 형태와 값 범위 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing(shuffle=True, random_state=42):\n",
        "    \"\"\"\n",
        "    데이터를 로드하고 전처리하여 학습 준비\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    shuffle : bool\n",
        "        훈련 데이터 셔플링 여부\n",
        "    random_state : int\n",
        "        셔플링 시 사용할 랜덤 시드\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "        정규화되고 전처리된 데이터셋\n",
        "        실패 시 (None, None, None, None) 반환\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"🔄 데이터 전처리 프로세스 시작\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # 1. 데이터 로딩\n",
        "    print(\"📂 1단계: 데이터 로딩\")\n",
        "    X_train, y_train, X_test, y_test = loadData()\n",
        "    \n",
        "    if X_train is None:\n",
        "        print(\"❌ 데이터 로딩 실패\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    print(\"✅ 데이터 로딩 완료\")\n",
        "    \n",
        "    # 2. 데이터 검증\n",
        "    print(f\"\\n🔍 2단계: 데이터 검증\")\n",
        "    print(f\"  📊 훈련 데이터: {X_train.shape}, 타입: {X_train.dtype}\")\n",
        "    print(f\"  📊 테스트 데이터: {X_test.shape}, 타입: {X_test.dtype}\")\n",
        "    print(f\"  📊 픽셀 값 범위: {X_train.min()} ~ {X_train.max()}\")\n",
        "    \n",
        "    # 데이터 타입 확인\n",
        "    if X_train.dtype != np.uint8 and (X_train.min() < 0 or X_train.max() > 255):\n",
        "        print(\"⚠️  비정상적인 픽셀 값 범위 감지\")\n",
        "    else:\n",
        "        print(\"✅ 픽셀 값 범위 정상\")\n",
        "    \n",
        "    # 3. 데이터 정규화\n",
        "    print(f\"\\n⚡ 3단계: 픽셀 값 정규화 (0~255 → 0~1)\")\n",
        "    X_train_scaled = X_train.astype(np.float32) / 255.0\n",
        "    X_test_scaled = X_test.astype(np.float32) / 255.0\n",
        "    \n",
        "    print(f\"  ✅ 정규화 완료\")\n",
        "    print(f\"  📊 훈련 데이터 범위: {X_train_scaled.min():.6f} ~ {X_train_scaled.max():.6f}\")\n",
        "    print(f\"  📊 테스트 데이터 범위: {X_test_scaled.min():.6f} ~ {X_test_scaled.max():.6f}\")\n",
        "    \n",
        "    # 4. 데이터 셔플링 (훈련 데이터만)\n",
        "    if shuffle:\n",
        "        print(f\"\\n🔀 4단계: 훈련 데이터 셔플링\")\n",
        "        np.random.seed(random_state)\n",
        "        shuffle_idx = np.random.permutation(len(X_train_scaled))\n",
        "        X_train_scaled = X_train_scaled[shuffle_idx]\n",
        "        y_train = y_train[shuffle_idx]\n",
        "        print(f\"  ✅ 셔플링 완료 (시드: {random_state})\")\n",
        "    else:\n",
        "        print(f\"\\n⏭️ 4단계: 셔플링 건너뛰기\")\n",
        "    \n",
        "    # 5. 레이블 검증 및 분포 확인\n",
        "    print(f\"\\n📈 5단계: 레이블 분석\")\n",
        "    \n",
        "    # 레이블 형태 확인\n",
        "    print(f\"  📊 훈련 레이블: {y_train.shape}, 타입: {y_train.dtype}\")\n",
        "    print(f\"  📊 테스트 레이블: {y_test.shape}, 타입: {y_test.dtype}\")\n",
        "    \n",
        "    # 레이블 값 확인\n",
        "    unique_train_labels = np.unique(y_train)\n",
        "    unique_test_labels = np.unique(y_test)\n",
        "    \n",
        "    print(f\"  📊 훈련 세트 레이블: {unique_train_labels}\")\n",
        "    print(f\"  📊 테스트 세트 레이블: {unique_test_labels}\")\n",
        "    \n",
        "    # 레이블 분포\n",
        "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "    \n",
        "    print(f\"\\n📊 클래스 분포:\")\n",
        "    train_total = len(y_train)\n",
        "    test_total = len(y_test)\n",
        "    \n",
        "    for label, count in zip(unique_train, counts_train):\n",
        "        class_name = \"Daisy\" if label == 0 else \"Dandelion\"\n",
        "        percentage = (count / train_total) * 100\n",
        "        print(f\"  📚 훈련 - {class_name}: {count}개 ({percentage:.1f}%)\")\n",
        "    \n",
        "    for label, count in zip(unique_test, counts_test):\n",
        "        class_name = \"Daisy\" if label == 0 else \"Dandelion\"  \n",
        "        percentage = (count / test_total) * 100\n",
        "        print(f\"  🧪 테스트 - {class_name}: {count}개 ({percentage:.1f}%)\")\n",
        "    \n",
        "    # 6. 메모리 사용량 확인\n",
        "    print(f\"\\n💾 6단계: 메모리 사용량\")\n",
        "    train_memory = X_train_scaled.nbytes / (1024**2)  # MB\n",
        "    test_memory = X_test_scaled.nbytes / (1024**2)   # MB\n",
        "    total_memory = train_memory + test_memory\n",
        "    \n",
        "    print(f\"  📊 훈련 데이터: {train_memory:.2f} MB\")\n",
        "    print(f\"  📊 테스트 데이터: {test_memory:.2f} MB\")\n",
        "    print(f\"  📊 총 메모리: {total_memory:.2f} MB\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✅ 데이터 전처리 완료\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return X_train_scaled, y_train, X_test_scaled, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "ini"
        }
      },
      "source": [
        "## 🚀 4. 모델 학습 및 평가\n",
        "\n",
        "### 4.1 통합 학습 파이프라인\n",
        "\n",
        "전체 딥러닝 파이프라인을 실행하는 메인 함수입니다:\n",
        "\n",
        "#### 📋 학습 프로세스:\n",
        "1. **데이터 전처리**: 로딩, 정규화, 셔플링\n",
        "2. **모델 생성**: CNN 아키텍처 구성 및 컴파일  \n",
        "3. **모델 학습**: 훈련 데이터로 모델 훈련\n",
        "4. **성능 평가**: 테스트 데이터로 모델 평가\n",
        "5. **결과 분석**: 학습 과정 및 성능 분석\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(epochs=10, batch_size=32, validation_split=0.0, save_model=False, model_name=\"flower_classifier\"):\n",
        "    \"\"\"\n",
        "    전체 학습 파이프라인을 실행하는 메인 함수\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    epochs : int\n",
        "        학습 에포크 수 (기본값: 10)\n",
        "    batch_size : int\n",
        "        배치 크기 (기본값: 32)\n",
        "    validation_split : float\n",
        "        훈련 데이터에서 검증용으로 분할할 비율 (0.0~1.0)\n",
        "    save_model : bool\n",
        "        모델 저장 여부\n",
        "    model_name : str\n",
        "        저장할 모델 파일명 (확장자 제외)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (model, history, results)\n",
        "        - model: 훈련된 모델\n",
        "        - history: 학습 기록\n",
        "        - results: 평가 결과 딕셔너리\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"🌸 꽃 분류 CNN 모델 학습 파이프라인 시작\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    start_time = pd.Timestamp.now()\n",
        "    \n",
        "    # 학습 설정 출력\n",
        "    print(f\"⚙️ 학습 설정:\")\n",
        "    print(f\"  🔹 에포크 수: {epochs}\")\n",
        "    print(f\"  🔹 배치 크기: {batch_size}\")\n",
        "    print(f\"  🔹 검증 분할: {validation_split}\")\n",
        "    print(f\"  🔹 모델 저장: {'예' if save_model else '아니요'}\")\n",
        "    if save_model:\n",
        "        print(f\"  🔹 모델명: {model_name}\")\n",
        "    print()\n",
        "    \n",
        "    # 1. 데이터 전처리\n",
        "    print(\"📊 1단계: 데이터 전처리\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = preprocessing()\n",
        "    \n",
        "    if X_train is None:\n",
        "        print(\"❌ 데이터 전처리 실패. 먼저 initData() 함수를 실행하세요.\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # 2. 모델 생성\n",
        "    print(\"\\n🧠 2단계: 모델 생성\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    model = createModel(summary=True)\n",
        "    \n",
        "    # 3. 콜백 설정\n",
        "    print(\"\\n⚙️ 3단계: 콜백 설정\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=0.0001,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    if save_model:\n",
        "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=f\"{model_name}_best.h5\",\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            verbose=1\n",
        "        )\n",
        "        callbacks.append(checkpoint_callback)\n",
        "    \n",
        "    print(f\"  ✅ 콜백 설정 완료: {len(callbacks)}개\")\n",
        "    for i, callback in enumerate(callbacks, 1):\n",
        "        print(f\"    {i}. {callback.__class__.__name__}\")\n",
        "    \n",
        "    # 4. 모델 학습\n",
        "    print(f\"\\n🎯 4단계: 모델 학습\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  🚀 학습 시작...\")\n",
        "    \n",
        "    # 검증 데이터 설정\n",
        "    if validation_split > 0:\n",
        "        validation_data = None\n",
        "        print(f\"  📊 훈련 데이터에서 {validation_split*100:.1f}% 검증용으로 분할\")\n",
        "    else:\n",
        "        validation_data = (X_test, y_test)\n",
        "        validation_split = 0.0\n",
        "        print(f\"  📊 별도 테스트 세트를 검증용으로 사용\")\n",
        "    \n",
        "    try:\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=validation_data,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        training_time = pd.Timestamp.now() - start_time\n",
        "        print(f\"\\n✅ 학습 완료! (소요시간: {training_time.total_seconds():.1f}초)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 학습 중 오류 발생: {e}\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # 5. 모델 평가\n",
        "    print(f\"\\n📊 5단계: 모델 성능 평가\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # 훈련 데이터 평가\n",
        "    print(\"🔍 훈련 데이터 평가...\")\n",
        "    train_results = model.evaluate(X_train, y_train, verbose=0)\n",
        "    train_metrics = dict(zip(model.metrics_names, train_results))\n",
        "    \n",
        "    # 테스트 데이터 평가\n",
        "    print(\"🔍 테스트 데이터 평가...\")\n",
        "    test_results = model.evaluate(X_test, y_test, verbose=0)\n",
        "    test_metrics = dict(zip(model.metrics_names, test_results))\n",
        "    \n",
        "    # 결과 정리\n",
        "    results = {\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'training_time': training_time.total_seconds(),\n",
        "        'epochs_completed': len(history.history['loss']),\n",
        "        'best_epoch': np.argmax(history.history['val_accuracy']) + 1 if 'val_accuracy' in history.history else len(history.history['loss'])\n",
        "    }\n",
        "    \n",
        "    # 결과 출력\n",
        "    print(f\"\\n📈 최종 성능 결과:\")\n",
        "    print(f\"  📚 훈련 세트:\")\n",
        "    for metric, value in train_metrics.items():\n",
        "        print(f\"    🔹 {metric}: {value:.4f}\")\n",
        "    \n",
        "    print(f\"  🧪 테스트 세트:\")\n",
        "    for metric, value in test_metrics.items():\n",
        "        print(f\"    🔹 {metric}: {value:.4f}\")\n",
        "    \n",
        "    # 일반화 성능 분석\n",
        "    accuracy_gap = train_metrics.get('accuracy', 0) - test_metrics.get('accuracy', 0)\n",
        "    print(f\"\\n🎯 일반화 성능 분석:\")\n",
        "    print(f\"  🔹 정확도 차이: {accuracy_gap:.4f}\")\n",
        "    \n",
        "    if accuracy_gap > 0.15:\n",
        "        print(f\"  ⚠️  심각한 과적합 감지\")\n",
        "        print(f\"      → 정규화 강화, 드롭아웃 증가, 데이터 증강 고려\")\n",
        "    elif accuracy_gap > 0.05:\n",
        "        print(f\"  ⚠️  경미한 과적합 감지\")\n",
        "        print(f\"      → 조기 종료, 정규화 추가 고려\")\n",
        "    else:\n",
        "        print(f\"  ✅ 양호한 일반화 성능\")\n",
        "    \n",
        "    # 6. 모델 저장\n",
        "    if save_model:\n",
        "        print(f\"\\n💾 6단계: 모델 저장\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        try:\n",
        "            final_model_path = f\"{model_name}_final.h5\"\n",
        "            model.save(final_model_path)\n",
        "            print(f\"  ✅ 최종 모델 저장: {final_model_path}\")\n",
        "            \n",
        "            # 모델 정보 저장\n",
        "            model_info = {\n",
        "                'model_name': model_name,\n",
        "                'training_time': training_time.total_seconds(),\n",
        "                'epochs': epochs,\n",
        "                'batch_size': batch_size,\n",
        "                'final_metrics': test_metrics,\n",
        "                'save_time': pd.Timestamp.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            import json\n",
        "            info_path = f\"{model_name}_info.json\"\n",
        "            with open(info_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(model_info, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"  ✅ 모델 정보 저장: {info_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 모델 저장 실패: {e}\")\n",
        "    \n",
        "    # 최종 요약\n",
        "    total_time = pd.Timestamp.now() - start_time\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(f\"🎉 파이프라인 완료\")\n",
        "    print(f\"=\"*70)\n",
        "    print(f\"⏱️  총 소요시간: {total_time.total_seconds():.1f}초\")\n",
        "    print(f\"🎯 최종 테스트 정확도: {test_metrics.get('accuracy', 0)*100:.2f}%\")\n",
        "    print(f\"📊 학습된 에포크: {results['epochs_completed']}/{epochs}\")\n",
        "    print(f\"🏆 최고 성능 에포크: {results['best_epoch']}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return model, history, results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 5. 결과 분석 및 시각화\n",
        "\n",
        "### 5.1 학습 과정 시각화 및 상세 분석\n",
        "\n",
        "학습된 모델의 성능을 다각도로 분석하고 시각화하는 도구들입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 학습 곡선 시각화 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    학습 과정의 손실값과 정확도 변화를 시각화\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    history : tf.keras.callbacks.History\n",
        "        모델 학습 기록\n",
        "    save_path : str, optional\n",
        "        그래프 저장 경로\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        print(\"❌ 학습 기록이 없습니다.\")\n",
        "        return\n",
        "    \n",
        "    print(\"📊 학습 곡선 시각화 중...\")\n",
        "    \n",
        "    # 학습 기록에서 메트릭 추출\n",
        "    metrics = list(history.history.keys())\n",
        "    train_metrics = [m for m in metrics if not m.startswith('val_')]\n",
        "    val_metrics = [m for m in metrics if m.startswith('val_')]\n",
        "    \n",
        "    n_metrics = len(train_metrics)\n",
        "    fig, axes = plt.subplots(1, n_metrics, figsize=(6*n_metrics, 5))\n",
        "    \n",
        "    if n_metrics == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    \n",
        "    for i, metric in enumerate(train_metrics):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # 훈련 메트릭 플롯\n",
        "        epochs = range(1, len(history.history[metric]) + 1)\n",
        "        ax.plot(epochs, history.history[metric], \n",
        "               color=colors[0], marker='o', markersize=4, \n",
        "               label=f'Training {metric.title()}', linewidth=2)\n",
        "        \n",
        "        # 검증 메트릭 플롯 (있는 경우)\n",
        "        val_metric = f'val_{metric}'\n",
        "        if val_metric in history.history:\n",
        "            ax.plot(epochs, history.history[val_metric], \n",
        "                   color=colors[1], marker='s', markersize=4,\n",
        "                   label=f'Validation {metric.title()}', linewidth=2)\n",
        "        \n",
        "        # 그래프 설정\n",
        "        ax.set_title(f'{metric.title()} over Epochs', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Epoch', fontsize=12)\n",
        "        ax.set_ylabel(metric.title(), fontsize=12)\n",
        "        ax.legend(fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 최적값 표시\n",
        "        if val_metric in history.history:\n",
        "            if 'loss' in metric:\n",
        "                best_epoch = np.argmin(history.history[val_metric]) + 1\n",
        "                best_value = min(history.history[val_metric])\n",
        "            else:\n",
        "                best_epoch = np.argmax(history.history[val_metric]) + 1\n",
        "                best_value = max(history.history[val_metric])\n",
        "            \n",
        "            ax.axvline(x=best_epoch, color=colors[2], linestyle='--', alpha=0.7)\n",
        "            ax.text(best_epoch, best_value, f'Best: {best_value:.3f}\\\\nEpoch: {best_epoch}',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=colors[2], alpha=0.3),\n",
        "                   fontsize=9, ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"  ✅ 학습 곡선 저장: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # 학습 요약 출력\n",
        "    final_epoch = len(history.history['loss'])\n",
        "    print(f\"\\n📈 학습 요약:\")\n",
        "    print(f\"  🔹 총 에포크: {final_epoch}\")\n",
        "    \n",
        "    for metric in train_metrics:\n",
        "        final_train = history.history[metric][-1]\n",
        "        val_metric = f'val_{metric}'\n",
        "        \n",
        "        if val_metric in history.history:\n",
        "            final_val = history.history[val_metric][-1]\n",
        "            print(f\"  🔹 최종 {metric}: 훈련 {final_train:.4f}, 검증 {final_val:.4f}\")\n",
        "            \n",
        "            if 'loss' in metric:\n",
        "                best_val = min(history.history[val_metric])\n",
        "                best_epoch = np.argmin(history.history[val_metric]) + 1\n",
        "            else:\n",
        "                best_val = max(history.history[val_metric])\n",
        "                best_epoch = np.argmax(history.history[val_metric]) + 1\n",
        "            \n",
        "            print(f\"  🔹 최고 검증 {metric}: {best_val:.4f} (에포크 {best_epoch})\")\n",
        "        else:\n",
        "            print(f\"  🔹 최종 {metric}: {final_train:.4f}\")\n",
        "\n",
        "print(\"✅ 학습 곡선 시각화 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 5.2 추가 분석 및 시각화 도구\n",
        "\n",
        "모델 성능을 더 자세히 분석하고 시각화하는 추가 도구들입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 고급 분석 함수들이 정의되었습니다!\n",
            "💡 사용 예시:\n",
            "   evaluate_model_detailed(model, X_test_scaled, y_test)\n",
            "   show_sample_predictions(model, X_test_scaled, y_test, num_samples=12)\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model_detailed(model, X_test, y_test, save_path=None):\n",
        "    \"\"\"\n",
        "    모델의 상세한 성능 평가 및 혼동행렬 시각화\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tf.keras.Model\n",
        "        평가할 모델\n",
        "    X_test : numpy.ndarray\n",
        "        테스트 이미지 데이터\n",
        "    y_test : numpy.ndarray\n",
        "        테스트 레이블\n",
        "    save_path : str, optional\n",
        "        혼동행렬 저장 경로\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        print(\"❌ 모델이 없습니다.\")\n",
        "        return None, None\n",
        "    \n",
        "    print(\"🔍 상세 모델 평가 진행 중...\")\n",
        "    \n",
        "    # 예측 수행\n",
        "    y_pred_prob = model.predict(X_test, verbose=0)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # 혼동행렬\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # 분류 리포트\n",
        "    class_names = ['Daisy', 'Dandelion']\n",
        "    target_names = [f'{name} ({i})' for i, name in enumerate(class_names)]\n",
        "    report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
        "    \n",
        "    print(\"📊 상세 성능 평가 결과:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 클래스별 성능\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_key = f'{class_name} ({i})'\n",
        "        metrics = report[class_key]\n",
        "        print(f\"🌸 {class_name}:\")\n",
        "        print(f\"   정밀도(Precision): {metrics['precision']:.4f}\")\n",
        "        print(f\"   재현율(Recall):     {metrics['recall']:.4f}\")\n",
        "        print(f\"   F1-점수:           {metrics['f1-score']:.4f}\")\n",
        "        print(f\"   지원 샘플:         {int(metrics['support'])}\")\n",
        "        print()\n",
        "    \n",
        "    # 전체 성능\n",
        "    print(f\"📈 전체 성능:\")\n",
        "    print(f\"   정확도:            {report['accuracy']:.4f}\")\n",
        "    print(f\"   매크로 평균 F1:    {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"   가중 평균 F1:      {report['weighted avg']['f1-score']:.4f}\")\n",
        "    \n",
        "    # 혼동행렬 시각화\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    \n",
        "    # 정규화된 혼동행렬 계산\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    # 히트맵 생성\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                square=True, linewidths=0.5)\n",
        "    \n",
        "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    \n",
        "    # 정확도 표시\n",
        "    total_samples = cm.sum()\n",
        "    correct_predictions = np.diag(cm).sum()\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    \n",
        "    plt.figtext(0.02, 0.02, f'Total Accuracy: {accuracy:.4f} ({correct_predictions}/{total_samples})',\n",
        "                fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.5))\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"  ✅ 혼동행렬 저장: {save_path}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return y_pred, y_pred_prob\n",
        "\n",
        "def show_sample_predictions(model, X_test, y_test, num_samples=8, save_path=None):\n",
        "    \"\"\"\n",
        "    랜덤 샘플 이미지들의 예측 결과를 시각화\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tf.keras.Model\n",
        "        예측에 사용할 모델\n",
        "    X_test : numpy.ndarray\n",
        "        테스트 이미지 데이터\n",
        "    y_test : numpy.ndarray\n",
        "        테스트 레이블\n",
        "    num_samples : int\n",
        "        표시할 샘플 수\n",
        "    save_path : str, optional\n",
        "        결과 이미지 저장 경로\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        print(\"❌ 모델이 없습니다.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"🖼️ {num_samples}개 샘플 예측 시각화 중...\")\n",
        "    \n",
        "    # 랜덤 샘플 선택\n",
        "    np.random.seed(42)  # 재현 가능한 결과를 위해\n",
        "    indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
        "    \n",
        "    # 예측 수행\n",
        "    predictions = model.predict(X_test[indices], verbose=0)\n",
        "    \n",
        "    # 시각화 설정\n",
        "    rows = 2\n",
        "    cols = num_samples // rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    class_names = ['Daisy', 'Dandelion']\n",
        "    correct_count = 0\n",
        "    \n",
        "    for i, idx in enumerate(indices):\n",
        "        # 이미지 표시\n",
        "        axes[i].imshow(X_test[idx])\n",
        "        axes[i].axis('off')\n",
        "        \n",
        "        # 예측 결과\n",
        "        pred_prob = predictions[i][0]\n",
        "        pred_class = int(pred_prob > 0.5)\n",
        "        actual_class = int(y_test[idx])\n",
        "        \n",
        "        # 정확도 카운트\n",
        "        if pred_class == actual_class:\n",
        "            correct_count += 1\n",
        "        \n",
        "        # 신뢰도 색상 설정\n",
        "        confidence = max(pred_prob, 1 - pred_prob)\n",
        "        if pred_class == actual_class:\n",
        "            if confidence > 0.8:\n",
        "                color = 'darkgreen'\n",
        "            else:\n",
        "                color = 'green'\n",
        "        else:\n",
        "            if confidence > 0.8:\n",
        "                color = 'darkred'\n",
        "            else:\n",
        "                color = 'red'\n",
        "        \n",
        "        # 제목 설정\n",
        "        title = f'예측: {class_names[pred_class]} ({pred_prob:.3f})\\\\n실제: {class_names[actual_class]}'\n",
        "        axes[i].set_title(title, color=color, fontsize=11, fontweight='bold')\n",
        "        \n",
        "        # 테두리 색상 설정\n",
        "        for spine in axes[i].spines.values():\n",
        "            spine.set_color(color)\n",
        "            spine.set_linewidth(2)\n",
        "    \n",
        "    # 전체 제목\n",
        "    sample_accuracy = correct_count / num_samples\n",
        "    fig.suptitle(f'샘플 예측 결과 (정확도: {sample_accuracy:.2%})', \n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"  ✅ 샘플 예측 결과 저장: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"📊 샘플 분석 결과:\")\n",
        "    print(f\"  🔹 정확 예측: {correct_count}/{num_samples} ({sample_accuracy:.2%})\")\n",
        "    print(f\"  🔹 오류 예측: {num_samples - correct_count}/{num_samples}\")\n",
        "\n",
        "print(\"✅ 고급 분석 함수들이 정의되었습니다!\")\n",
        "print(\"💡 사용 예시:\")\n",
        "print(\"   evaluate_model_detailed(model, X_test_scaled, y_test)\")\n",
        "print(\"   show_sample_predictions(model, X_test_scaled, y_test, num_samples=12)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🚀 6. 실행 가이드\n",
        "\n",
        "### 6.1 단계별 실행 방법\n",
        "\n",
        "이 섹션에서는 프로젝트를 처음부터 끝까지 실행하는 방법을 단계별로 안내합니다.\n",
        "\n",
        "#### 📋 실행 순서:\n",
        "\n",
        "1. **첫 실행 시 (데이터 생성)**\n",
        "2. **모델 학습 및 평가**  \n",
        "3. **결과 분석 및 시각화**\n",
        "\n",
        "### 6.2 실제 실행 코드\n",
        "\n",
        "아래 코드들의 주석을 해제하여 단계별로 실행하세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "🌸 꽃 분류 CNN 프로젝트 실행 가이드\n",
            "================================================================================\n",
            "\n",
            "📋 실행 단계:\n",
            "  1️⃣ 첫 실행 시: initData() - 이미지를 .npz 파일로 변환 (시간 소요)\n",
            "  2️⃣ 모델 학습: main() - CNN 모델 훈련 및 평가\n",
            "  3️⃣ 결과 분석: 시각화 함수들로 상세 분석\n",
            "\n",
            "💡 주석을 해제하여 코드를 실행하세요!\n",
            "================================================================================\n",
            "1️⃣ 데이터 초기화 시작...\n",
            "============================================================\n",
            "🚀 데이터 초기화 프로세스 시작\n",
            "============================================================\n",
            "\n",
            "🌸 DAISY 처리 중... (레이블: 0)\n",
            "----------------------------------------\n",
            "📂 훈련 데이터 처리\n",
            "🔄 처리 중: train/daisy\n",
            "📂 경로: ../../data/flowers\\train\\daisy\n",
            "📊 총 529개 이미지 파일 발견\n",
            "📈 진행률: 100/529 (18.9%)\n",
            "📈 진행률: 200/529 (37.8%)\n",
            "📈 진행률: 300/529 (56.7%)\n",
            "📈 진행률: 400/529 (75.6%)\n",
            "📈 진행률: 500/529 (94.5%)\n",
            "📈 진행률: 529/529 (100.0%)\n",
            "✅ 저장 완료: ../../data/npz/imagedata0_train.npz\n",
            "📊 처리 결과: 529개 이미지 성공, 0개 오류\n",
            "\n",
            "📂 테스트 데이터 처리\n",
            "🔄 처리 중: test/daisy\n",
            "📂 경로: ../../data/flowers\\test\\daisy\n",
            "📊 총 77개 이미지 파일 발견\n",
            "📈 진행률: 77/77 (100.0%)\n",
            "✅ 저장 완료: ../../data/npz/imagedata0_test.npz\n",
            "📊 처리 결과: 77개 이미지 성공, 0개 오류\n",
            "✅ daisy 완료: Train 529개, Test 77개, 총 606개\n",
            "\n",
            "🌸 DANDELION 처리 중... (레이블: 1)\n",
            "----------------------------------------\n",
            "📂 훈련 데이터 처리\n",
            "🔄 처리 중: train/dandelion\n",
            "📂 경로: ../../data/flowers\\train\\dandelion\n",
            "📊 총 746개 이미지 파일 발견\n",
            "📈 진행률: 100/746 (13.4%)\n",
            "📈 진행률: 200/746 (26.8%)\n",
            "📈 진행률: 300/746 (40.2%)\n",
            "📈 진행률: 400/746 (53.6%)\n",
            "📈 진행률: 500/746 (67.0%)\n",
            "📈 진행률: 600/746 (80.4%)\n",
            "📈 진행률: 700/746 (93.8%)\n",
            "📈 진행률: 746/746 (100.0%)\n",
            "✅ 저장 완료: ../../data/npz/imagedata1_train.npz\n",
            "📊 처리 결과: 746개 이미지 성공, 0개 오류\n",
            "\n",
            "📂 테스트 데이터 처리\n",
            "🔄 처리 중: test/dandelion\n",
            "📂 경로: ../../data/flowers\\test\\dandelion\n",
            "📊 총 105개 이미지 파일 발견\n",
            "📈 진행률: 100/105 (95.2%)\n",
            "📈 진행률: 105/105 (100.0%)\n",
            "✅ 저장 완료: ../../data/npz/imagedata1_test.npz\n",
            "📊 처리 결과: 105개 이미지 성공, 0개 오류\n",
            "✅ dandelion 완료: Train 746개, Test 105개, 총 851개\n",
            "\n",
            "============================================================\n",
            "🎉 데이터 초기화 완료\n",
            "============================================================\n",
            "\n",
            "📊 처리 결과 요약:\n",
            "   flower  label  train_count  test_count  total_count\n",
            "    daisy      0          529          77          606\n",
            "dandelion      1          746         105          851\n",
            "\n",
            "⏱️  총 처리 시간: 7.7초\n",
            "📈 총 처리 이미지: 1457개\n",
            "⚡ 평균 처리 속도: 190.2개/초\n",
            "\n",
            "📄 생성된 .npz 파일들:\n",
            "\n",
            "💾 총 파일 크기: 0.00 MB\n",
            "============================================================\n",
            "✅ 데이터 초기화 완료!\n",
            "\n",
            "2️⃣ 빠른 모델 테스트...\n",
            "======================================================================\n",
            "🌸 꽃 분류 CNN 모델 학습 파이프라인 시작\n",
            "======================================================================\n",
            "⚙️ 학습 설정:\n",
            "  🔹 에포크 수: 3\n",
            "  🔹 배치 크기: 16\n",
            "  🔹 검증 분할: 0.0\n",
            "  🔹 모델 저장: 아니요\n",
            "\n",
            "📊 1단계: 데이터 전처리\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "🔄 데이터 전처리 프로세스 시작\n",
            "==================================================\n",
            "📂 1단계: 데이터 로딩\n",
            "==================================================\n",
            "📂 데이터 로딩 프로세스 시작\n",
            "==================================================\n",
            "❌ 누락된 데이터 파일:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "💡 해결 방법: initData() 함수를 먼저 실행하여 데이터를 생성하세요.\n",
            "❌ 데이터 로딩 실패\n",
            "❌ 데이터 전처리 실패. 먼저 initData() 함수를 실행하세요.\n",
            "\n",
            "3️⃣ 정식 모델 학습...\n",
            "======================================================================\n",
            "🌸 꽃 분류 CNN 모델 학습 파이프라인 시작\n",
            "======================================================================\n",
            "⚙️ 학습 설정:\n",
            "  🔹 에포크 수: 10\n",
            "  🔹 배치 크기: 32\n",
            "  🔹 검증 분할: 0.0\n",
            "  🔹 모델 저장: 예\n",
            "  🔹 모델명: flower_cnn_v1\n",
            "\n",
            "📊 1단계: 데이터 전처리\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "🔄 데이터 전처리 프로세스 시작\n",
            "==================================================\n",
            "📂 1단계: 데이터 로딩\n",
            "==================================================\n",
            "📂 데이터 로딩 프로세스 시작\n",
            "==================================================\n",
            "❌ 누락된 데이터 파일:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "💡 해결 방법: initData() 함수를 먼저 실행하여 데이터를 생성하세요.\n",
            "❌ 데이터 로딩 실패\n",
            "❌ 데이터 전처리 실패. 먼저 initData() 함수를 실행하세요.\n",
            "\n",
            "4️⃣ 결과 분석 및 시각화...\n",
            "==================================================\n",
            "🔄 데이터 전처리 프로세스 시작\n",
            "==================================================\n",
            "📂 1단계: 데이터 로딩\n",
            "==================================================\n",
            "📂 데이터 로딩 프로세스 시작\n",
            "==================================================\n",
            "❌ 누락된 데이터 파일:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "💡 해결 방법: initData() 함수를 먼저 실행하여 데이터를 생성하세요.\n",
            "❌ 데이터 로딩 실패\n",
            "❌ 데이터 또는 모델이 준비되지 않았습니다.\n",
            "\n",
            "5️⃣ 개별 함수 테스트...\n",
            "==================================================\n",
            "📂 데이터 로딩 프로세스 시작\n",
            "==================================================\n",
            "❌ 누락된 데이터 파일:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "💡 해결 방법: initData() 함수를 먼저 실행하여 데이터를 생성하세요.\n",
            "==================================================\n",
            "🔄 데이터 전처리 프로세스 시작\n",
            "==================================================\n",
            "📂 1단계: 데이터 로딩\n",
            "==================================================\n",
            "📂 데이터 로딩 프로세스 시작\n",
            "==================================================\n",
            "❌ 누락된 데이터 파일:\n",
            "  - imagedata0_train.npz\n",
            "  - imagedata1_train.npz\n",
            "  - imagedata0_test.npz\n",
            "  - imagedata1_test.npz\n",
            "\n",
            "💡 해결 방법: initData() 함수를 먼저 실행하여 데이터를 생성하세요.\n",
            "❌ 데이터 로딩 실패\n",
            "==================================================\n",
            "🏗️ CNN 모델 생성 시작\n",
            "==================================================\n",
            "✅ 모델 생성 및 컴파일 완료!\n",
            "\n",
            "📋 모델 구조 요약:\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FlowerClassifier\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"FlowerClassifier\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,638,528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,638,528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,662,945</span> (6.34 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,662,945\u001b[0m (6.34 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,662,945</span> (6.34 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,662,945\u001b[0m (6.34 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 모델 파라미터 정보:\n",
            "  🔹 총 파라미터: 1,662,945\n",
            "  🔹 훈련 가능한 파라미터: 1,662,945\n",
            "  🔹 고정 파라미터: 0\n",
            "  🔹 예상 모델 크기: 6.34 MB\n",
            "==================================================\n",
            "\n",
            "================================================================================\n",
            "🎯 실행 완료 후 다음과 같은 파일들이 생성됩니다:\n",
            "  📄 imagedata*.npz - 전처리된 이미지 데이터\n",
            "  🤖 *_final.h5 - 훈련된 모델 파일\n",
            "  📋 *_info.json - 모델 정보\n",
            "  📊 *.png - 분석 결과 이미지\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 🚀 실행 가이드 - 아래 주석을 해제하여 단계별로 실행하세요\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🌸 꽃 분류 CNN 프로젝트 실행 가이드\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"📋 실행 단계:\")\n",
        "print(\"  1️⃣ 첫 실행 시: initData() - 이미지를 .npz 파일로 변환 (시간 소요)\")\n",
        "print(\"  2️⃣ 모델 학습: main() - CNN 모델 훈련 및 평가\")  \n",
        "print(\"  3️⃣ 결과 분석: 시각화 함수들로 상세 분석\")\n",
        "print()\n",
        "print(\"💡 주석을 해제하여 코드를 실행하세요!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1단계: 데이터 초기화 (첫 실행 시에만, 약 5-10분 소요)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"1️⃣ 데이터 초기화 시작...\")\n",
        "success = initData()\n",
        "if not success:\n",
        "    print(\"❌ 데이터 초기화 실패\")\n",
        "else:\n",
        "    print(\"✅ 데이터 초기화 완료!\")\n",
        "\n",
        "# -----------------------------------------------------------------------------  \n",
        "# 2단계: 빠른 테스트 (3 에포크, 약 1-2분)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n2️⃣ 빠른 모델 테스트...\")\n",
        "model, history, results = main(\n",
        "    epochs=3, \n",
        "    batch_size=16,\n",
        "    save_model=False\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3단계: 정식 학습 (10 에포크, 약 5-10분)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n3️⃣ 정식 모델 학습...\")\n",
        "model, history, results = main(\n",
        "    epochs=10,\n",
        "    batch_size=32, \n",
        "    save_model=True,\n",
        "    model_name=\"flower_cnn_v1\"\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4단계: 결과 분석 및 시각화\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n4️⃣ 결과 분석 및 시각화...\")\n",
        "\n",
        "# 전처리된 데이터 로드\n",
        "X_train, y_train, X_test, y_test = preprocessing()\n",
        "\n",
        "if X_test is not None and model is not None:\n",
        "    # 학습 곡선 시각화\n",
        "    plot_training_history(history, save_path=\"training_history.png\")\n",
        "    \n",
        "    # 상세 성능 평가\n",
        "    y_pred, y_pred_prob = evaluate_model_detailed(\n",
        "        model, X_test, y_test, \n",
        "        save_path=\"confusion_matrix.png\"\n",
        "    )\n",
        "    \n",
        "    # 샘플 예측 시각화\n",
        "    show_sample_predictions(\n",
        "        model, X_test, y_test, \n",
        "        num_samples=12,\n",
        "        save_path=\"sample_predictions.png\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\n🎉 모든 분석 완료!\")\n",
        "else:\n",
        "    print(\"❌ 데이터 또는 모델이 준비되지 않았습니다.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5단계: 개별 함수 테스트 (선택사항)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n5️⃣ 개별 함수 테스트...\")\n",
        "\n",
        "# 데이터만 로드하여 확인\n",
        "X_train, y_train, X_test, y_test = loadData()\n",
        "\n",
        "# 전처리만 테스트\n",
        "X_train_scaled, y_train, X_test_scaled, y_test = preprocessing()\n",
        "\n",
        "# 모델만 생성하여 구조 확인\n",
        "test_model = createModel(summary=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 실행 완료 후 다음과 같은 파일들이 생성됩니다:\")\n",
        "print(\"  📄 imagedata*.npz - 전처리된 이미지 데이터\")\n",
        "print(\"  🤖 *_final.h5 - 훈련된 모델 파일\")\n",
        "print(\"  📋 *_info.json - 모델 정보\")\n",
        "print(\"  📊 *.png - 분석 결과 이미지\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📋 7. 프로젝트 정리 및 확장 방안\n",
        "\n",
        "### 7.1 현재 구현된 기능\n",
        "\n",
        "#### ✅ 완성된 기능들:\n",
        "- **데이터 파이프라인**: 이미지 전처리, 정규화, 배치 처리\n",
        "- **CNN 모델**: Conv2D, MaxPooling, Dropout이 포함된 최적화된 구조\n",
        "- **학습 시스템**: 조기 종료, 학습률 스케줄링 등 고급 기능\n",
        "- **평가 도구**: 혼동행렬, 분류 리포트, 시각화\n",
        "- **모델 관리**: 자동 저장, 메타데이터 기록\n",
        "\n",
        "#### 🔧 기술적 특징:\n",
        "- **메모리 효율성**: NPZ 압축 형식으로 데이터 저장\n",
        "- **확장성**: 모듈화된 함수 구조로 쉬운 수정 가능\n",
        "- **재현성**: 랜덤 시드 고정으로 결과 재현 보장\n",
        "- **사용자 친화성**: 상세한 진행률 표시 및 오류 처리\n",
        "\n",
        "### 7.2 개선 및 확장 가능 사항\n",
        "\n",
        "#### 🚀 성능 향상 방안:\n",
        "\n",
        "1. **모델 아키텍처 개선**:\n",
        "   ```python\n",
        "   # 배치 정규화 추가\n",
        "   layers.BatchNormalization()\n",
        "   \n",
        "   # ResNet 스타일 잔차 연결\n",
        "   # VGG, ResNet 등 사전 훈련된 모델 활용\n",
        "   ```\n",
        "\n",
        "2. **데이터 증강**:\n",
        "   ```python\n",
        "   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "   \n",
        "   datagen = ImageDataGenerator(\n",
        "       rotation_range=30,\n",
        "       width_shift_range=0.3,\n",
        "       height_shift_range=0.3,\n",
        "       horizontal_flip=True,\n",
        "       zoom_range=0.3,\n",
        "       brightness_range=[0.8, 1.2],\n",
        "       fill_mode='nearest'\n",
        "   )\n",
        "   ```\n",
        "\n",
        "3. **고급 최적화 기법**:\n",
        "   - **Learning Rate Scheduling**: 코사인 어닐링, 웜업\n",
        "   - **정규화 기법**: L1/L2 정규화, 스펙트럴 정규화\n",
        "   - **앙상블 기법**: 여러 모델의 결과 조합\n",
        "\n",
        "#### 📊 확장 가능한 기능들:\n",
        "\n",
        "1. **다중 클래스 분류**: 더 많은 꽃 종류 추가\n",
        "2. **실시간 분류**: 웹캠을 통한 실시간 예측\n",
        "3. **모바일 배포**: TensorFlow Lite로 모바일 최적화\n",
        "4. **웹 서비스**: Flask/FastAPI를 통한 API 서버 구축\n",
        "\n",
        "### 7.3 실제 운영 고려사항\n",
        "\n",
        "#### 💡 배포 전 체크리스트:\n",
        "- [ ] 다양한 해상도 이미지 테스트\n",
        "- [ ] 메모리 사용량 프로파일링\n",
        "- [ ] 모델 추론 속도 벤치마크\n",
        "- [ ] 엣지 케이스 처리 (흐릿한 이미지, 다른 객체 등)\n",
        "- [ ] A/B 테스트를 통한 성능 검증\n",
        "\n",
        "#### 🔒 보안 및 품질 관리:\n",
        "- **모델 버전 관리**: MLflow, DVC 등 활용\n",
        "- **데이터 검증**: 입력 데이터 형식 및 크기 검증\n",
        "- **모니터링**: 예측 성능 실시간 모니터링\n",
        "- **롤백 시스템**: 문제 발생 시 이전 버전으로 복구\n",
        "\n",
        "이 프로젝트는 기본적인 이미지 분류부터 실제 운영 가능한 시스템까지의 완전한 파이프라인을 제공합니다. 🌸✨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "🎉 꽃 분류 CNN 프로젝트 완료!\n",
            "================================================================================\n",
            "\n",
            "📋 프로젝트 요약:\n",
            "  • 목표: Daisy vs Dandelion 이진 분류\n",
            "  • 기술: CNN, TensorFlow/Keras\n",
            "  • 특징: 완전 자동화된 파이프라인\n",
            "\n",
            "🛠️ 구현된 주요 기능:\n",
            "  ✓ 이미지 데이터 전처리 및 정규화\n",
            "  ✓ 최적화된 CNN 모델 아키텍처\n",
            "  ✓ 고급 학습 기법 (조기 종료, 학습률 스케줄링)\n",
            "  ✓ 상세한 성능 평가 및 시각화\n",
            "  ✓ 모델 저장 및 메타데이터 관리\n",
            "\n",
            "🚀 다음 학습 방향:\n",
            "  • 전이 학습 (Transfer Learning)\n",
            "  • 고급 데이터 증강 기법\n",
            "  • 모델 앙상블 및 최적화\n",
            "  • 실시간 추론 시스템 구축\n",
            "  • 웹/모바일 앱 배포\n",
            "\n",
            "📚 관련 학습 자료:\n",
            "  • TensorFlow 공식 문서: https://tensorflow.org\n",
            "  • Keras 가이드: https://keras.io/guides/\n",
            "  • 컴퓨터 비전 튜토리얼: https://tensorflow.org/tutorials/images\n",
            "\n",
            "================================================================================\n",
            "감사합니다! 즐거운 딥러닝 여정이 되시길 바랍니다! 🌸✨\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 프로젝트 완료 - 요약 및 다음 단계\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🎉 꽃 분류 CNN 프로젝트 완료!\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"📋 프로젝트 요약:\")\n",
        "print(\"  • 목표: Daisy vs Dandelion 이진 분류\")\n",
        "print(\"  • 기술: CNN, TensorFlow/Keras\") \n",
        "print(\"  • 특징: 완전 자동화된 파이프라인\")\n",
        "print()\n",
        "print(\"🛠️ 구현된 주요 기능:\")\n",
        "print(\"  ✓ 이미지 데이터 전처리 및 정규화\")\n",
        "print(\"  ✓ 최적화된 CNN 모델 아키텍처\")\n",
        "print(\"  ✓ 고급 학습 기법 (조기 종료, 학습률 스케줄링)\")\n",
        "print(\"  ✓ 상세한 성능 평가 및 시각화\")\n",
        "print(\"  ✓ 모델 저장 및 메타데이터 관리\")\n",
        "print()\n",
        "print(\"🚀 다음 학습 방향:\")\n",
        "print(\"  • 전이 학습 (Transfer Learning)\")\n",
        "print(\"  • 고급 데이터 증강 기법\")\n",
        "print(\"  • 모델 앙상블 및 최적화\")\n",
        "print(\"  • 실시간 추론 시스템 구축\")\n",
        "print(\"  • 웹/모바일 앱 배포\")\n",
        "print()\n",
        "print(\"📚 관련 학습 자료:\")\n",
        "print(\"  • TensorFlow 공식 문서: https://tensorflow.org\")\n",
        "print(\"  • Keras 가이드: https://keras.io/guides/\")\n",
        "print(\"  • 컴퓨터 비전 튜토리얼: https://tensorflow.org/tutorials/images\")\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"감사합니다! 즐거운 딥러닝 여정이 되시길 바랍니다! 🌸✨\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
