{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8fd6cd",
   "metadata": {},
   "source": [
    "# Cats vs Dogs ConvNet (PyTorch)\n",
    "\n",
    "원본 `.py` 스크립트를 Jupyter Notebook 형태로 재구성했습니다. 각 단계는 독립 셀로 나뉘어 실행 순서에 맞게 배치되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c528e",
   "metadata": {},
   "source": [
    "## 0. 필요한 패키지 설치(필요 시)\n",
    "로컬 환경에 미설치라면 아래 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0567a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 시만 실행\n",
    "%pip install kagglehub torch torchvision matplotlib --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a27f8",
   "metadata": {},
   "source": [
    "## 1. 기본 설정 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889781d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import kagglehub\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b523a",
   "metadata": {},
   "source": [
    "## 2. Kaggle 데이터셋 다운로드(선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ace3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"salader/dogs-vs-cats\")\n",
    "print(\"KaggleHub dataset path:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00107574",
   "metadata": {},
   "source": [
    "## 3. 경로 및 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = pathlib.Path(\"07_Deep_Learning/data/cats_and_dogs\") / \"train\"\n",
    "new_base_dir = pathlib.Path(\"07_Deep_Learning/data/cats_and_dogs_small\")\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "num_epochs = 3\n",
    "model_save_path = \"convnet_from_scratch.pth\"\n",
    "\n",
    "print(\"Path to original dataset:\", original_dir)\n",
    "print(\"Path to subset dataset:\", new_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc85e3e",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 서브셋 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335feb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        # KaggleHub 데이터는 이미 cats/dogs 폴더로 구분되어 있음\n",
    "        source_dir = original_dir / f\"{category}s\"  # cats 또는 dogs 폴더\n",
    "        dest_dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        \n",
    "        # 파일명은 cat.0.jpg, dog.0.jpg 형태\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            src_path = source_dir / fname\n",
    "            dst_path = dest_dir / fname\n",
    "            if src_path.exists():  # 파일이 존재하는 경우만 복사\n",
    "                shutil.copyfile(src=src_path, dst=dst_path)\n",
    "\n",
    "if not os.path.exists(new_base_dir):\n",
    "    print(\"데이터셋 서브셋 생성 중...\")\n",
    "    make_subset(\"train\", start_index=0, end_index=1000)\n",
    "    make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "    make_subset(\"test\", start_index=1500, end_index=2500)\n",
    "    print(\"데이터셋 서브셋 생성 완료.\")\n",
    "else:\n",
    "    print(\"서브셋 디렉토리가 이미 존재합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1278393",
   "metadata": {},
   "source": [
    "## 5. 데이터 증강 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d569a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109a211",
   "metadata": {},
   "source": [
    "## 6. 데이터셋 로드 및 DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 확인\n",
    "for subset in [\"train\", \"validation\", \"test\"]:\n",
    "    subset_path = new_base_dir / subset\n",
    "    if not subset_path.exists():\n",
    "        print(f\"경고: {subset_path} 경로가 존재하지 않습니다.\")\n",
    "    else:\n",
    "        print(f\"{subset} 데이터셋 경로: {subset_path}\")\n",
    "\n",
    "train_dataset = datasets.ImageFolder(new_base_dir / \"train\", transform=train_transforms)\n",
    "validation_dataset = datasets.ImageFolder(new_base_dir / \"validation\", transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(new_base_dir / \"test\", transform=val_test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"데이터셋 크기 - 훈련: {len(train_dataset)}, 검증: {len(validation_dataset)}, 테스트: {len(test_dataset)}\")\n",
    "print(f\"클래스: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9adb9",
   "metadata": {},
   "source": [
    "## 7. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 11 * 11, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f698e4",
   "metadata": {},
   "source": [
    "## 8. 최적화기와 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b74cd",
   "metadata": {},
   "source": [
    "## 9. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    epoch_val_loss = val_loss / len(validation_loader.dataset)\n",
    "    epoch_val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"train_loss: {epoch_loss:.4f}, train_acc: {epoch_acc:.4f} - \"\n",
    "          f\"val_loss: {epoch_val_loss:.4f}, val_acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['accuracy'].append(epoch_acc)\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['val_accuracy'].append(epoch_val_acc)\n",
    "\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved from {best_val_loss:.4f} to {epoch_val_loss:.4f}. Saving model...\")\n",
    "        best_val_loss = epoch_val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6609a1a",
   "metadata": {},
   "source": [
    "## 10. 훈련 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history[\"accuracy\"]\n",
    "val_accuracy = history[\"val_accuracy\"]\n",
    "loss = history[\"loss\"]\n",
    "val_loss = history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, accuracy, label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2bb70",
   "metadata": {},
   "source": [
    "## 11. 최고 성능 모델 로드 및 테스트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b22cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = ConvNet().to(device)\n",
    "test_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "test_model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        outputs = test_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"테스트 정확도: {test_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
