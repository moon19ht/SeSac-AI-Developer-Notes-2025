{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch를 이용한 Iris 분류 모델\n",
    "\n",
    "이 노트북은 PyTorch를 사용하여 Iris 데이터셋을 분류하는 신경망 모델을 구현합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- PyTorch 기본 구조 이해\n",
    "- 다층 퍼셉트론(MLP) 구현\n",
    "- 데이터 전처리 및 DataLoader 사용\n",
    "- 모델 학습 및 평가 과정 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 데이터셋 로딩\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(f\"특성 개수: {X.shape[1]}\")\n",
    "print(f\"샘플 개수: {X.shape[0]}\")\n",
    "print(f\"클래스 개수: {len(np.unique(y))}\")\n",
    "print(f\"\\n특성 이름: {iris.feature_names}\")\n",
    "print(f\"클래스 이름: {iris.target_names}\")\n",
    "\n",
    "# 클래스별 데이터 분포\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\n클래스별 데이터 분포:\")\n",
    "for i, (cls, count) in enumerate(zip(iris.target_names, counts)):\n",
    "    print(f\"  {cls}: {count}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 표준화\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"표준화 전:\")\n",
    "print(f\"평균: {np.round(X.mean(axis=0), 3)}\")\n",
    "print(f\"표준편차: {np.round(X.std(axis=0), 3)}\")\n",
    "\n",
    "print(\"\\n표준화 후:\")\n",
    "print(f\"평균: {np.round(X_scaled.mean(axis=0), 3)}\")\n",
    "print(f\"표준편차: {np.round(X_scaled.std(axis=0), 3)}\")\n",
    "\n",
    "# 특성별 상세 통계\n",
    "print(\"\\n특성별 상세 통계 (표준화 전):\")\n",
    "for i, feature_name in enumerate(iris.feature_names):\n",
    "    print(f\"  {feature_name}: 평균={X[:, i].mean():.2f}, 표준편차={X[:, i].std():.2f}\")\n",
    "    \n",
    "print(\"\\n특성별 상세 통계 (표준화 후):\")\n",
    "for i, feature_name in enumerate(iris.feature_names):\n",
    "    print(f\"  {feature_name}: 평균={X_scaled[:, i].mean():.2f}, 표준편차={X_scaled[:, i].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"학습 데이터 크기: {X_train.shape}\")\n",
    "print(f\"테스트 데이터 크기: {X_test.shape}\")\n",
    "\n",
    "# 분할 후 클래스 분포 확인\n",
    "print(f\"\\n학습 데이터 클래스 분포: {np.bincount(y_train)}\")\n",
    "print(f\"테스트 데이터 클래스 분포: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch 텐서 변환 및 DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(f\"학습 특성 텐서 크기: {X_train_tensor.shape}\")\n",
    "print(f\"학습 라벨 텐서 크기: {y_train_tensor.shape}\")\n",
    "print(f\"특성 데이터 타입: {X_train_tensor.dtype}\")\n",
    "print(f\"라벨 데이터 타입: {y_train_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset과 DataLoader 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"배치 크기: {batch_size}\")\n",
    "print(f\"학습 배치 개수: {len(train_loader)}\")\n",
    "print(f\"테스트 배치 개수: {len(test_loader)}\")\n",
    "\n",
    "# 첫 번째 배치 확인\n",
    "for batch_features, batch_labels in train_loader:\n",
    "    print(f\"\\n첫 번째 배치 특성 크기: {batch_features.shape}\")\n",
    "    print(f\"첫 번째 배치 라벨 크기: {batch_labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 신경망 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Iris 분류를 위한 다층 퍼셉트론 모델\n",
    "    \n",
    "    구조:\n",
    "    - 입력층: 4개 특성\n",
    "    - 은닉층1: 16개 뉴런 (ReLU 활성화)\n",
    "    - 은닉층2: 8개 뉴런 (ReLU 활성화)\n",
    "    - 출력층: 3개 클래스\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=4, hidden1_size=16, hidden2_size=8, num_classes=3):\n",
    "        super(IrisClassifier, self).__init__()\n",
    "        \n",
    "        # 레이어 정의\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)  # 입력 → 첫 번째 은닉층\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)  # 첫 번째 → 두 번째 은닉층\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)  # 두 번째 은닉층 → 출력층\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 드롭아웃 (과적합 방지)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수\n",
    "        \"\"\"\n",
    "        # 첫 번째 은닉층\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 두 번째 은닉층\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 출력층 (softmax는 CrossEntropyLoss에서 자동 적용)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = IrisClassifier()\n",
    "print(\"모델 구조:\")\n",
    "print(model)\n",
    "\n",
    "# 모델 파라미터 개수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n총 파라미터 개수: {total_params:,}\")\n",
    "print(f\"학습 가능한 파라미터 개수: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 손실함수와 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수: 다중 클래스 분류를 위한 CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저: Adam 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# 학습률 스케줄러 (선택사항)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "print(f\"손실함수: {criterion}\")\n",
    "print(f\"옵티마이저: {optimizer}\")\n",
    "print(f\"초기 학습률: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=100, print_every=10):\n",
    "    \"\"\"\n",
    "    모델 학습 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        train_loader: 학습 데이터 로더\n",
    "        criterion: 손실함수\n",
    "        optimizer: 옵티마이저\n",
    "        epochs: 학습 에포크 수\n",
    "        print_every: 출력 주기\n",
    "    \n",
    "    Returns:\n",
    "        loss_history: 손실값 히스토리\n",
    "    \"\"\"\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    loss_history = []\n",
    "    \n",
    "    print(\"학습 시작...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # 그래디언트 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 역전파\n",
    "            loss.backward()\n",
    "            \n",
    "            # 가중치 업데이트\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # 에포크 평균 손실\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        # 학습률 스케줄러 업데이트\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch [{epoch+1:3d}/{epochs}] | Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"학습 완료!\")\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# 모델 학습 실행\n",
    "epochs = 100\n",
    "loss_history = train_model(model, train_loader, criterion, optimizer, epochs, print_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 곡선 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 그래프\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, 'b-', linewidth=2)\n",
    "plt.title('학습 곡선 (Loss)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"최종 손실값: {loss_history[-1]:.4f}\")\n",
    "print(f\"최소 손실값: {min(loss_history):.4f} (Epoch {loss_history.index(min(loss_history)) + 1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, dataset_name=\"데이터셋\"):\n",
    "    \"\"\"\n",
    "    모델 평가 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 평가할 모델\n",
    "        data_loader: 평가 데이터 로더\n",
    "        dataset_name: 데이터셋 이름\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: 정확도\n",
    "        predictions: 예측값\n",
    "        true_labels: 실제값\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # 가장 높은 확률의 클래스 선택\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{dataset_name} 정확도: {accuracy:.2f}% ({correct}/{total})\")\n",
    "    \n",
    "    return accuracy, all_predictions, all_labels\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 평가\n",
    "print(\"=\" * 50)\n",
    "print(\"모델 평가 결과\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_acc, train_pred, train_true = evaluate_model(model, train_loader, \"학습\")\n",
    "test_acc, test_pred, test_true = evaluate_model(model, test_loader, \"테스트\")\n",
    "\n",
    "print(f\"\\n과적합 정도: {train_acc - test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 상세 분류 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 보고서\n",
    "print(\"테스트 데이터 분류 보고서:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(test_true, test_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동 행렬 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(test_true, test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names)\n",
    "plt.title('혼동 행렬 (Confusion Matrix)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('예측 클래스', fontsize=12)\n",
    "plt.ylabel('실제 클래스', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 클래스별 정확도 계산\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "print(\"\\n클래스별 정확도:\")\n",
    "for i, (cls_name, acc) in enumerate(zip(iris.target_names, class_accuracy)):\n",
    "    print(f\"  {cls_name}: {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 예측 확률 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터의 예측 확률 분석\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_probs = torch.softmax(test_outputs, dim=1)\n",
    "\n",
    "# 예측 확률 히스토그램\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, class_name in enumerate(iris.target_names):\n",
    "    class_probs = test_probs[:, i].numpy()\n",
    "    axes[i].hist(class_probs, bins=20, alpha=0.7, color=f'C{i}')\n",
    "    axes[i].set_title(f'{class_name} 클래스 예측 확률')\n",
    "    axes[i].set_xlabel('확률')\n",
    "    axes[i].set_ylabel('빈도')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 가장 확신 있는 예측과 가장 불확실한 예측\n",
    "max_probs = torch.max(test_probs, dim=1)[0]\n",
    "most_confident_idx = torch.argmax(max_probs)\n",
    "least_confident_idx = torch.argmin(max_probs)\n",
    "\n",
    "print(f\"가장 확신 있는 예측:\")\n",
    "print(f\"  샘플 인덱스: {most_confident_idx}\")\n",
    "print(f\"  예측 클래스: {iris.target_names[test_pred[most_confident_idx]]}\")\n",
    "print(f\"  확률: {max_probs[most_confident_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\n가장 불확실한 예측:\")\n",
    "print(f\"  샘플 인덱스: {least_confident_idx}\")\n",
    "print(f\"  예측 클래스: {iris.target_names[test_pred[least_confident_idx]]}\")\n",
    "print(f\"  확률: {max_probs[least_confident_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 새로운 데이터 예측 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(model, scaler, sample_features, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    새로운 샘플에 대한 예측 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 입력 데이터 전처리\n",
    "    sample_scaled = scaler.transform([sample_features])\n",
    "    sample_tensor = torch.tensor(sample_scaled, dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(sample_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    print(f\"입력 특성:\")\n",
    "    for name, value in zip(feature_names, sample_features):\n",
    "        print(f\"  {name}: {value}\")\n",
    "    \n",
    "    print(f\"\\n예측 결과: {class_names[predicted_class]}\")\n",
    "    print(f\"\\n클래스별 확률:\")\n",
    "    for i, (class_name, prob) in enumerate(zip(class_names, probabilities[0])):\n",
    "        print(f\"  {class_name}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "\n",
    "# 예시 샘플들로 테스트\n",
    "test_samples = [\n",
    "    [5.1, 3.5, 1.4, 0.2],  # setosa 같은 특성\n",
    "    [6.2, 2.9, 4.3, 1.3],  # versicolor 같은 특성\n",
    "    [7.3, 2.9, 6.3, 1.8]   # virginica 같은 특성\n",
    "]\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"테스트 샘플 {i}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    predict_sample(model, scaler, sample, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 모델 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# 저장 경로 설정 및 디렉토리 생성\n",
    "data_dir = os.path.join('..', '..', 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(data_dir, 'iris_classifier_model.pth')\n",
    "scaler_save_path = os.path.join(data_dir, 'iris_scaler.pkl')\n",
    "\n",
    "# PyTorch 모델 저장 (가중치만 저장)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"모델이 저장되었습니다: {model_save_path}\")\n",
    "\n",
    "# 스케일러 저장\n",
    "with open(scaler_save_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"스케일러가 저장되었습니다: {scaler_save_path}\")\n",
    "\n",
    "# 모델 불러오기 예시\n",
    "print(\"\\n모델 불러오기 테스트...\")\n",
    "new_model = IrisClassifier()\n",
    "new_model.load_state_dict(torch.load(model_save_path))\n",
    "new_model.eval()\n",
    "\n",
    "# 불러온 모델로 테스트\n",
    "with torch.no_grad():\n",
    "    test_output = new_model(X_test_tensor[:5])  # 처음 5개 샘플 테스트\n",
    "    _, predicted = torch.max(test_output, 1)\n",
    "    print(f\"불러온 모델 예측 결과: {predicted.numpy()}\")\n",
    "    print(f\"실제 라벨: {y_test[:5]}\")\n",
    "    print(\"모델이 성공적으로 저장/불러오기 되었습니다!\")\n",
    "\n",
    "# 저장된 파일 크기 확인\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "scaler_size = os.path.getsize(scaler_save_path)\n",
    "print(f\"\\n파일 크기:\")\n",
    "print(f\"  모델 파일: {model_size:,} bytes ({model_size/1024:.1f} KB)\")\n",
    "print(f\"  스케일러 파일: {scaler_size:,} bytes ({scaler_size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 실습 요약\n",
    "\n",
    "### 주요 학습 내용\n",
    "\n",
    "1. **PyTorch 기본 구조**\n",
    "   - `nn.Module`을 상속한 모델 클래스 정의\n",
    "   - `forward()` 메서드를 통한 순전파 구현\n",
    "   - 텐서 변환과 데이터 타입 관리\n",
    "\n",
    "2. **데이터 처리 파이프라인**\n",
    "   - StandardScaler를 이용한 특성 정규화\n",
    "   - `TensorDataset`과 `DataLoader`를 이용한 배치 처리\n",
    "   - 학습/테스트 데이터 분할\n",
    "\n",
    "3. **모델 구조**\n",
    "   - 다층 퍼셉트론 (MLP) 구조\n",
    "   - ReLU 활성화 함수\n",
    "   - Dropout을 이용한 과적합 방지\n",
    "\n",
    "4. **학습 과정**\n",
    "   - CrossEntropyLoss 손실함수\n",
    "   - Adam 옵티마이저\n",
    "   - 학습률 스케줄링\n",
    "\n",
    "5. **평가 및 분석**\n",
    "   - 정확도, 혼동행렬, 분류보고서\n",
    "   - 예측 확률 분석\n",
    "   - 모델 저장 및 불러오기\n",
    "\n",
    "### 성능 요약\n",
    "- 최종 테스트 정확도: {test_acc:.1f}%\n",
    "- 모델 복잡도: {total_params:,}개 파라미터\n",
    "- 학습 시간: {epochs} 에포크\n",
    "\n",
    "### 다음 단계\n",
    "- 더 복잡한 데이터셋으로 실습\n",
    "- CNN, RNN 등 다른 신경망 구조 학습\n",
    "- 하이퍼파라미터 튜닝 및 교차검증"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
