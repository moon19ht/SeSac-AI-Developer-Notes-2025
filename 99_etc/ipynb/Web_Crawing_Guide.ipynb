{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🕷️ 웹 크롤링 완전 정복 가이드\n",
        "\n",
        "## 📋 학습 목표\n",
        "이 노트북을 완료하면 다음을 할 수 있게 됩니다:\n",
        "- 웹 크롤링의 기본 원리와 동작 방식 이해\n",
        "- `requests` 모듈로 웹페이지 데이터 수집\n",
        "- `BeautifulSoup`로 HTML 문서 파싱 및 데이터 추출\n",
        "- CSS 선택자를 활용한 정확한 요소 선택\n",
        "- 실전 웹사이트에서 구조화된 데이터 추출\n",
        "- 크롤링 시 발생할 수 있는 문제 해결 방법\n",
        "\n",
        "## 📚 목차\n",
        "1. **환경 설정 및 패키지 확인**\n",
        "2. **웹 크롤링 기본 개념과 원리**\n",
        "3. **HTTP 통신과 requests 모듈 기초**\n",
        "4. **HTML 구조 이해하기**\n",
        "5. **BeautifulSoup 기초 파싱**\n",
        "6. **CSS 선택자와 고급 파싱 기법**\n",
        "7. **실전 예제: 복합 웹페이지 파싱**\n",
        "8. **에러 처리와 예외 상황 대응**\n",
        "9. **크롤링 윤리와 법적 주의사항**\n",
        "10. **종합 정리 및 실전 프로젝트 제안**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. 🔧 환경 설정 및 패키지 확인\n",
        "\n",
        "웹 크롤링을 위해 필요한 라이브러리들을 확인하고 설치해보겠습니다.\n",
        "\n",
        "### 필수 패키지\n",
        "- **requests**: HTTP 요청을 보내고 응답을 받는 라이브러리\n",
        "- **beautifulsoup4**: HTML 파싱 및 데이터 추출 라이브러리\n",
        "- **lxml**: BeautifulSoup의 빠른 파서 (선택사항)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ requests 버전: 2.32.4\n",
            "✅ beautifulsoup4 설치됨\n",
            "✅ lxml 설치됨\n",
            "✅ urllib.parse 사용 가능 (URL 조작용)\n",
            "✅ time 모듈 사용 가능 (요청 간격 조절용)\n",
            "\n",
            "🎯 모든 필수 패키지가 준비되었습니다!\n"
          ]
        }
      ],
      "source": [
        "# 🔍 패키지 설치 및 버전 확인\n",
        "\n",
        "# 필요한 패키지들을 import하여 설치 상태 확인\n",
        "try:\n",
        "    import requests\n",
        "    print(f\"✅ requests 버전: {requests.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"❌ requests가 설치되지 않았습니다. 'pip install requests'로 설치하세요.\")\n",
        "\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "    import bs4\n",
        "    print(f\"✅ beautifulsoup4 설치됨\")\n",
        "except ImportError:\n",
        "    print(\"❌ beautifulsoup4가 설치되지 않았습니다. 'pip install beautifulsoup4'로 설치하세요.\")\n",
        "\n",
        "try:\n",
        "    import lxml  # type: ignore\n",
        "    print(f\"✅ lxml 설치됨\")\n",
        "except ImportError:\n",
        "    print(\"⚠️  lxml이 설치되지 않았습니다 (선택사항). 설치하면 파싱 속도가 향상됩니다.\")\n",
        "\n",
        "# 추가 유용한 패키지들\n",
        "try:\n",
        "    import urllib.parse\n",
        "    print(\"✅ urllib.parse 사용 가능 (URL 조작용)\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import time\n",
        "    print(\"✅ time 모듈 사용 가능 (요청 간격 조절용)\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "print(\"\\n🎯 모든 필수 패키지가 준비되었습니다!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. 🌐 웹 크롤링 기본 개념과 원리\n",
        "\n",
        "### 웹 크롤링이란?\n",
        "웹 크롤링(Web Crawling) 또는 웹 스크래핑(Web Scraping)은 웹사이트에서 필요한 정보를 자동으로 수집하는 기술입니다.\n",
        "\n",
        "### 웹 크롤링의 동작 원리\n",
        "```markdown\n",
        "📱 클라이언트(Python) ----HTTP 요청----> 🖥️ 웹서버\n",
        "                      <---HTML 응답----\n",
        "                      \n",
        "1. 요청(Request): 특정 URL에 HTTP 요청을 보냄\n",
        "2. 응답(Response): 서버에서 HTML, JSON 등의 데이터로 응답\n",
        "3. 파싱(Parsing): 받은 데이터에서 필요한 정보만 추출\n",
        "4. 저장(Storage): 추출한 데이터를 파일이나 DB에 저장\n",
        "```\n",
        "\n",
        "### 크롤링 도구의 발전\n",
        "- **urllib** (초기) → **requests** (현재 표준)\n",
        "- **정규표현식** → **BeautifulSoup/lxml** (HTML 파싱)\n",
        "- **정적 페이지** → **Selenium** (동적 페이지/JavaScript)\n",
        "\n",
        "### 크롤링이 어려운 이유\n",
        "- 웹사이트마다 다른 구조와 방식\n",
        "- 지속적인 웹사이트 업데이트\n",
        "- 봇 차단 기술 (reCAPTCHA, 요청 제한 등)\n",
        "- 동적 콘텐츠 증가 (JavaScript 렌더링)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. 📡 HTTP 통신과 requests 모듈 기초\n",
        "\n",
        "### HTTP 상태 코드의 이해\n",
        "웹 서버는 요청에 대한 응답으로 상태 코드를 보냅니다:\n",
        "\n",
        "| 코드 | 의미 | 설명 |\n",
        "|------|------|------|\n",
        "| **2xx** | **성공** | 요청이 성공적으로 처리됨 |\n",
        "| 200 | OK | 정상적인 응답 |\n",
        "| **4xx** | **클라이언트 에러** | 요청에 문제가 있음 |\n",
        "| 404 | Not Found | 페이지를 찾을 수 없음 |\n",
        "| 403 | Forbidden | 접근 권한이 없음 |\n",
        "| **5xx** | **서버 에러** | 서버에서 처리 중 오류 발생 |\n",
        "| 500 | Internal Server Error | 서버 내부 오류 |\n",
        "\n",
        "### requests 모듈의 주요 기능\n",
        "- **간단한 API**: `requests.get(url)` 한 줄로 웹페이지 가져오기\n",
        "- **자동 인코딩**: 응답 데이터의 인코딩 자동 처리\n",
        "- **세션 관리**: 쿠키와 세션 자동 처리\n",
        "- **다양한 HTTP 메소드**: GET, POST, PUT, DELETE 등 지원\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "🧪 테스트 1: http://www.pythonscraping.com/exercises/exercise1.html\n",
            "============================================================\n",
            "🔄 요청 중: http://www.pythonscraping.com/exercises/exercise1.html\n",
            "📊 응답 상태 코드: 200\n",
            "📏 응답 크기: 564 문자\n",
            "🔤 인코딩: ISO-8859-1\n",
            "✅ 성공적으로 페이지를 가져왔습니다!\n",
            "\n",
            "📄 HTML 미리보기:\n",
            "<html>\n",
            "<head>\n",
            "<title>A Useful Page</title>\n",
            "</head>\n",
            "<body>\n",
            "<h1>An Interesting Title</h1>\n",
            "<div>\n",
            "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliqui...\n",
            "\n",
            "============================================================\n",
            "🧪 테스트 2: https://httpbin.org/status/404\n",
            "============================================================\n",
            "🔄 요청 중: https://httpbin.org/status/404\n",
            "📊 응답 상태 코드: 404\n",
            "📏 응답 크기: 0 문자\n",
            "🔤 인코딩: utf-8\n",
            "❌ 페이지를 찾을 수 없습니다 (404)\n",
            "❌ 실패: 페이지 없음\n",
            "\n",
            "============================================================\n",
            "🧪 테스트 3: https://httpbin.org/delay/2\n",
            "============================================================\n",
            "🔄 요청 중: https://httpbin.org/delay/2\n",
            "📊 응답 상태 코드: 200\n",
            "📏 응답 크기: 396 문자\n",
            "🔤 인코딩: utf-8\n",
            "✅ 성공적으로 페이지를 가져왔습니다!\n",
            "\n",
            "📄 HTML 미리보기:\n",
            "{\n",
            "  \"args\": {}, \n",
            "  \"data\": \"\", \n",
            "  \"files\": {}, \n",
            "  \"form\": {}, \n",
            "  \"headers\": {\n",
            "    \"Accept\": \"*/*\", \n",
            "    \"Accept-Encoding\": \"gzip, deflate\", \n",
            "    \"Host\": \"httpbin.org\", \n",
            "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\", \n",
            "    \"X-Amzn-Trace-Id\": \"Root=1-687dc0e5-6fba8c85...\n"
          ]
        }
      ],
      "source": [
        "# 🚀 첫 번째 웹 크롤링 실습\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def fetch_webpage(url):\n",
        "    \"\"\"\n",
        "    웹페이지를 가져오는 안전한 함수\n",
        "    \n",
        "    Args:\n",
        "        url (str): 가져올 웹페이지의 URL\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (성공여부, 응답객체 또는 에러메시지)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"🔄 요청 중: {url}\")\n",
        "        \n",
        "        # User-Agent 헤더 추가 (일부 사이트는 브라우저인 척 해야 함)\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        \n",
        "        # 요청 보내기 (타임아웃 10초)\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        \n",
        "        print(f\"📊 응답 상태 코드: {response.status_code}\")\n",
        "        print(f\"📏 응답 크기: {len(response.text):,} 문자\")\n",
        "        print(f\"🔤 인코딩: {response.encoding}\")\n",
        "        \n",
        "        # 상태 코드에 따른 처리\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ 성공적으로 페이지를 가져왔습니다!\")\n",
        "            return True, response\n",
        "        elif response.status_code == 404:\n",
        "            print(\"❌ 페이지를 찾을 수 없습니다 (404)\")\n",
        "            return False, \"페이지 없음\"\n",
        "        elif response.status_code == 403:\n",
        "            print(\"🚫 접근이 거부되었습니다 (403)\")\n",
        "            return False, \"접근 거부\"\n",
        "        else:\n",
        "            print(f\"⚠️  예상치 못한 상태 코드: {response.status_code}\")\n",
        "            return False, f\"상태 코드: {response.status_code}\"\n",
        "            \n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"⏰ 요청 시간이 초과되었습니다\")\n",
        "        return False, \"타임아웃\"\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"🌐 네트워크 연결 오류입니다\")\n",
        "        return False, \"연결 오류\"\n",
        "    except Exception as e:\n",
        "        print(f\"💥 알 수 없는 오류: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# 실습용 웹페이지들\n",
        "test_urls = [\n",
        "    \"http://www.pythonscraping.com/exercises/exercise1.html\",  # 성공 예제\n",
        "    \"https://httpbin.org/status/404\",  # 404 에러 예제\n",
        "    \"https://httpbin.org/delay/2\"  # 지연 응답 예제\n",
        "]\n",
        "\n",
        "for i, url in enumerate(test_urls, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🧪 테스트 {i}: {url}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    success, result = fetch_webpage(url)\n",
        "    \n",
        "    if success:\n",
        "        # 성공한 경우 HTML 일부 출력 (result는 response 객체)\n",
        "        html_text = result.text  # type: ignore\n",
        "        html_preview = html_text[:300] + \"...\" if len(html_text) > 300 else html_text\n",
        "        print(f\"\\n📄 HTML 미리보기:\\n{html_preview}\")\n",
        "    else:\n",
        "        print(f\"❌ 실패: {result}\")\n",
        "    \n",
        "    # 서버에 부담을 주지 않기 위해 잠시 대기\n",
        "    time.sleep(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. 📝 HTML 구조 이해하기\n",
        "\n",
        "### HTML의 기본 구조\n",
        "HTML(HyperText Markup Language)은 웹페이지의 구조를 정의하는 마크업 언어입니다.\n",
        "\n",
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>페이지 제목</title>\n",
        "    <meta charset=\"UTF-8\">\n",
        "</head>\n",
        "<body>\n",
        "    <h1 id=\"main-title\" class=\"header\">메인 제목</h1>\n",
        "    <div class=\"content\">\n",
        "        <p>본문 내용</p>\n",
        "        <ul>\n",
        "            <li>목록 항목 1</li>\n",
        "            <li>목록 항목 2</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "```\n",
        "\n",
        "### HTML 요소의 구성\n",
        "- **태그(Tag)**: `<태그명>내용</태그명>`\n",
        "- **속성(Attribute)**: `<태그 속성=\"값\">`\n",
        "- **ID**: 고유 식별자 `id=\"unique-name\"`\n",
        "- **Class**: 스타일링/그룹핑용 `class=\"group-name\"`\n",
        "\n",
        "### 크롤링 관점에서 중요한 HTML 요소들\n",
        "\n",
        "| 태그 | 용도 | 예시 |\n",
        "|------|------|------|\n",
        "| `<title>` | 페이지 제목 | `<title>뉴스 제목</title>` |\n",
        "| `<h1>~<h6>` | 제목/헤더 | `<h1>메인 제목</h1>` |\n",
        "| `<p>` | 문단 | `<p>본문 내용</p>` |\n",
        "| `<div>` | 구역 나누기 | `<div class=\"article\">...</div>` |\n",
        "| `<span>` | 인라인 요소 | `<span class=\"price\">1,000원</span>` |\n",
        "| `<table>` | 표 | `<table><tr><td>데이터</td></tr></table>` |\n",
        "| `<ul>`, `<li>` | 목록 | `<ul><li>항목</li></ul>` |\n",
        "| `<a>` | 링크 | `<a href=\"url\">링크텍스트</a>` |\n",
        "\n",
        "### CSS 선택자 기초\n",
        "크롤링에서 원하는 데이터를 정확히 찾기 위해 CSS 선택자를 이해해야 합니다:\n",
        "\n",
        "- **태그 선택자**: `h1`, `p`, `div`\n",
        "- **ID 선택자**: `#main-title`\n",
        "- **클래스 선택자**: `.content`, `.header`\n",
        "- **속성 선택자**: `[href]`, `[class=\"content\"]`\n",
        "- **조합 선택자**: `div.content p` (div 안의 p 태그)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. 🍲 BeautifulSoup 기초 파싱\n",
        "\n",
        "### BeautifulSoup란?\n",
        "BeautifulSoup는 HTML과 XML 문서를 파싱하여 구조화된 데이터로 변환해주는 Python 라이브러리입니다.\n",
        "\n",
        "### 주요 특징\n",
        "- **간단한 API**: 직관적이고 사용하기 쉬운 인터페이스\n",
        "- **관대한 파싱**: 잘못된 HTML도 잘 처리\n",
        "- **다양한 파서**: html.parser, lxml, html5lib 지원\n",
        "- **CSS 선택자**: CSS 선택자를 사용한 요소 선택\n",
        "\n",
        "### 기본 사용법\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# HTML 파싱\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# 요소 찾기\n",
        "soup.find('태그명')              # 첫 번째 요소\n",
        "soup.find_all('태그명')          # 모든 요소\n",
        "soup.select('CSS선택자')         # CSS 선택자로 찾기\n",
        "```\n",
        "\n",
        "### find() vs find_all() vs select()\n",
        "- **find()**: 조건에 맞는 **첫 번째 요소만** 반환\n",
        "- **find_all()**: 조건에 맞는 **모든 요소를 리스트로** 반환\n",
        "- **select()**: **CSS 선택자**를 사용하여 요소 찾기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 HTML 파싱 중...\n",
            "✅ 파싱 완료!\n",
            "\n",
            "============================================================\n",
            "📖 BeautifulSoup 기본 메소드 실습\n",
            "============================================================\n",
            "\n",
            "1️⃣ find() - 첫 번째 요소 찾기\n",
            "------------------------------\n",
            "페이지 제목: 온라인 서점 - 베스트셀러\n",
            "첫 번째 책 제목: 파이썬 완벽 가이드\n",
            "\n",
            "2️⃣ find_all() - 모든 요소 찾기\n",
            "------------------------------\n",
            "총 3권의 책이 있습니다:\n",
            "  1. 파이썬 완벽 가이드\n",
            "  2. 웹 크롤링 마스터\n",
            "  3. 데이터 분석 실전\n",
            "\n",
            "3️⃣ 속성으로 요소 찾기\n",
            "------------------------------\n",
            "메인 제목: 📚 온라인 서점\n",
            "data-id 속성이 있는 책: 3권\n",
            "\n",
            "4️⃣ 텍스트와 속성 값 추출\n",
            "------------------------------\n",
            "📗 도서 ID: 1\n",
            "   제목: 파이썬 완벽 가이드\n",
            "   저자: 김개발\n",
            "   가격: 25,000원\n",
            "   평점: ⭐⭐⭐⭐⭐ (4.8)\n",
            "\n",
            "📗 도서 ID: 2\n",
            "   제목: 웹 크롤링 마스터\n",
            "   저자: 이크롤링\n",
            "   가격: 30,000원\n",
            "   평점: ⭐⭐⭐⭐ (4.5)\n",
            "\n",
            "📗 도서 ID: 3\n",
            "   제목: 데이터 분석 실전\n",
            "   저자: 박데이터\n",
            "   가격: 28,000원\n",
            "   평점: ⭐⭐⭐⭐⭐ (4.9)\n",
            "\n",
            "🎯 BeautifulSoup 기초 실습 완료!\n"
          ]
        }
      ],
      "source": [
        "# 🥄 BeautifulSoup 기초 실습\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# 실습용 샘플 HTML 생성\n",
        "sample_html = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>온라인 서점 - 베스트셀러</title>\n",
        "    <meta charset=\"UTF-8\">\n",
        "</head>\n",
        "<body>\n",
        "    <header>\n",
        "        <h1 id=\"main-title\" class=\"site-title\">📚 온라인 서점</h1>\n",
        "        <nav class=\"navigation\">\n",
        "            <a href=\"/books\">도서</a>\n",
        "            <a href=\"/authors\">작가</a>\n",
        "        </nav>\n",
        "    </header>\n",
        "    \n",
        "    <main class=\"content\">\n",
        "        <section class=\"bestsellers\">\n",
        "            <h2 class=\"section-title\">이번 주 베스트셀러</h2>\n",
        "            \n",
        "            <div class=\"book\" data-id=\"1\">\n",
        "                <h3 class=\"book-title\">파이썬 완벽 가이드</h3>\n",
        "                <p class=\"author\">김개발</p>\n",
        "                <span class=\"price\">25,000원</span>\n",
        "                <div class=\"rating\">⭐⭐⭐⭐⭐ (4.8)</div>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"book\" data-id=\"2\">\n",
        "                <h3 class=\"book-title\">웹 크롤링 마스터</h3>\n",
        "                <p class=\"author\">이크롤링</p>\n",
        "                <span class=\"price\">30,000원</span>\n",
        "                <div class=\"rating\">⭐⭐⭐⭐ (4.5)</div>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"book\" data-id=\"3\">\n",
        "                <h3 class=\"book-title\">데이터 분석 실전</h3>\n",
        "                <p class=\"author\">박데이터</p>\n",
        "                <span class=\"price\">28,000원</span>\n",
        "                <div class=\"rating\">⭐⭐⭐⭐⭐ (4.9)</div>\n",
        "            </div>\n",
        "        </section>\n",
        "        \n",
        "        <aside class=\"sidebar\">\n",
        "            <h3>추천 카테고리</h3>\n",
        "            <ul class=\"categories\">\n",
        "                <li><a href=\"/category/programming\">프로그래밍</a></li>\n",
        "                <li><a href=\"/category/data-science\">데이터 사이언스</a></li>\n",
        "                <li><a href=\"/category/web\">웹 개발</a></li>\n",
        "            </ul>\n",
        "        </aside>\n",
        "    </main>\n",
        "    \n",
        "    <footer>\n",
        "        <p>&copy; 2024 온라인 서점. All rights reserved.</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# BeautifulSoup로 파싱\n",
        "print(\"🔄 HTML 파싱 중...\")\n",
        "soup = BeautifulSoup(sample_html, 'html.parser')\n",
        "print(\"✅ 파싱 완료!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📖 BeautifulSoup 기본 메소드 실습\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. find() - 첫 번째 요소만 찾기\n",
        "print(\"\\n1️⃣ find() - 첫 번째 요소 찾기\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "title = soup.find('title')\n",
        "if title:\n",
        "    print(f\"페이지 제목: {title.text}\")\n",
        "\n",
        "first_book_title = soup.find('h3', class_='book-title')\n",
        "if first_book_title:\n",
        "    print(f\"첫 번째 책 제목: {first_book_title.text}\")\n",
        "\n",
        "# 2. find_all() - 모든 요소 찾기\n",
        "print(\"\\n2️⃣ find_all() - 모든 요소 찾기\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "all_book_titles = soup.find_all('h3', class_='book-title')\n",
        "print(f\"총 {len(all_book_titles)}권의 책이 있습니다:\")\n",
        "for i, title in enumerate(all_book_titles, 1):\n",
        "    print(f\"  {i}. {title.text}\")\n",
        "\n",
        "# 3. 속성으로 찾기\n",
        "print(\"\\n3️⃣ 속성으로 요소 찾기\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# ID로 찾기\n",
        "main_title = soup.find('h1', id='main-title')\n",
        "if main_title:\n",
        "    print(f\"메인 제목: {main_title.text}\")\n",
        "\n",
        "# 여러 속성으로 찾기\n",
        "books_with_data_id = soup.find_all('div', {'class': 'book', 'data-id': True})\n",
        "print(f\"data-id 속성이 있는 책: {len(books_with_data_id)}권\")\n",
        "\n",
        "# 4. 텍스트 내용과 속성 값 가져오기\n",
        "print(\"\\n4️⃣ 텍스트와 속성 값 추출\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for book in soup.find_all('div', class_='book'):\n",
        "    title = book.find('h3', class_='book-title').text\n",
        "    author = book.find('p', class_='author').text\n",
        "    price = book.find('span', class_='price').text\n",
        "    rating = book.find('div', class_='rating').text\n",
        "    book_id = book.get('data-id')  # 속성 값 가져오기\n",
        "    \n",
        "    print(f\"📗 도서 ID: {book_id}\")\n",
        "    print(f\"   제목: {title}\")\n",
        "    print(f\"   저자: {author}\")\n",
        "    print(f\"   가격: {price}\")\n",
        "    print(f\"   평점: {rating}\")\n",
        "    print()\n",
        "\n",
        "print(\"🎯 BeautifulSoup 기초 실습 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. 🎯 CSS 선택자와 고급 파싱 기법\n",
        "\n",
        "### CSS 선택자의 강력함\n",
        "CSS 선택자를 사용하면 복잡한 HTML 구조에서도 정확한 요소를 찾을 수 있습니다.\n",
        "\n",
        "### BeautifulSoup의 select() 메소드\n",
        "`soup.select(CSS선택자)`를 사용하여 더 정교한 요소 선택이 가능합니다.\n",
        "\n",
        "### 주요 CSS 선택자 패턴\n",
        "\n",
        "| 선택자 | 의미 | 예시 |\n",
        "|--------|------|------|\n",
        "| `tag` | 태그 선택 | `soup.select('h1')` |\n",
        "| `.class` | 클래스 선택 | `soup.select('.book-title')` |\n",
        "| `#id` | ID 선택 | `soup.select('#main-title')` |\n",
        "| `[attribute]` | 속성 선택 | `soup.select('[data-id]')` |\n",
        "| `parent > child` | 직접 자식 | `soup.select('div > h3')` |\n",
        "| `ancestor descendant` | 후손 | `soup.select('section h3')` |\n",
        "| `:nth-child(n)` | n번째 자식 | `soup.select('li:nth-child(2)')` |\n",
        "\n",
        "### 고급 데이터 추출 기법\n",
        "- **텍스트 정제**: `strip()`, `replace()` 활용\n",
        "- **정규표현식**: 복잡한 패턴 추출\n",
        "- **상대 위치**: 형제/부모 요소 탐색\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 CSS 선택자 실습 시작!\n",
            "============================================================\n",
            "\n",
            "1️⃣ 기본 선택자 비교 (find vs select)\n",
            "----------------------------------------\n",
            "find() 결과: 파이썬 완벽 가이드\n",
            "select() 결과: 파이썬 완벽 가이드\n",
            "\n",
            "2️⃣ 다양한 CSS 선택자\n",
            "----------------------------------------\n",
            "📚 모든 책 제목 (3개):\n",
            "  1. 파이썬 완벽 가이드\n",
            "  2. 웹 크롤링 마스터\n",
            "  3. 데이터 분석 실전\n",
            "\n",
            "🏠 메인 제목: 📚 온라인 서점\n",
            "\n",
            "📊 data-id 속성이 있는 요소: 3개\n",
            "\n",
            "3️⃣ 조합 선택자\n",
            "----------------------------------------\n",
            "📖 section 안의 h3 태그: 3개\n",
            "📘 .book의 직접 자식 h3: 3개\n",
            "👥 h3 바로 다음의 p 태그: 3개\n",
            "\n",
            "4️⃣ 가상 선택자\n",
            "----------------------------------------\n",
            "🥉 마지막 책: 데이터 분석 실전\n",
            "🥈 두 번째 책: 파이썬 완벽 가이드\n",
            "\n",
            "5️⃣ 복합 조건 선택\n",
            "----------------------------------------\n",
            "📗 ID가 1인 책: 파이썬 완벽 가이드\n",
            "\n",
            "6️⃣ 고급 데이터 추출\n",
            "----------------------------------------\n",
            "📚 추출된 책 정보:\n",
            "  📖 파이썬 완벽 가이드\n",
            "     저자: 김개발\n",
            "     가격: 25,000원\n",
            "     평점: 4.8 (5⭐)\n",
            "\n",
            "  📖 웹 크롤링 마스터\n",
            "     저자: 이크롤링\n",
            "     가격: 30,000원\n",
            "     평점: 4.5 (4⭐)\n",
            "\n",
            "  📖 데이터 분석 실전\n",
            "     저자: 박데이터\n",
            "     가격: 28,000원\n",
            "     평점: 4.9 (5⭐)\n",
            "\n",
            "7️⃣ 간단한 데이터 분석\n",
            "----------------------------------------\n",
            "📊 총 도서 수: 3권\n",
            "💰 평균 가격: 27,667원\n",
            "⭐ 평균 평점: 4.7\n",
            "🏆 최고 평점 도서: 데이터 분석 실전 (4.9점)\n",
            "\n",
            "🎯 CSS 선택자 마스터 클래스 완료!\n"
          ]
        }
      ],
      "source": [
        "# 🎯 CSS 선택자 마스터 클래스\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 앞에서 사용한 온라인 서점 HTML을 재사용\n",
        "# (soup 변수가 이미 정의되어 있다고 가정)\n",
        "\n",
        "print(\"🔍 CSS 선택자 실습 시작!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. 기본 선택자 비교\n",
        "print(\"\\n1️⃣ 기본 선택자 비교 (find vs select)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# find() 방식\n",
        "find_title = soup.find('h3', class_='book-title')\n",
        "print(f\"find() 결과: {find_title.text if find_title else 'None'}\")\n",
        "\n",
        "# select() 방식 - CSS 선택자\n",
        "select_title = soup.select('.book-title')[0]  # 첫 번째 요소\n",
        "print(f\"select() 결과: {select_title.text}\")\n",
        "\n",
        "# 2. 다양한 CSS 선택자 실습\n",
        "print(\"\\n2️⃣ 다양한 CSS 선택자\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 클래스 선택자\n",
        "book_titles = soup.select('.book-title')\n",
        "print(f\"📚 모든 책 제목 ({len(book_titles)}개):\")\n",
        "for i, title in enumerate(book_titles, 1):\n",
        "    print(f\"  {i}. {title.text}\")\n",
        "\n",
        "# ID 선택자\n",
        "main_title = soup.select('#main-title')\n",
        "if main_title:\n",
        "    print(f\"\\n🏠 메인 제목: {main_title[0].text}\")\n",
        "\n",
        "# 속성 선택자\n",
        "books_with_data_id = soup.select('[data-id]')\n",
        "print(f\"\\n📊 data-id 속성이 있는 요소: {len(books_with_data_id)}개\")\n",
        "\n",
        "# 3. 조합 선택자 (Combinator)\n",
        "print(\"\\n3️⃣ 조합 선택자\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 후손 선택자 (공백)\n",
        "section_titles = soup.select('section h3')\n",
        "print(f\"📖 section 안의 h3 태그: {len(section_titles)}개\")\n",
        "\n",
        "# 직접 자식 선택자 (>)\n",
        "direct_children = soup.select('.book > h3')\n",
        "print(f\"📘 .book의 직접 자식 h3: {len(direct_children)}개\")\n",
        "\n",
        "# 인접 형제 선택자 (+) - 바로 다음 형제\n",
        "next_siblings = soup.select('h3 + p')  # h3 바로 다음의 p 태그\n",
        "print(f\"👥 h3 바로 다음의 p 태그: {len(next_siblings)}개\")\n",
        "\n",
        "# 4. 가상 선택자 (Pseudo-selector)\n",
        "print(\"\\n4️⃣ 가상 선택자\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# n번째 자식\n",
        "first_book = soup.select('.book:first-child')\n",
        "if first_book:\n",
        "    title = first_book[0].select('.book-title')[0].text\n",
        "    print(f\"🥇 첫 번째 책: {title}\")\n",
        "\n",
        "last_book = soup.select('.book:last-child')\n",
        "if last_book:\n",
        "    title = last_book[0].select('.book-title')[0].text\n",
        "    print(f\"🥉 마지막 책: {title}\")\n",
        "\n",
        "# n번째 특정 자식\n",
        "second_book = soup.select('.book:nth-child(2)')\n",
        "if second_book:\n",
        "    title = second_book[0].select('.book-title')[0].text\n",
        "    print(f\"🥈 두 번째 책: {title}\")\n",
        "\n",
        "# 5. 복합 조건 선택\n",
        "print(\"\\n5️⃣ 복합 조건 선택\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 여러 클래스 조건\n",
        "# 만약 'book premium' 클래스가 있다면\n",
        "# premium_books = soup.select('.book.premium')\n",
        "\n",
        "# 속성 값으로 선택\n",
        "book_1 = soup.select('[data-id=\"1\"]')\n",
        "if book_1:\n",
        "    title = book_1[0].select('.book-title')[0].text\n",
        "    print(f\"📗 ID가 1인 책: {title}\")\n",
        "\n",
        "# 6. 텍스트 기반 고급 추출\n",
        "print(\"\\n6️⃣ 고급 데이터 추출\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "def extract_book_info(book_element):\n",
        "    \"\"\"책 정보를 구조화하여 추출하는 함수\"\"\"\n",
        "    info = {}\n",
        "    \n",
        "    # 기본 정보 추출\n",
        "    info['id'] = book_element.get('data-id')\n",
        "    info['title'] = book_element.select('.book-title')[0].text.strip()\n",
        "    info['author'] = book_element.select('.author')[0].text.strip()\n",
        "    \n",
        "    # 가격에서 숫자만 추출 (정규표현식 사용)\n",
        "    price_text = book_element.select('.price')[0].text\n",
        "    price_match = re.search(r'[\\d,]+', price_text)\n",
        "    info['price'] = int(price_match.group().replace(',', '')) if price_match else 0\n",
        "    \n",
        "    # 평점에서 숫자 추출\n",
        "    rating_text = book_element.select('.rating')[0].text\n",
        "    rating_match = re.search(r'\\((\\d+\\.\\d+)\\)', rating_text)\n",
        "    info['rating'] = float(rating_match.group(1)) if rating_match else 0.0\n",
        "    \n",
        "    # 별 개수 계산\n",
        "    star_count = rating_text.count('⭐')\n",
        "    info['stars'] = star_count\n",
        "    \n",
        "    return info\n",
        "\n",
        "# 모든 책 정보 추출\n",
        "books_data = []\n",
        "for book in soup.select('.book'):\n",
        "    book_info = extract_book_info(book)\n",
        "    books_data.append(book_info)\n",
        "\n",
        "# 추출된 데이터 출력\n",
        "print(\"📚 추출된 책 정보:\")\n",
        "for book in books_data:\n",
        "    print(f\"  📖 {book['title']}\")\n",
        "    print(f\"     저자: {book['author']}\")\n",
        "    print(f\"     가격: {book['price']:,}원\")\n",
        "    print(f\"     평점: {book['rating']} ({book['stars']}⭐)\")\n",
        "    print()\n",
        "\n",
        "# 7. 데이터 분석\n",
        "print(\"7️⃣ 간단한 데이터 분석\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "total_books = len(books_data)\n",
        "avg_price = sum(book['price'] for book in books_data) / total_books\n",
        "avg_rating = sum(book['rating'] for book in books_data) / total_books\n",
        "highest_rated = max(books_data, key=lambda x: x['rating'])\n",
        "\n",
        "print(f\"📊 총 도서 수: {total_books}권\")\n",
        "print(f\"💰 평균 가격: {avg_price:,.0f}원\")\n",
        "print(f\"⭐ 평균 평점: {avg_rating:.1f}\")\n",
        "print(f\"🏆 최고 평점 도서: {highest_rated['title']} ({highest_rated['rating']}점)\")\n",
        "\n",
        "print(\"\\n🎯 CSS 선택자 마스터 클래스 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 실제 웹사이트 크롤링 전략\n",
        "\n",
        "이제 실제 웹사이트에서 데이터를 추출해보겠습니다. 안전하고 윤리적인 크롤링을 위한 원칙들을 적용합니다.\n",
        "\n",
        "### 크롤링 전략 수립\n",
        "1. **robots.txt 확인**: `사이트주소/robots.txt`\n",
        "2. **요청 간격 조절**: 서버 부하 방지\n",
        "3. **User-Agent 설정**: 적절한 브라우저 헤더\n",
        "4. **에러 처리**: 네트워크 오류, 파싱 오류 대응\n",
        "5. **데이터 검증**: 추출한 데이터의 유효성 확인\n",
        "\n",
        "### 실습 대상\n",
        "- **HTTPBin**: 테스트용 HTTP 서비스\n",
        "- **Python Scraping 예제 사이트**: 학습용 사이트\n",
        "- **공개 API**: JSON 데이터 처리 연습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 실전 크롤링 프로젝트 시작!\n",
            "============================================================\n",
            "\n",
            "1️⃣ 기본 HTML 페이지 크롤링\n",
            "----------------------------------------\n",
            "🔄 요청: http://www.pythonscraping.com/exercises/exercise1.html\n",
            "✅ 성공 (크기: 564 문자)\n",
            "📄 페이지 제목: A Useful Page\n",
            "🔤 메인 헤딩: An Interesting Title\n",
            "\n",
            "2️⃣ JSON API 데이터 처리\n",
            "----------------------------------------\n",
            "🔄 요청: https://httpbin.org/json\n",
            "✅ 성공 (크기: 429 문자)\n",
            "📊 JSON 데이터:\n",
            "{\n",
            "  \"slideshow\": {\n",
            "    \"author\": \"Yours Truly\",\n",
            "    \"date\": \"date of publication\",\n",
            "    \"slides\": [\n",
            "      {\n",
            "        \"title\": \"Wake up to WonderWidgets!\",\n",
            "        \"type\": \"all\"\n",
            "      },\n",
            "      {\n",
            "        \"items\": [\n",
            "          \"Why <em>WonderWidgets</em> are great\",\n",
            "          \"Who <em>buys</em> WonderWidgets\"\n",
            "        ],\n",
            "        \"title\": \"Overview\",\n",
            "        \"type\": \"all\"\n",
            "      }\n",
            "    ],\n",
            "    \"title\": \"Sample Slide Show\"\n",
            "  }\n",
            "}\n",
            "\n",
            "3️⃣ 복잡한 HTML 구조 분석\n",
            "----------------------------------------\n",
            "🔄 요청: http://www.pythonscraping.com/pages/warandpeace.html\n",
            "✅ 성공 (크기: 11,723 문자)\n",
            "🟢 녹색 텍스트 (41개):\n",
            "  1. Anna\n",
            "Pavlovna Scherer\n",
            "  2. Empress Marya\n",
            "Fedorovna\n",
            "  3. Prince Vasili Kuragin\n",
            "  4. Anna Pavlovna\n",
            "  5. St. Petersburg\n",
            "  ... 및 36개 더\n",
            "\n",
            "4️⃣ 페이지 내 링크 수집\n",
            "----------------------------------------\n",
            "🔄 요청: http://www.pythonscraping.com/pages/page3.html\n",
            "✅ 성공 (크기: 2,405 문자)\n",
            "🔗 내부 링크 (0개):\n",
            "\n",
            "🌐 외부 링크 (0개):\n",
            "\n",
            "5️⃣ 테이블 데이터 구조화\n",
            "----------------------------------------\n",
            "📊 테이블 데이터 (6행):\n",
            "  1. Item Title | Description | Cost | Image\n",
            "  2. Vegetable Basket | This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
            "Now with super-colorful bell peppers! | $15.00 | \n",
            "  3. Russian Nesting Dolls | Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! 8 entire dolls per set! Octuple the presents! | $10,000.52 | \n",
            "  4. Fish Painting | If something seems fishy about this painting, it's because it's a fish! Also hand-painted by trained monkeys! | $10,005.00 | \n",
            "  5. Dead Parrot | This is an ex-parrot! Or maybe he's only resting? | $0.50 | \n",
            "  ... 및 1행 더\n",
            "\n",
            "6️⃣ 에러 처리 테스트\n",
            "----------------------------------------\n",
            "🔄 요청: http://www.pythonscraping.com/nonexistent-page\n",
            "❌ HTTP 404 오류\n",
            "🔄 요청: http://this-domain-does-not-exist-12345.com\n",
            "💥 요청 오류: HTTPConnectionPool(host='this-domain-does-not-exist-12345.com', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDFC930CD0>: Failed to resolve 'this-domain-does-not-exist-12345.com' ([Errno 11001] getaddrinfo failed)\"))\n",
            "\n",
            "🎯 실전 크롤링 프로젝트 완료!\n",
            "\n",
            "💡 핵심 포인트:\n",
            "  - 요청 간격 조절로 서버 부하 방지\n",
            "  - 다양한 에러 상황에 대한 적절한 처리\n",
            "  - 구조화된 데이터 추출 및 정제\n",
            "  - JSON과 HTML 두 가지 형태의 데이터 처리\n"
          ]
        }
      ],
      "source": [
        "# 🚀 실전 크롤링 프로젝트\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "class WebCrawler:\n",
        "    \"\"\"안전하고 효율적인 웹 크롤러 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self, delay=1):\n",
        "        self.session = requests.Session()\n",
        "        self.delay = delay  # 요청 간격 (초)\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "    \n",
        "    def fetch_page(self, url, timeout=10):\n",
        "        \"\"\"안전하게 웹페이지를 가져오는 메소드\"\"\"\n",
        "        try:\n",
        "            print(f\"🔄 요청: {url}\")\n",
        "            response = self.session.get(url, timeout=timeout)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                print(f\"✅ 성공 (크기: {len(response.text):,} 문자)\")\n",
        "                time.sleep(self.delay)  # 요청 간격 조절\n",
        "                return response\n",
        "            else:\n",
        "                print(f\"❌ HTTP {response.status_code} 오류\")\n",
        "                return None\n",
        "                \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"💥 요청 오류: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def parse_html(self, html_content):\n",
        "        \"\"\"HTML을 파싱하여 BeautifulSoup 객체 반환\"\"\"\n",
        "        return BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# 크롤러 인스턴스 생성\n",
        "crawler = WebCrawler(delay=2)  # 2초 간격\n",
        "\n",
        "print(\"🎯 실전 크롤링 프로젝트 시작!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. 간단한 HTML 페이지 크롤링\n",
        "print(\"\\n1️⃣ 기본 HTML 페이지 크롤링\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"http://www.pythonscraping.com/exercises/exercise1.html\")\n",
        "if response:\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    title = soup.find('title')\n",
        "    h1 = soup.find('h1')\n",
        "    \n",
        "    print(f\"📄 페이지 제목: {title.text if title else '없음'}\")\n",
        "    print(f\"🔤 메인 헤딩: {h1.text if h1 else '없음'}\")\n",
        "\n",
        "# 2. JSON API 데이터 크롤링\n",
        "print(\"\\n2️⃣ JSON API 데이터 처리\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"https://httpbin.org/json\")\n",
        "if response:\n",
        "    try:\n",
        "        data = response.json()  # JSON 파싱\n",
        "        print(\"📊 JSON 데이터:\")\n",
        "        print(json.dumps(data, indent=2, ensure_ascii=False))\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"❌ JSON 파싱 실패\")\n",
        "\n",
        "# 3. 복잡한 구조의 페이지 크롤링\n",
        "print(\"\\n3️⃣ 복잡한 HTML 구조 분석\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"http://www.pythonscraping.com/pages/warandpeace.html\")\n",
        "if response:\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    \n",
        "    # 여러 데이터 추출\n",
        "    name_list = soup.find_all(\"span\", {\"class\": \"green\"})\n",
        "    print(f\"🟢 녹색 텍스트 ({len(name_list)}개):\")\n",
        "    for i, name in enumerate(name_list[:5], 1):  # 처음 5개만\n",
        "        print(f\"  {i}. {name.text.strip()}\")\n",
        "    \n",
        "    if len(name_list) > 5:\n",
        "        print(f\"  ... 및 {len(name_list) - 5}개 더\")\n",
        "\n",
        "# 4. 링크 수집하기\n",
        "print(\"\\n4️⃣ 페이지 내 링크 수집\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"http://www.pythonscraping.com/pages/page3.html\")\n",
        "if response:\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    \n",
        "    # 모든 링크 수집\n",
        "    links = soup.find_all('a', href=True)\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    \n",
        "    base_url = \"http://www.pythonscraping.com\"\n",
        "    \n",
        "    for link in links:\n",
        "        href = link['href']\n",
        "        full_url = urljoin(base_url, href)\n",
        "        link_text = link.text.strip()\n",
        "        \n",
        "        if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
        "            internal_links.append((link_text, full_url))\n",
        "        else:\n",
        "            external_links.append((link_text, full_url))\n",
        "    \n",
        "    print(f\"🔗 내부 링크 ({len(internal_links)}개):\")\n",
        "    for text, url in internal_links[:3]:\n",
        "        print(f\"  📄 {text}: {url}\")\n",
        "    \n",
        "    print(f\"\\n🌐 외부 링크 ({len(external_links)}개):\")\n",
        "    for text, url in external_links[:3]:\n",
        "        print(f\"  🔗 {text}: {url}\")\n",
        "\n",
        "# 5. 표(Table) 데이터 추출\n",
        "print(\"\\n5️⃣ 테이블 데이터 구조화\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 테이블이 있는 페이지에서 데이터 추출\n",
        "if response:  # 이전 response 재사용\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    \n",
        "    # 테이블 찾기\n",
        "    try:\n",
        "        table = soup.find('table')\n",
        "        if table:\n",
        "            rows = table.find_all('tr')  # type: ignore\n",
        "            table_data = []\n",
        "            \n",
        "            for row in rows:\n",
        "                cells = row.find_all(['th', 'td'])  # type: ignore\n",
        "                row_data = [cell.text.strip() for cell in cells]\n",
        "                if row_data:  # 빈 행이 아닌 경우만\n",
        "                    table_data.append(row_data)\n",
        "            \n",
        "            print(f\"📊 테이블 데이터 ({len(table_data)}행):\")\n",
        "            for i, row in enumerate(table_data[:5], 1):  # 처음 5행만\n",
        "                print(f\"  {i}. {' | '.join(row)}\")\n",
        "            \n",
        "            if len(table_data) > 5:\n",
        "                print(f\"  ... 및 {len(table_data) - 5}행 더\")\n",
        "        else:\n",
        "            print(\"📊 테이블이 없습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"📊 테이블 처리 중 오류: {e}\")\n",
        "\n",
        "# 6. 에러 처리 테스트\n",
        "print(\"\\n6️⃣ 에러 처리 테스트\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 존재하지 않는 페이지 요청\n",
        "error_response = crawler.fetch_page(\"http://www.pythonscraping.com/nonexistent-page\")\n",
        "\n",
        "# 잘못된 도메인 요청\n",
        "error_response2 = crawler.fetch_page(\"http://this-domain-does-not-exist-12345.com\")\n",
        "\n",
        "print(\"\\n🎯 실전 크롤링 프로젝트 완료!\")\n",
        "print(\"\\n💡 핵심 포인트:\")\n",
        "print(\"  - 요청 간격 조절로 서버 부하 방지\")\n",
        "print(\"  - 다양한 에러 상황에 대한 적절한 처리\")\n",
        "print(\"  - 구조화된 데이터 추출 및 정제\")\n",
        "print(\"  - JSON과 HTML 두 가지 형태의 데이터 처리\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. ⚠️ 에러 처리와 예외 상황 대응\n",
        "\n",
        "### 웹 크롤링에서 발생할 수 있는 문제들\n",
        "\n",
        "웹 크롤링은 다양한 예상치 못한 상황들이 발생할 수 있는 작업입니다. 안정적인 크롤러를 만들기 위해서는 적절한 에러 처리가 필수입니다.\n",
        "\n",
        "#### 주요 예외 상황들:\n",
        "- **네트워크 오류**: 연결 실패, 타임아웃\n",
        "- **HTTP 오류**: 404, 403, 500 등\n",
        "- **파싱 오류**: 예상과 다른 HTML 구조\n",
        "- **인코딩 문제**: 문자 깨짐\n",
        "- **동적 콘텐츠**: JavaScript로 생성되는 내용\n",
        "- **봇 차단**: CAPTCHA, IP 차단\n",
        "\n",
        "#### 방어적 프로그래밍 원칙:\n",
        "1. **항상 예외 처리**: try-except 블록 사용\n",
        "2. **데이터 검증**: 예상한 형태의 데이터인지 확인\n",
        "3. **안전한 접근**: None 체크, 배열 인덱스 검증\n",
        "4. **재시도 로직**: 일시적 오류에 대한 재시도\n",
        "5. **로깅**: 문제 상황 기록 및 디버깅\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. ⚖️ 크롤링 윤리와 법적 주의사항\n",
        "\n",
        "### 🚨 반드시 지켜야 할 윤리적 원칙\n",
        "\n",
        "웹 크롤링은 강력한 도구이지만, 책임감 있게 사용해야 합니다.\n",
        "\n",
        "#### 법적 준수사항:\n",
        "- **저작권 보호**: 콘텐츠의 저작권 존중\n",
        "- **개인정보 보호**: GDPR, 개인정보보호법 준수\n",
        "- **이용약관 확인**: 웹사이트의 Terms of Service 검토\n",
        "- **robots.txt 준수**: 사이트의 크롤링 정책 확인\n",
        "\n",
        "#### 기술적 윤리:\n",
        "- **서버 부하 최소화**: 적절한 요청 간격 유지\n",
        "- **대역폭 고려**: 불필요한 대용량 파일 다운로드 금지\n",
        "- **User-Agent 명시**: 본인의 정체를 숨기지 않기\n",
        "- **재시도 제한**: 무한 재시도로 서버 공격하지 않기\n",
        "\n",
        "#### robots.txt 확인 방법\n",
        "```python\n",
        "# robots.txt 확인 예제\n",
        "import requests\n",
        "\n",
        "def check_robots_txt(domain):\n",
        "    robots_url = f\"{domain}/robots.txt\"\n",
        "    try:\n",
        "        response = requests.get(robots_url)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"🤖 {domain}의 robots.txt:\")\n",
        "            print(response.text)\n",
        "        else:\n",
        "            print(f\"❌ robots.txt를 찾을 수 없습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"💥 오류: {e}\")\n",
        "\n",
        "# 예시: check_robots_txt(\"http://www.example.com\")\n",
        "```\n",
        "\n",
        "### 💡 책임감 있는 크롤링을 위한 체크리스트\n",
        "\n",
        "✅ **법적 검토**\n",
        "- [ ] 대상 사이트의 이용약관 확인\n",
        "- [ ] robots.txt 정책 준수\n",
        "- [ ] 개인정보 포함 여부 확인\n",
        "- [ ] 저작권 침해 가능성 검토\n",
        "\n",
        "✅ **기술적 고려**\n",
        "- [ ] 적절한 요청 간격 설정 (최소 1초)\n",
        "- [ ] User-Agent 헤더 명시\n",
        "- [ ] 에러 처리 및 재시도 로직 구현\n",
        "- [ ] 서버 응답 코드 확인\n",
        "\n",
        "✅ **데이터 사용**\n",
        "- [ ] 수집 목적에 맞는 최소한의 데이터만 추출\n",
        "- [ ] 개인정보 처리 시 적절한 보안 조치\n",
        "- [ ] 데이터 보관 기간 및 파기 정책 수립\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. 🎓 종합 정리 및 실전 프로젝트 제안\n",
        "\n",
        "### 🏆 학습 완료! 축하합니다!\n",
        "\n",
        "이 노트북을 통해 웹 크롤링의 전체 과정을 체계적으로 학습했습니다.\n",
        "\n",
        "### 📚 학습한 핵심 내용\n",
        "\n",
        "#### 1. 기초 개념\n",
        "- ✅ HTTP 통신과 웹 크롤링의 원리\n",
        "- ✅ requests 모듈을 이용한 웹페이지 요청\n",
        "- ✅ 상태 코드와 에러 처리\n",
        "\n",
        "#### 2. HTML 파싱\n",
        "- ✅ BeautifulSoup 라이브러리 사용법\n",
        "- ✅ find(), find_all(), select() 메소드\n",
        "- ✅ CSS 선택자를 이용한 정교한 요소 선택\n",
        "\n",
        "#### 3. 고급 기법\n",
        "- ✅ 정규표현식을 이용한 데이터 정제\n",
        "- ✅ 구조화된 데이터 추출 및 분석\n",
        "- ✅ 세션 관리와 헤더 설정\n",
        "\n",
        "#### 4. 실전 응용\n",
        "- ✅ 안전한 크롤링 클래스 구현\n",
        "- ✅ JSON과 HTML 데이터 처리\n",
        "- ✅ 링크 수집 및 페이지 탐색\n",
        "\n",
        "#### 5. 윤리와 법적 고려\n",
        "- ✅ robots.txt 확인 방법\n",
        "- ✅ 책임감 있는 크롤링 원칙\n",
        "- ✅ 개인정보 보호와 저작권 준수\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 다음 단계: 실전 프로젝트 제안\n",
        "\n",
        "이제 배운 지식을 활용해 실제 프로젝트를 시도해보세요!\n",
        "\n",
        "#### 🌟 초급 프로젝트\n",
        "1. **날씨 정보 수집기**\n",
        "   - 기상청 또는 날씨 사이트에서 오늘의 날씨 정보 추출\n",
        "   - 온도, 습도, 강수량 등을 정리해서 텍스트 파일로 저장\n",
        "\n",
        "2. **뉴스 제목 모음집**\n",
        "   - 뉴스 사이트에서 오늘의 주요 뉴스 제목들 수집\n",
        "   - 카테고리별로 분류하여 정리\n",
        "\n",
        "3. **온라인 쇼핑몰 가격 비교**\n",
        "   - 특정 상품의 가격을 여러 쇼핑몰에서 수집\n",
        "   - 최저가와 평균가 계산\n",
        "\n",
        "#### 🔥 중급 프로젝트\n",
        "4. **부동산 시세 분석기**\n",
        "   - 부동산 정보 사이트에서 지역별 매매가 정보 수집\n",
        "   - 시계열 데이터로 변환하여 트렌드 분석\n",
        "\n",
        "5. **채용 정보 모니터링**\n",
        "   - 채용 사이트에서 특정 직무의 공고 정보 수집\n",
        "   - 요구 스킬, 연봉 범위 등을 분석하여 인사이트 도출\n",
        "\n",
        "6. **소셜 미디어 트렌드 분석**\n",
        "   - 공개 API를 활용한 트렌드 키워드 수집\n",
        "   - 시간대별, 지역별 인기 토픽 분석\n",
        "\n",
        "#### 🚀 고급 프로젝트\n",
        "7. **학술 논문 메타데이터 수집**\n",
        "   - arXiv, Google Scholar 등에서 특정 분야 논문 정보 수집\n",
        "   - 저자 네트워크, 인용 관계 분석\n",
        "\n",
        "8. **전자상거래 리뷰 분석**\n",
        "   - 제품 리뷰 데이터 대량 수집\n",
        "   - 감정 분석과 키워드 추출을 통한 제품 평가\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ 추가 학습 로드맵\n",
        "\n",
        "#### 다음에 배울 고급 기술들:\n",
        "\n",
        "1. **Selenium WebDriver**\n",
        "   - JavaScript 렌더링이 필요한 동적 웹페이지 크롤링\n",
        "   - 브라우저 자동화를 통한 복잡한 상호작용\n",
        "\n",
        "2. **Scrapy Framework**\n",
        "   - 대규모 크롤링을 위한 전문 프레임워크\n",
        "   - 분산 크롤링과 데이터 파이프라인\n",
        "\n",
        "3. **데이터 저장 및 처리**\n",
        "   - 데이터베이스 연동 (MySQL, MongoDB)\n",
        "   - 데이터 분석 라이브러리 (pandas, numpy)\n",
        "\n",
        "4. **성능 최적화**\n",
        "   - 비동기 프로그래밍 (asyncio, aiohttp)\n",
        "   - 멀티스레딩과 멀티프로세싱\n",
        "\n",
        "5. **클라우드 배포**\n",
        "   - AWS, GCP를 이용한 크롤러 배포\n",
        "   - 스케줄링과 모니터링\n",
        "\n",
        "---\n",
        "\n",
        "### 💪 마무리 메시지\n",
        "\n",
        "웹 크롤링은 데이터 과학, 비즈니스 인텔리전스, 마케팅 등 다양한 분야에서 활용되는 실용적인 기술입니다. \n",
        "\n",
        "**기억하세요:**\n",
        "- 항상 윤리적이고 법적인 기준을 준수하세요\n",
        "- 작은 프로젝트부터 시작해서 점진적으로 복잡한 것에 도전하세요\n",
        "- 실패를 두려워하지 말고, 문제 해결 과정에서 배우세요\n",
        "- 커뮤니티와 함께 지식을 공유하고 성장하세요\n",
        "\n",
        "**🎯 여러분의 첫 번째 크롤링 프로젝트를 시작해보세요!**\n",
        "\n",
        "---\n",
        "\n",
        "*\"데이터는 21세기의 석유다. 하지만 석유와 달리 데이터는 사용할수록 더 가치있어진다.\"*\n",
        "\n",
        "**Happy Crawling! 🕷️✨**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
