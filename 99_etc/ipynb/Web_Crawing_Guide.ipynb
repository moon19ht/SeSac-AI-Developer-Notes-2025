{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ•·ï¸ ì›¹ í¬ë¡¤ë§ ì™„ì „ ì •ë³µ ê°€ì´ë“œ\n",
        "\n",
        "## ğŸ“‹ í•™ìŠµ ëª©í‘œ\n",
        "ì´ ë…¸íŠ¸ë¶ì„ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤:\n",
        "- ì›¹ í¬ë¡¤ë§ì˜ ê¸°ë³¸ ì›ë¦¬ì™€ ë™ì‘ ë°©ì‹ ì´í•´\n",
        "- `requests` ëª¨ë“ˆë¡œ ì›¹í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘\n",
        "- `BeautifulSoup`ë¡œ HTML ë¬¸ì„œ íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ\n",
        "- CSS ì„ íƒìë¥¼ í™œìš©í•œ ì •í™•í•œ ìš”ì†Œ ì„ íƒ\n",
        "- ì‹¤ì „ ì›¹ì‚¬ì´íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ\n",
        "- í¬ë¡¤ë§ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ í•´ê²° ë°©ë²•\n",
        "\n",
        "## ğŸ“š ëª©ì°¨\n",
        "1. **í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ í™•ì¸**\n",
        "2. **ì›¹ í¬ë¡¤ë§ ê¸°ë³¸ ê°œë…ê³¼ ì›ë¦¬**\n",
        "3. **HTTP í†µì‹ ê³¼ requests ëª¨ë“ˆ ê¸°ì´ˆ**\n",
        "4. **HTML êµ¬ì¡° ì´í•´í•˜ê¸°**\n",
        "5. **BeautifulSoup ê¸°ì´ˆ íŒŒì‹±**\n",
        "6. **CSS ì„ íƒìì™€ ê³ ê¸‰ íŒŒì‹± ê¸°ë²•**\n",
        "7. **ì‹¤ì „ ì˜ˆì œ: ë³µí•© ì›¹í˜ì´ì§€ íŒŒì‹±**\n",
        "8. **ì—ëŸ¬ ì²˜ë¦¬ì™€ ì˜ˆì™¸ ìƒí™© ëŒ€ì‘**\n",
        "9. **í¬ë¡¤ë§ ìœ¤ë¦¬ì™€ ë²•ì  ì£¼ì˜ì‚¬í•­**\n",
        "10. **ì¢…í•© ì •ë¦¬ ë° ì‹¤ì „ í”„ë¡œì íŠ¸ ì œì•ˆ**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. ğŸ”§ í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ í™•ì¸\n",
        "\n",
        "ì›¹ í¬ë¡¤ë§ì„ ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ í™•ì¸í•˜ê³  ì„¤ì¹˜í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "### í•„ìˆ˜ íŒ¨í‚¤ì§€\n",
        "- **requests**: HTTP ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "- **beautifulsoup4**: HTML íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "- **lxml**: BeautifulSoupì˜ ë¹ ë¥¸ íŒŒì„œ (ì„ íƒì‚¬í•­)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… requests ë²„ì „: 2.32.4\n",
            "âœ… beautifulsoup4 ì„¤ì¹˜ë¨\n",
            "âœ… lxml ì„¤ì¹˜ë¨\n",
            "âœ… urllib.parse ì‚¬ìš© ê°€ëŠ¥ (URL ì¡°ì‘ìš©)\n",
            "âœ… time ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ (ìš”ì²­ ê°„ê²© ì¡°ì ˆìš©)\n",
            "\n",
            "ğŸ¯ ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ” íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë²„ì „ í™•ì¸\n",
        "\n",
        "# í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ importí•˜ì—¬ ì„¤ì¹˜ ìƒíƒœ í™•ì¸\n",
        "try:\n",
        "    import requests\n",
        "    print(f\"âœ… requests ë²„ì „: {requests.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"âŒ requestsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install requests'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.\")\n",
        "\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "    import bs4\n",
        "    print(f\"âœ… beautifulsoup4 ì„¤ì¹˜ë¨\")\n",
        "except ImportError:\n",
        "    print(\"âŒ beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install beautifulsoup4'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.\")\n",
        "\n",
        "try:\n",
        "    import lxml  # type: ignore\n",
        "    print(f\"âœ… lxml ì„¤ì¹˜ë¨\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸  lxmlì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­). ì„¤ì¹˜í•˜ë©´ íŒŒì‹± ì†ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ì¶”ê°€ ìœ ìš©í•œ íŒ¨í‚¤ì§€ë“¤\n",
        "try:\n",
        "    import urllib.parse\n",
        "    print(\"âœ… urllib.parse ì‚¬ìš© ê°€ëŠ¥ (URL ì¡°ì‘ìš©)\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import time\n",
        "    print(\"âœ… time ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ (ìš”ì²­ ê°„ê²© ì¡°ì ˆìš©)\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "print(\"\\nğŸ¯ ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. ğŸŒ ì›¹ í¬ë¡¤ë§ ê¸°ë³¸ ê°œë…ê³¼ ì›ë¦¬\n",
        "\n",
        "### ì›¹ í¬ë¡¤ë§ì´ë€?\n",
        "ì›¹ í¬ë¡¤ë§(Web Crawling) ë˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘(Web Scraping)ì€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ì›¹ í¬ë¡¤ë§ì˜ ë™ì‘ ì›ë¦¬\n",
        "```markdown\n",
        "ğŸ“± í´ë¼ì´ì–¸íŠ¸(Python) ----HTTP ìš”ì²­----> ğŸ–¥ï¸ ì›¹ì„œë²„\n",
        "                      <---HTML ì‘ë‹µ----\n",
        "                      \n",
        "1. ìš”ì²­(Request): íŠ¹ì • URLì— HTTP ìš”ì²­ì„ ë³´ëƒ„\n",
        "2. ì‘ë‹µ(Response): ì„œë²„ì—ì„œ HTML, JSON ë“±ì˜ ë°ì´í„°ë¡œ ì‘ë‹µ\n",
        "3. íŒŒì‹±(Parsing): ë°›ì€ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œ\n",
        "4. ì €ì¥(Storage): ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ íŒŒì¼ì´ë‚˜ DBì— ì €ì¥\n",
        "```\n",
        "\n",
        "### í¬ë¡¤ë§ ë„êµ¬ì˜ ë°œì „\n",
        "- **urllib** (ì´ˆê¸°) â†’ **requests** (í˜„ì¬ í‘œì¤€)\n",
        "- **ì •ê·œí‘œí˜„ì‹** â†’ **BeautifulSoup/lxml** (HTML íŒŒì‹±)\n",
        "- **ì •ì  í˜ì´ì§€** â†’ **Selenium** (ë™ì  í˜ì´ì§€/JavaScript)\n",
        "\n",
        "### í¬ë¡¤ë§ì´ ì–´ë ¤ìš´ ì´ìœ \n",
        "- ì›¹ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ êµ¬ì¡°ì™€ ë°©ì‹\n",
        "- ì§€ì†ì ì¸ ì›¹ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸\n",
        "- ë´‡ ì°¨ë‹¨ ê¸°ìˆ  (reCAPTCHA, ìš”ì²­ ì œí•œ ë“±)\n",
        "- ë™ì  ì½˜í…ì¸  ì¦ê°€ (JavaScript ë Œë”ë§)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. ğŸ“¡ HTTP í†µì‹ ê³¼ requests ëª¨ë“ˆ ê¸°ì´ˆ\n",
        "\n",
        "### HTTP ìƒíƒœ ì½”ë“œì˜ ì´í•´\n",
        "ì›¹ ì„œë²„ëŠ” ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ìƒíƒœ ì½”ë“œë¥¼ ë³´ëƒ…ë‹ˆë‹¤:\n",
        "\n",
        "| ì½”ë“œ | ì˜ë¯¸ | ì„¤ëª… |\n",
        "|------|------|------|\n",
        "| **2xx** | **ì„±ê³µ** | ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë¨ |\n",
        "| 200 | OK | ì •ìƒì ì¸ ì‘ë‹µ |\n",
        "| **4xx** | **í´ë¼ì´ì–¸íŠ¸ ì—ëŸ¬** | ìš”ì²­ì— ë¬¸ì œê°€ ìˆìŒ |\n",
        "| 404 | Not Found | í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ |\n",
        "| 403 | Forbidden | ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŒ |\n",
        "| **5xx** | **ì„œë²„ ì—ëŸ¬** | ì„œë²„ì—ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ |\n",
        "| 500 | Internal Server Error | ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ |\n",
        "\n",
        "### requests ëª¨ë“ˆì˜ ì£¼ìš” ê¸°ëŠ¥\n",
        "- **ê°„ë‹¨í•œ API**: `requests.get(url)` í•œ ì¤„ë¡œ ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°\n",
        "- **ìë™ ì¸ì½”ë”©**: ì‘ë‹µ ë°ì´í„°ì˜ ì¸ì½”ë”© ìë™ ì²˜ë¦¬\n",
        "- **ì„¸ì…˜ ê´€ë¦¬**: ì¿ í‚¤ì™€ ì„¸ì…˜ ìë™ ì²˜ë¦¬\n",
        "- **ë‹¤ì–‘í•œ HTTP ë©”ì†Œë“œ**: GET, POST, PUT, DELETE ë“± ì§€ì›\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ§ª í…ŒìŠ¤íŠ¸ 1: http://www.pythonscraping.com/exercises/exercise1.html\n",
            "============================================================\n",
            "ğŸ”„ ìš”ì²­ ì¤‘: http://www.pythonscraping.com/exercises/exercise1.html\n",
            "ğŸ“Š ì‘ë‹µ ìƒíƒœ ì½”ë“œ: 200\n",
            "ğŸ“ ì‘ë‹µ í¬ê¸°: 564 ë¬¸ì\n",
            "ğŸ”¤ ì¸ì½”ë”©: ISO-8859-1\n",
            "âœ… ì„±ê³µì ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!\n",
            "\n",
            "ğŸ“„ HTML ë¯¸ë¦¬ë³´ê¸°:\n",
            "<html>\n",
            "<head>\n",
            "<title>A Useful Page</title>\n",
            "</head>\n",
            "<body>\n",
            "<h1>An Interesting Title</h1>\n",
            "<div>\n",
            "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliqui...\n",
            "\n",
            "============================================================\n",
            "ğŸ§ª í…ŒìŠ¤íŠ¸ 2: https://httpbin.org/status/404\n",
            "============================================================\n",
            "ğŸ”„ ìš”ì²­ ì¤‘: https://httpbin.org/status/404\n",
            "ğŸ“Š ì‘ë‹µ ìƒíƒœ ì½”ë“œ: 404\n",
            "ğŸ“ ì‘ë‹µ í¬ê¸°: 0 ë¬¸ì\n",
            "ğŸ”¤ ì¸ì½”ë”©: utf-8\n",
            "âŒ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (404)\n",
            "âŒ ì‹¤íŒ¨: í˜ì´ì§€ ì—†ìŒ\n",
            "\n",
            "============================================================\n",
            "ğŸ§ª í…ŒìŠ¤íŠ¸ 3: https://httpbin.org/delay/2\n",
            "============================================================\n",
            "ğŸ”„ ìš”ì²­ ì¤‘: https://httpbin.org/delay/2\n",
            "ğŸ“Š ì‘ë‹µ ìƒíƒœ ì½”ë“œ: 200\n",
            "ğŸ“ ì‘ë‹µ í¬ê¸°: 396 ë¬¸ì\n",
            "ğŸ”¤ ì¸ì½”ë”©: utf-8\n",
            "âœ… ì„±ê³µì ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!\n",
            "\n",
            "ğŸ“„ HTML ë¯¸ë¦¬ë³´ê¸°:\n",
            "{\n",
            "  \"args\": {}, \n",
            "  \"data\": \"\", \n",
            "  \"files\": {}, \n",
            "  \"form\": {}, \n",
            "  \"headers\": {\n",
            "    \"Accept\": \"*/*\", \n",
            "    \"Accept-Encoding\": \"gzip, deflate\", \n",
            "    \"Host\": \"httpbin.org\", \n",
            "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\", \n",
            "    \"X-Amzn-Trace-Id\": \"Root=1-687dc0e5-6fba8c85...\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ ì²« ë²ˆì§¸ ì›¹ í¬ë¡¤ë§ ì‹¤ìŠµ\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def fetch_webpage(url):\n",
        "    \"\"\"\n",
        "    ì›¹í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì•ˆì „í•œ í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        url (str): ê°€ì ¸ì˜¬ ì›¹í˜ì´ì§€ì˜ URL\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (ì„±ê³µì—¬ë¶€, ì‘ë‹µê°ì²´ ë˜ëŠ” ì—ëŸ¬ë©”ì‹œì§€)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"ğŸ”„ ìš”ì²­ ì¤‘: {url}\")\n",
        "        \n",
        "        # User-Agent í—¤ë” ì¶”ê°€ (ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” ë¸Œë¼ìš°ì €ì¸ ì²™ í•´ì•¼ í•¨)\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        \n",
        "        # ìš”ì²­ ë³´ë‚´ê¸° (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        \n",
        "        print(f\"ğŸ“Š ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}\")\n",
        "        print(f\"ğŸ“ ì‘ë‹µ í¬ê¸°: {len(response.text):,} ë¬¸ì\")\n",
        "        print(f\"ğŸ”¤ ì¸ì½”ë”©: {response.encoding}\")\n",
        "        \n",
        "        # ìƒíƒœ ì½”ë“œì— ë”°ë¥¸ ì²˜ë¦¬\n",
        "        if response.status_code == 200:\n",
        "            print(\"âœ… ì„±ê³µì ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!\")\n",
        "            return True, response\n",
        "        elif response.status_code == 404:\n",
        "            print(\"âŒ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (404)\")\n",
        "            return False, \"í˜ì´ì§€ ì—†ìŒ\"\n",
        "        elif response.status_code == 403:\n",
        "            print(\"ğŸš« ì ‘ê·¼ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤ (403)\")\n",
        "            return False, \"ì ‘ê·¼ ê±°ë¶€\"\n",
        "        else:\n",
        "            print(f\"âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ ìƒíƒœ ì½”ë“œ: {response.status_code}\")\n",
        "            return False, f\"ìƒíƒœ ì½”ë“œ: {response.status_code}\"\n",
        "            \n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"â° ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
        "        return False, \"íƒ€ì„ì•„ì›ƒ\"\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"ğŸŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜ì…ë‹ˆë‹¤\")\n",
        "        return False, \"ì—°ê²° ì˜¤ë¥˜\"\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸ’¥ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}\")\n",
        "        return False, str(e)\n",
        "\n",
        "# ì‹¤ìŠµìš© ì›¹í˜ì´ì§€ë“¤\n",
        "test_urls = [\n",
        "    \"http://www.pythonscraping.com/exercises/exercise1.html\",  # ì„±ê³µ ì˜ˆì œ\n",
        "    \"https://httpbin.org/status/404\",  # 404 ì—ëŸ¬ ì˜ˆì œ\n",
        "    \"https://httpbin.org/delay/2\"  # ì§€ì—° ì‘ë‹µ ì˜ˆì œ\n",
        "]\n",
        "\n",
        "for i, url in enumerate(test_urls, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸ§ª í…ŒìŠ¤íŠ¸ {i}: {url}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    success, result = fetch_webpage(url)\n",
        "    \n",
        "    if success:\n",
        "        # ì„±ê³µí•œ ê²½ìš° HTML ì¼ë¶€ ì¶œë ¥ (resultëŠ” response ê°ì²´)\n",
        "        html_text = result.text  # type: ignore\n",
        "        html_preview = html_text[:300] + \"...\" if len(html_text) > 300 else html_text\n",
        "        print(f\"\\nğŸ“„ HTML ë¯¸ë¦¬ë³´ê¸°:\\n{html_preview}\")\n",
        "    else:\n",
        "        print(f\"âŒ ì‹¤íŒ¨: {result}\")\n",
        "    \n",
        "    # ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸°\n",
        "    time.sleep(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. ğŸ“ HTML êµ¬ì¡° ì´í•´í•˜ê¸°\n",
        "\n",
        "### HTMLì˜ ê¸°ë³¸ êµ¬ì¡°\n",
        "HTML(HyperText Markup Language)ì€ ì›¹í˜ì´ì§€ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ë§ˆí¬ì—… ì–¸ì–´ì…ë‹ˆë‹¤.\n",
        "\n",
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>í˜ì´ì§€ ì œëª©</title>\n",
        "    <meta charset=\"UTF-8\">\n",
        "</head>\n",
        "<body>\n",
        "    <h1 id=\"main-title\" class=\"header\">ë©”ì¸ ì œëª©</h1>\n",
        "    <div class=\"content\">\n",
        "        <p>ë³¸ë¬¸ ë‚´ìš©</p>\n",
        "        <ul>\n",
        "            <li>ëª©ë¡ í•­ëª© 1</li>\n",
        "            <li>ëª©ë¡ í•­ëª© 2</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "```\n",
        "\n",
        "### HTML ìš”ì†Œì˜ êµ¬ì„±\n",
        "- **íƒœê·¸(Tag)**: `<íƒœê·¸ëª…>ë‚´ìš©</íƒœê·¸ëª…>`\n",
        "- **ì†ì„±(Attribute)**: `<íƒœê·¸ ì†ì„±=\"ê°’\">`\n",
        "- **ID**: ê³ ìœ  ì‹ë³„ì `id=\"unique-name\"`\n",
        "- **Class**: ìŠ¤íƒ€ì¼ë§/ê·¸ë£¹í•‘ìš© `class=\"group-name\"`\n",
        "\n",
        "### í¬ë¡¤ë§ ê´€ì ì—ì„œ ì¤‘ìš”í•œ HTML ìš”ì†Œë“¤\n",
        "\n",
        "| íƒœê·¸ | ìš©ë„ | ì˜ˆì‹œ |\n",
        "|------|------|------|\n",
        "| `<title>` | í˜ì´ì§€ ì œëª© | `<title>ë‰´ìŠ¤ ì œëª©</title>` |\n",
        "| `<h1>~<h6>` | ì œëª©/í—¤ë” | `<h1>ë©”ì¸ ì œëª©</h1>` |\n",
        "| `<p>` | ë¬¸ë‹¨ | `<p>ë³¸ë¬¸ ë‚´ìš©</p>` |\n",
        "| `<div>` | êµ¬ì—­ ë‚˜ëˆ„ê¸° | `<div class=\"article\">...</div>` |\n",
        "| `<span>` | ì¸ë¼ì¸ ìš”ì†Œ | `<span class=\"price\">1,000ì›</span>` |\n",
        "| `<table>` | í‘œ | `<table><tr><td>ë°ì´í„°</td></tr></table>` |\n",
        "| `<ul>`, `<li>` | ëª©ë¡ | `<ul><li>í•­ëª©</li></ul>` |\n",
        "| `<a>` | ë§í¬ | `<a href=\"url\">ë§í¬í…ìŠ¤íŠ¸</a>` |\n",
        "\n",
        "### CSS ì„ íƒì ê¸°ì´ˆ\n",
        "í¬ë¡¤ë§ì—ì„œ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ì •í™•íˆ ì°¾ê¸° ìœ„í•´ CSS ì„ íƒìë¥¼ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "\n",
        "- **íƒœê·¸ ì„ íƒì**: `h1`, `p`, `div`\n",
        "- **ID ì„ íƒì**: `#main-title`\n",
        "- **í´ë˜ìŠ¤ ì„ íƒì**: `.content`, `.header`\n",
        "- **ì†ì„± ì„ íƒì**: `[href]`, `[class=\"content\"]`\n",
        "- **ì¡°í•© ì„ íƒì**: `div.content p` (div ì•ˆì˜ p íƒœê·¸)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. ğŸ² BeautifulSoup ê¸°ì´ˆ íŒŒì‹±\n",
        "\n",
        "### BeautifulSoupë€?\n",
        "BeautifulSoupëŠ” HTMLê³¼ XML ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•´ì£¼ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ì£¼ìš” íŠ¹ì§•\n",
        "- **ê°„ë‹¨í•œ API**: ì§ê´€ì ì´ê³  ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì¸í„°í˜ì´ìŠ¤\n",
        "- **ê´€ëŒ€í•œ íŒŒì‹±**: ì˜ëª»ëœ HTMLë„ ì˜ ì²˜ë¦¬\n",
        "- **ë‹¤ì–‘í•œ íŒŒì„œ**: html.parser, lxml, html5lib ì§€ì›\n",
        "- **CSS ì„ íƒì**: CSS ì„ íƒìë¥¼ ì‚¬ìš©í•œ ìš”ì†Œ ì„ íƒ\n",
        "\n",
        "### ê¸°ë³¸ ì‚¬ìš©ë²•\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# HTML íŒŒì‹±\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# ìš”ì†Œ ì°¾ê¸°\n",
        "soup.find('íƒœê·¸ëª…')              # ì²« ë²ˆì§¸ ìš”ì†Œ\n",
        "soup.find_all('íƒœê·¸ëª…')          # ëª¨ë“  ìš”ì†Œ\n",
        "soup.select('CSSì„ íƒì')         # CSS ì„ íƒìë¡œ ì°¾ê¸°\n",
        "```\n",
        "\n",
        "### find() vs find_all() vs select()\n",
        "- **find()**: ì¡°ê±´ì— ë§ëŠ” **ì²« ë²ˆì§¸ ìš”ì†Œë§Œ** ë°˜í™˜\n",
        "- **find_all()**: ì¡°ê±´ì— ë§ëŠ” **ëª¨ë“  ìš”ì†Œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ** ë°˜í™˜\n",
        "- **select()**: **CSS ì„ íƒì**ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì†Œ ì°¾ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ HTML íŒŒì‹± ì¤‘...\n",
            "âœ… íŒŒì‹± ì™„ë£Œ!\n",
            "\n",
            "============================================================\n",
            "ğŸ“– BeautifulSoup ê¸°ë³¸ ë©”ì†Œë“œ ì‹¤ìŠµ\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ find() - ì²« ë²ˆì§¸ ìš”ì†Œ ì°¾ê¸°\n",
            "------------------------------\n",
            "í˜ì´ì§€ ì œëª©: ì˜¨ë¼ì¸ ì„œì  - ë² ìŠ¤íŠ¸ì…€ëŸ¬\n",
            "ì²« ë²ˆì§¸ ì±… ì œëª©: íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "\n",
            "2ï¸âƒ£ find_all() - ëª¨ë“  ìš”ì†Œ ì°¾ê¸°\n",
            "------------------------------\n",
            "ì´ 3ê¶Œì˜ ì±…ì´ ìˆìŠµë‹ˆë‹¤:\n",
            "  1. íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "  2. ì›¹ í¬ë¡¤ë§ ë§ˆìŠ¤í„°\n",
            "  3. ë°ì´í„° ë¶„ì„ ì‹¤ì „\n",
            "\n",
            "3ï¸âƒ£ ì†ì„±ìœ¼ë¡œ ìš”ì†Œ ì°¾ê¸°\n",
            "------------------------------\n",
            "ë©”ì¸ ì œëª©: ğŸ“š ì˜¨ë¼ì¸ ì„œì \n",
            "data-id ì†ì„±ì´ ìˆëŠ” ì±…: 3ê¶Œ\n",
            "\n",
            "4ï¸âƒ£ í…ìŠ¤íŠ¸ì™€ ì†ì„± ê°’ ì¶”ì¶œ\n",
            "------------------------------\n",
            "ğŸ“— ë„ì„œ ID: 1\n",
            "   ì œëª©: íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "   ì €ì: ê¹€ê°œë°œ\n",
            "   ê°€ê²©: 25,000ì›\n",
            "   í‰ì : â­â­â­â­â­ (4.8)\n",
            "\n",
            "ğŸ“— ë„ì„œ ID: 2\n",
            "   ì œëª©: ì›¹ í¬ë¡¤ë§ ë§ˆìŠ¤í„°\n",
            "   ì €ì: ì´í¬ë¡¤ë§\n",
            "   ê°€ê²©: 30,000ì›\n",
            "   í‰ì : â­â­â­â­ (4.5)\n",
            "\n",
            "ğŸ“— ë„ì„œ ID: 3\n",
            "   ì œëª©: ë°ì´í„° ë¶„ì„ ì‹¤ì „\n",
            "   ì €ì: ë°•ë°ì´í„°\n",
            "   ê°€ê²©: 28,000ì›\n",
            "   í‰ì : â­â­â­â­â­ (4.9)\n",
            "\n",
            "ğŸ¯ BeautifulSoup ê¸°ì´ˆ ì‹¤ìŠµ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ¥„ BeautifulSoup ê¸°ì´ˆ ì‹¤ìŠµ\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# ì‹¤ìŠµìš© ìƒ˜í”Œ HTML ìƒì„±\n",
        "sample_html = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>ì˜¨ë¼ì¸ ì„œì  - ë² ìŠ¤íŠ¸ì…€ëŸ¬</title>\n",
        "    <meta charset=\"UTF-8\">\n",
        "</head>\n",
        "<body>\n",
        "    <header>\n",
        "        <h1 id=\"main-title\" class=\"site-title\">ğŸ“š ì˜¨ë¼ì¸ ì„œì </h1>\n",
        "        <nav class=\"navigation\">\n",
        "            <a href=\"/books\">ë„ì„œ</a>\n",
        "            <a href=\"/authors\">ì‘ê°€</a>\n",
        "        </nav>\n",
        "    </header>\n",
        "    \n",
        "    <main class=\"content\">\n",
        "        <section class=\"bestsellers\">\n",
        "            <h2 class=\"section-title\">ì´ë²ˆ ì£¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬</h2>\n",
        "            \n",
        "            <div class=\"book\" data-id=\"1\">\n",
        "                <h3 class=\"book-title\">íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ</h3>\n",
        "                <p class=\"author\">ê¹€ê°œë°œ</p>\n",
        "                <span class=\"price\">25,000ì›</span>\n",
        "                <div class=\"rating\">â­â­â­â­â­ (4.8)</div>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"book\" data-id=\"2\">\n",
        "                <h3 class=\"book-title\">ì›¹ í¬ë¡¤ë§ ë§ˆìŠ¤í„°</h3>\n",
        "                <p class=\"author\">ì´í¬ë¡¤ë§</p>\n",
        "                <span class=\"price\">30,000ì›</span>\n",
        "                <div class=\"rating\">â­â­â­â­ (4.5)</div>\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"book\" data-id=\"3\">\n",
        "                <h3 class=\"book-title\">ë°ì´í„° ë¶„ì„ ì‹¤ì „</h3>\n",
        "                <p class=\"author\">ë°•ë°ì´í„°</p>\n",
        "                <span class=\"price\">28,000ì›</span>\n",
        "                <div class=\"rating\">â­â­â­â­â­ (4.9)</div>\n",
        "            </div>\n",
        "        </section>\n",
        "        \n",
        "        <aside class=\"sidebar\">\n",
        "            <h3>ì¶”ì²œ ì¹´í…Œê³ ë¦¬</h3>\n",
        "            <ul class=\"categories\">\n",
        "                <li><a href=\"/category/programming\">í”„ë¡œê·¸ë˜ë°</a></li>\n",
        "                <li><a href=\"/category/data-science\">ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤</a></li>\n",
        "                <li><a href=\"/category/web\">ì›¹ ê°œë°œ</a></li>\n",
        "            </ul>\n",
        "        </aside>\n",
        "    </main>\n",
        "    \n",
        "    <footer>\n",
        "        <p>&copy; 2024 ì˜¨ë¼ì¸ ì„œì . All rights reserved.</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# BeautifulSoupë¡œ íŒŒì‹±\n",
        "print(\"ğŸ”„ HTML íŒŒì‹± ì¤‘...\")\n",
        "soup = BeautifulSoup(sample_html, 'html.parser')\n",
        "print(\"âœ… íŒŒì‹± ì™„ë£Œ!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“– BeautifulSoup ê¸°ë³¸ ë©”ì†Œë“œ ì‹¤ìŠµ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. find() - ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì°¾ê¸°\n",
        "print(\"\\n1ï¸âƒ£ find() - ì²« ë²ˆì§¸ ìš”ì†Œ ì°¾ê¸°\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "title = soup.find('title')\n",
        "if title:\n",
        "    print(f\"í˜ì´ì§€ ì œëª©: {title.text}\")\n",
        "\n",
        "first_book_title = soup.find('h3', class_='book-title')\n",
        "if first_book_title:\n",
        "    print(f\"ì²« ë²ˆì§¸ ì±… ì œëª©: {first_book_title.text}\")\n",
        "\n",
        "# 2. find_all() - ëª¨ë“  ìš”ì†Œ ì°¾ê¸°\n",
        "print(\"\\n2ï¸âƒ£ find_all() - ëª¨ë“  ìš”ì†Œ ì°¾ê¸°\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "all_book_titles = soup.find_all('h3', class_='book-title')\n",
        "print(f\"ì´ {len(all_book_titles)}ê¶Œì˜ ì±…ì´ ìˆìŠµë‹ˆë‹¤:\")\n",
        "for i, title in enumerate(all_book_titles, 1):\n",
        "    print(f\"  {i}. {title.text}\")\n",
        "\n",
        "# 3. ì†ì„±ìœ¼ë¡œ ì°¾ê¸°\n",
        "print(\"\\n3ï¸âƒ£ ì†ì„±ìœ¼ë¡œ ìš”ì†Œ ì°¾ê¸°\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# IDë¡œ ì°¾ê¸°\n",
        "main_title = soup.find('h1', id='main-title')\n",
        "if main_title:\n",
        "    print(f\"ë©”ì¸ ì œëª©: {main_title.text}\")\n",
        "\n",
        "# ì—¬ëŸ¬ ì†ì„±ìœ¼ë¡œ ì°¾ê¸°\n",
        "books_with_data_id = soup.find_all('div', {'class': 'book', 'data-id': True})\n",
        "print(f\"data-id ì†ì„±ì´ ìˆëŠ” ì±…: {len(books_with_data_id)}ê¶Œ\")\n",
        "\n",
        "# 4. í…ìŠ¤íŠ¸ ë‚´ìš©ê³¼ ì†ì„± ê°’ ê°€ì ¸ì˜¤ê¸°\n",
        "print(\"\\n4ï¸âƒ£ í…ìŠ¤íŠ¸ì™€ ì†ì„± ê°’ ì¶”ì¶œ\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for book in soup.find_all('div', class_='book'):\n",
        "    title = book.find('h3', class_='book-title').text\n",
        "    author = book.find('p', class_='author').text\n",
        "    price = book.find('span', class_='price').text\n",
        "    rating = book.find('div', class_='rating').text\n",
        "    book_id = book.get('data-id')  # ì†ì„± ê°’ ê°€ì ¸ì˜¤ê¸°\n",
        "    \n",
        "    print(f\"ğŸ“— ë„ì„œ ID: {book_id}\")\n",
        "    print(f\"   ì œëª©: {title}\")\n",
        "    print(f\"   ì €ì: {author}\")\n",
        "    print(f\"   ê°€ê²©: {price}\")\n",
        "    print(f\"   í‰ì : {rating}\")\n",
        "    print()\n",
        "\n",
        "print(\"ğŸ¯ BeautifulSoup ê¸°ì´ˆ ì‹¤ìŠµ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. ğŸ¯ CSS ì„ íƒìì™€ ê³ ê¸‰ íŒŒì‹± ê¸°ë²•\n",
        "\n",
        "### CSS ì„ íƒìì˜ ê°•ë ¥í•¨\n",
        "CSS ì„ íƒìë¥¼ ì‚¬ìš©í•˜ë©´ ë³µì¡í•œ HTML êµ¬ì¡°ì—ì„œë„ ì •í™•í•œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### BeautifulSoupì˜ select() ë©”ì†Œë“œ\n",
        "`soup.select(CSSì„ íƒì)`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì •êµí•œ ìš”ì†Œ ì„ íƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ì£¼ìš” CSS ì„ íƒì íŒ¨í„´\n",
        "\n",
        "| ì„ íƒì | ì˜ë¯¸ | ì˜ˆì‹œ |\n",
        "|--------|------|------|\n",
        "| `tag` | íƒœê·¸ ì„ íƒ | `soup.select('h1')` |\n",
        "| `.class` | í´ë˜ìŠ¤ ì„ íƒ | `soup.select('.book-title')` |\n",
        "| `#id` | ID ì„ íƒ | `soup.select('#main-title')` |\n",
        "| `[attribute]` | ì†ì„± ì„ íƒ | `soup.select('[data-id]')` |\n",
        "| `parent > child` | ì§ì ‘ ìì‹ | `soup.select('div > h3')` |\n",
        "| `ancestor descendant` | í›„ì† | `soup.select('section h3')` |\n",
        "| `:nth-child(n)` | në²ˆì§¸ ìì‹ | `soup.select('li:nth-child(2)')` |\n",
        "\n",
        "### ê³ ê¸‰ ë°ì´í„° ì¶”ì¶œ ê¸°ë²•\n",
        "- **í…ìŠ¤íŠ¸ ì •ì œ**: `strip()`, `replace()` í™œìš©\n",
        "- **ì •ê·œí‘œí˜„ì‹**: ë³µì¡í•œ íŒ¨í„´ ì¶”ì¶œ\n",
        "- **ìƒëŒ€ ìœ„ì¹˜**: í˜•ì œ/ë¶€ëª¨ ìš”ì†Œ íƒìƒ‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” CSS ì„ íƒì ì‹¤ìŠµ ì‹œì‘!\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ ê¸°ë³¸ ì„ íƒì ë¹„êµ (find vs select)\n",
            "----------------------------------------\n",
            "find() ê²°ê³¼: íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "select() ê²°ê³¼: íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "\n",
            "2ï¸âƒ£ ë‹¤ì–‘í•œ CSS ì„ íƒì\n",
            "----------------------------------------\n",
            "ğŸ“š ëª¨ë“  ì±… ì œëª© (3ê°œ):\n",
            "  1. íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "  2. ì›¹ í¬ë¡¤ë§ ë§ˆìŠ¤í„°\n",
            "  3. ë°ì´í„° ë¶„ì„ ì‹¤ì „\n",
            "\n",
            "ğŸ  ë©”ì¸ ì œëª©: ğŸ“š ì˜¨ë¼ì¸ ì„œì \n",
            "\n",
            "ğŸ“Š data-id ì†ì„±ì´ ìˆëŠ” ìš”ì†Œ: 3ê°œ\n",
            "\n",
            "3ï¸âƒ£ ì¡°í•© ì„ íƒì\n",
            "----------------------------------------\n",
            "ğŸ“– section ì•ˆì˜ h3 íƒœê·¸: 3ê°œ\n",
            "ğŸ“˜ .bookì˜ ì§ì ‘ ìì‹ h3: 3ê°œ\n",
            "ğŸ‘¥ h3 ë°”ë¡œ ë‹¤ìŒì˜ p íƒœê·¸: 3ê°œ\n",
            "\n",
            "4ï¸âƒ£ ê°€ìƒ ì„ íƒì\n",
            "----------------------------------------\n",
            "ğŸ¥‰ ë§ˆì§€ë§‰ ì±…: ë°ì´í„° ë¶„ì„ ì‹¤ì „\n",
            "ğŸ¥ˆ ë‘ ë²ˆì§¸ ì±…: íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "\n",
            "5ï¸âƒ£ ë³µí•© ì¡°ê±´ ì„ íƒ\n",
            "----------------------------------------\n",
            "ğŸ“— IDê°€ 1ì¸ ì±…: íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "\n",
            "6ï¸âƒ£ ê³ ê¸‰ ë°ì´í„° ì¶”ì¶œ\n",
            "----------------------------------------\n",
            "ğŸ“š ì¶”ì¶œëœ ì±… ì •ë³´:\n",
            "  ğŸ“– íŒŒì´ì¬ ì™„ë²½ ê°€ì´ë“œ\n",
            "     ì €ì: ê¹€ê°œë°œ\n",
            "     ê°€ê²©: 25,000ì›\n",
            "     í‰ì : 4.8 (5â­)\n",
            "\n",
            "  ğŸ“– ì›¹ í¬ë¡¤ë§ ë§ˆìŠ¤í„°\n",
            "     ì €ì: ì´í¬ë¡¤ë§\n",
            "     ê°€ê²©: 30,000ì›\n",
            "     í‰ì : 4.5 (4â­)\n",
            "\n",
            "  ğŸ“– ë°ì´í„° ë¶„ì„ ì‹¤ì „\n",
            "     ì €ì: ë°•ë°ì´í„°\n",
            "     ê°€ê²©: 28,000ì›\n",
            "     í‰ì : 4.9 (5â­)\n",
            "\n",
            "7ï¸âƒ£ ê°„ë‹¨í•œ ë°ì´í„° ë¶„ì„\n",
            "----------------------------------------\n",
            "ğŸ“Š ì´ ë„ì„œ ìˆ˜: 3ê¶Œ\n",
            "ğŸ’° í‰ê·  ê°€ê²©: 27,667ì›\n",
            "â­ í‰ê·  í‰ì : 4.7\n",
            "ğŸ† ìµœê³  í‰ì  ë„ì„œ: ë°ì´í„° ë¶„ì„ ì‹¤ì „ (4.9ì )\n",
            "\n",
            "ğŸ¯ CSS ì„ íƒì ë§ˆìŠ¤í„° í´ë˜ìŠ¤ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ¯ CSS ì„ íƒì ë§ˆìŠ¤í„° í´ë˜ìŠ¤\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ì•ì—ì„œ ì‚¬ìš©í•œ ì˜¨ë¼ì¸ ì„œì  HTMLì„ ì¬ì‚¬ìš©\n",
        "# (soup ë³€ìˆ˜ê°€ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n",
        "\n",
        "print(\"ğŸ” CSS ì„ íƒì ì‹¤ìŠµ ì‹œì‘!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. ê¸°ë³¸ ì„ íƒì ë¹„êµ\n",
        "print(\"\\n1ï¸âƒ£ ê¸°ë³¸ ì„ íƒì ë¹„êµ (find vs select)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# find() ë°©ì‹\n",
        "find_title = soup.find('h3', class_='book-title')\n",
        "print(f\"find() ê²°ê³¼: {find_title.text if find_title else 'None'}\")\n",
        "\n",
        "# select() ë°©ì‹ - CSS ì„ íƒì\n",
        "select_title = soup.select('.book-title')[0]  # ì²« ë²ˆì§¸ ìš”ì†Œ\n",
        "print(f\"select() ê²°ê³¼: {select_title.text}\")\n",
        "\n",
        "# 2. ë‹¤ì–‘í•œ CSS ì„ íƒì ì‹¤ìŠµ\n",
        "print(\"\\n2ï¸âƒ£ ë‹¤ì–‘í•œ CSS ì„ íƒì\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# í´ë˜ìŠ¤ ì„ íƒì\n",
        "book_titles = soup.select('.book-title')\n",
        "print(f\"ğŸ“š ëª¨ë“  ì±… ì œëª© ({len(book_titles)}ê°œ):\")\n",
        "for i, title in enumerate(book_titles, 1):\n",
        "    print(f\"  {i}. {title.text}\")\n",
        "\n",
        "# ID ì„ íƒì\n",
        "main_title = soup.select('#main-title')\n",
        "if main_title:\n",
        "    print(f\"\\nğŸ  ë©”ì¸ ì œëª©: {main_title[0].text}\")\n",
        "\n",
        "# ì†ì„± ì„ íƒì\n",
        "books_with_data_id = soup.select('[data-id]')\n",
        "print(f\"\\nğŸ“Š data-id ì†ì„±ì´ ìˆëŠ” ìš”ì†Œ: {len(books_with_data_id)}ê°œ\")\n",
        "\n",
        "# 3. ì¡°í•© ì„ íƒì (Combinator)\n",
        "print(\"\\n3ï¸âƒ£ ì¡°í•© ì„ íƒì\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# í›„ì† ì„ íƒì (ê³µë°±)\n",
        "section_titles = soup.select('section h3')\n",
        "print(f\"ğŸ“– section ì•ˆì˜ h3 íƒœê·¸: {len(section_titles)}ê°œ\")\n",
        "\n",
        "# ì§ì ‘ ìì‹ ì„ íƒì (>)\n",
        "direct_children = soup.select('.book > h3')\n",
        "print(f\"ğŸ“˜ .bookì˜ ì§ì ‘ ìì‹ h3: {len(direct_children)}ê°œ\")\n",
        "\n",
        "# ì¸ì ‘ í˜•ì œ ì„ íƒì (+) - ë°”ë¡œ ë‹¤ìŒ í˜•ì œ\n",
        "next_siblings = soup.select('h3 + p')  # h3 ë°”ë¡œ ë‹¤ìŒì˜ p íƒœê·¸\n",
        "print(f\"ğŸ‘¥ h3 ë°”ë¡œ ë‹¤ìŒì˜ p íƒœê·¸: {len(next_siblings)}ê°œ\")\n",
        "\n",
        "# 4. ê°€ìƒ ì„ íƒì (Pseudo-selector)\n",
        "print(\"\\n4ï¸âƒ£ ê°€ìƒ ì„ íƒì\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# në²ˆì§¸ ìì‹\n",
        "first_book = soup.select('.book:first-child')\n",
        "if first_book:\n",
        "    title = first_book[0].select('.book-title')[0].text\n",
        "    print(f\"ğŸ¥‡ ì²« ë²ˆì§¸ ì±…: {title}\")\n",
        "\n",
        "last_book = soup.select('.book:last-child')\n",
        "if last_book:\n",
        "    title = last_book[0].select('.book-title')[0].text\n",
        "    print(f\"ğŸ¥‰ ë§ˆì§€ë§‰ ì±…: {title}\")\n",
        "\n",
        "# në²ˆì§¸ íŠ¹ì • ìì‹\n",
        "second_book = soup.select('.book:nth-child(2)')\n",
        "if second_book:\n",
        "    title = second_book[0].select('.book-title')[0].text\n",
        "    print(f\"ğŸ¥ˆ ë‘ ë²ˆì§¸ ì±…: {title}\")\n",
        "\n",
        "# 5. ë³µí•© ì¡°ê±´ ì„ íƒ\n",
        "print(\"\\n5ï¸âƒ£ ë³µí•© ì¡°ê±´ ì„ íƒ\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# ì—¬ëŸ¬ í´ë˜ìŠ¤ ì¡°ê±´\n",
        "# ë§Œì•½ 'book premium' í´ë˜ìŠ¤ê°€ ìˆë‹¤ë©´\n",
        "# premium_books = soup.select('.book.premium')\n",
        "\n",
        "# ì†ì„± ê°’ìœ¼ë¡œ ì„ íƒ\n",
        "book_1 = soup.select('[data-id=\"1\"]')\n",
        "if book_1:\n",
        "    title = book_1[0].select('.book-title')[0].text\n",
        "    print(f\"ğŸ“— IDê°€ 1ì¸ ì±…: {title}\")\n",
        "\n",
        "# 6. í…ìŠ¤íŠ¸ ê¸°ë°˜ ê³ ê¸‰ ì¶”ì¶œ\n",
        "print(\"\\n6ï¸âƒ£ ê³ ê¸‰ ë°ì´í„° ì¶”ì¶œ\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "def extract_book_info(book_element):\n",
        "    \"\"\"ì±… ì •ë³´ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    info = {}\n",
        "    \n",
        "    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ\n",
        "    info['id'] = book_element.get('data-id')\n",
        "    info['title'] = book_element.select('.book-title')[0].text.strip()\n",
        "    info['author'] = book_element.select('.author')[0].text.strip()\n",
        "    \n",
        "    # ê°€ê²©ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©)\n",
        "    price_text = book_element.select('.price')[0].text\n",
        "    price_match = re.search(r'[\\d,]+', price_text)\n",
        "    info['price'] = int(price_match.group().replace(',', '')) if price_match else 0\n",
        "    \n",
        "    # í‰ì ì—ì„œ ìˆ«ì ì¶”ì¶œ\n",
        "    rating_text = book_element.select('.rating')[0].text\n",
        "    rating_match = re.search(r'\\((\\d+\\.\\d+)\\)', rating_text)\n",
        "    info['rating'] = float(rating_match.group(1)) if rating_match else 0.0\n",
        "    \n",
        "    # ë³„ ê°œìˆ˜ ê³„ì‚°\n",
        "    star_count = rating_text.count('â­')\n",
        "    info['stars'] = star_count\n",
        "    \n",
        "    return info\n",
        "\n",
        "# ëª¨ë“  ì±… ì •ë³´ ì¶”ì¶œ\n",
        "books_data = []\n",
        "for book in soup.select('.book'):\n",
        "    book_info = extract_book_info(book)\n",
        "    books_data.append(book_info)\n",
        "\n",
        "# ì¶”ì¶œëœ ë°ì´í„° ì¶œë ¥\n",
        "print(\"ğŸ“š ì¶”ì¶œëœ ì±… ì •ë³´:\")\n",
        "for book in books_data:\n",
        "    print(f\"  ğŸ“– {book['title']}\")\n",
        "    print(f\"     ì €ì: {book['author']}\")\n",
        "    print(f\"     ê°€ê²©: {book['price']:,}ì›\")\n",
        "    print(f\"     í‰ì : {book['rating']} ({book['stars']}â­)\")\n",
        "    print()\n",
        "\n",
        "# 7. ë°ì´í„° ë¶„ì„\n",
        "print(\"7ï¸âƒ£ ê°„ë‹¨í•œ ë°ì´í„° ë¶„ì„\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "total_books = len(books_data)\n",
        "avg_price = sum(book['price'] for book in books_data) / total_books\n",
        "avg_rating = sum(book['rating'] for book in books_data) / total_books\n",
        "highest_rated = max(books_data, key=lambda x: x['rating'])\n",
        "\n",
        "print(f\"ğŸ“Š ì´ ë„ì„œ ìˆ˜: {total_books}ê¶Œ\")\n",
        "print(f\"ğŸ’° í‰ê·  ê°€ê²©: {avg_price:,.0f}ì›\")\n",
        "print(f\"â­ í‰ê·  í‰ì : {avg_rating:.1f}\")\n",
        "print(f\"ğŸ† ìµœê³  í‰ì  ë„ì„œ: {highest_rated['title']} ({highest_rated['rating']}ì )\")\n",
        "\n",
        "print(\"\\nğŸ¯ CSS ì„ íƒì ë§ˆìŠ¤í„° í´ë˜ìŠ¤ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì „ëµ\n",
        "\n",
        "ì´ì œ ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ë³´ê² ìŠµë‹ˆë‹¤. ì•ˆì „í•˜ê³  ìœ¤ë¦¬ì ì¸ í¬ë¡¤ë§ì„ ìœ„í•œ ì›ì¹™ë“¤ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "### í¬ë¡¤ë§ ì „ëµ ìˆ˜ë¦½\n",
        "1. **robots.txt í™•ì¸**: `ì‚¬ì´íŠ¸ì£¼ì†Œ/robots.txt`\n",
        "2. **ìš”ì²­ ê°„ê²© ì¡°ì ˆ**: ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
        "3. **User-Agent ì„¤ì •**: ì ì ˆí•œ ë¸Œë¼ìš°ì € í—¤ë”\n",
        "4. **ì—ëŸ¬ ì²˜ë¦¬**: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, íŒŒì‹± ì˜¤ë¥˜ ëŒ€ì‘\n",
        "5. **ë°ì´í„° ê²€ì¦**: ì¶”ì¶œí•œ ë°ì´í„°ì˜ ìœ íš¨ì„± í™•ì¸\n",
        "\n",
        "### ì‹¤ìŠµ ëŒ€ìƒ\n",
        "- **HTTPBin**: í…ŒìŠ¤íŠ¸ìš© HTTP ì„œë¹„ìŠ¤\n",
        "- **Python Scraping ì˜ˆì œ ì‚¬ì´íŠ¸**: í•™ìŠµìš© ì‚¬ì´íŠ¸\n",
        "- **ê³µê°œ API**: JSON ë°ì´í„° ì²˜ë¦¬ ì—°ìŠµ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ ì‹¤ì „ í¬ë¡¤ë§ í”„ë¡œì íŠ¸ ì‹œì‘!\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ ê¸°ë³¸ HTML í˜ì´ì§€ í¬ë¡¤ë§\n",
            "----------------------------------------\n",
            "ğŸ”„ ìš”ì²­: http://www.pythonscraping.com/exercises/exercise1.html\n",
            "âœ… ì„±ê³µ (í¬ê¸°: 564 ë¬¸ì)\n",
            "ğŸ“„ í˜ì´ì§€ ì œëª©: A Useful Page\n",
            "ğŸ”¤ ë©”ì¸ í—¤ë”©: An Interesting Title\n",
            "\n",
            "2ï¸âƒ£ JSON API ë°ì´í„° ì²˜ë¦¬\n",
            "----------------------------------------\n",
            "ğŸ”„ ìš”ì²­: https://httpbin.org/json\n",
            "âœ… ì„±ê³µ (í¬ê¸°: 429 ë¬¸ì)\n",
            "ğŸ“Š JSON ë°ì´í„°:\n",
            "{\n",
            "  \"slideshow\": {\n",
            "    \"author\": \"Yours Truly\",\n",
            "    \"date\": \"date of publication\",\n",
            "    \"slides\": [\n",
            "      {\n",
            "        \"title\": \"Wake up to WonderWidgets!\",\n",
            "        \"type\": \"all\"\n",
            "      },\n",
            "      {\n",
            "        \"items\": [\n",
            "          \"Why <em>WonderWidgets</em> are great\",\n",
            "          \"Who <em>buys</em> WonderWidgets\"\n",
            "        ],\n",
            "        \"title\": \"Overview\",\n",
            "        \"type\": \"all\"\n",
            "      }\n",
            "    ],\n",
            "    \"title\": \"Sample Slide Show\"\n",
            "  }\n",
            "}\n",
            "\n",
            "3ï¸âƒ£ ë³µì¡í•œ HTML êµ¬ì¡° ë¶„ì„\n",
            "----------------------------------------\n",
            "ğŸ”„ ìš”ì²­: http://www.pythonscraping.com/pages/warandpeace.html\n",
            "âœ… ì„±ê³µ (í¬ê¸°: 11,723 ë¬¸ì)\n",
            "ğŸŸ¢ ë…¹ìƒ‰ í…ìŠ¤íŠ¸ (41ê°œ):\n",
            "  1. Anna\n",
            "Pavlovna Scherer\n",
            "  2. Empress Marya\n",
            "Fedorovna\n",
            "  3. Prince Vasili Kuragin\n",
            "  4. Anna Pavlovna\n",
            "  5. St. Petersburg\n",
            "  ... ë° 36ê°œ ë”\n",
            "\n",
            "4ï¸âƒ£ í˜ì´ì§€ ë‚´ ë§í¬ ìˆ˜ì§‘\n",
            "----------------------------------------\n",
            "ğŸ”„ ìš”ì²­: http://www.pythonscraping.com/pages/page3.html\n",
            "âœ… ì„±ê³µ (í¬ê¸°: 2,405 ë¬¸ì)\n",
            "ğŸ”— ë‚´ë¶€ ë§í¬ (0ê°œ):\n",
            "\n",
            "ğŸŒ ì™¸ë¶€ ë§í¬ (0ê°œ):\n",
            "\n",
            "5ï¸âƒ£ í…Œì´ë¸” ë°ì´í„° êµ¬ì¡°í™”\n",
            "----------------------------------------\n",
            "ğŸ“Š í…Œì´ë¸” ë°ì´í„° (6í–‰):\n",
            "  1. Item Title | Description | Cost | Image\n",
            "  2. Vegetable Basket | This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
            "Now with super-colorful bell peppers! | $15.00 | \n",
            "  3. Russian Nesting Dolls | Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! 8 entire dolls per set! Octuple the presents! | $10,000.52 | \n",
            "  4. Fish Painting | If something seems fishy about this painting, it's because it's a fish! Also hand-painted by trained monkeys! | $10,005.00 | \n",
            "  5. Dead Parrot | This is an ex-parrot! Or maybe he's only resting? | $0.50 | \n",
            "  ... ë° 1í–‰ ë”\n",
            "\n",
            "6ï¸âƒ£ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
            "----------------------------------------\n",
            "ğŸ”„ ìš”ì²­: http://www.pythonscraping.com/nonexistent-page\n",
            "âŒ HTTP 404 ì˜¤ë¥˜\n",
            "ğŸ”„ ìš”ì²­: http://this-domain-does-not-exist-12345.com\n",
            "ğŸ’¥ ìš”ì²­ ì˜¤ë¥˜: HTTPConnectionPool(host='this-domain-does-not-exist-12345.com', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDFC930CD0>: Failed to resolve 'this-domain-does-not-exist-12345.com' ([Errno 11001] getaddrinfo failed)\"))\n",
            "\n",
            "ğŸ¯ ì‹¤ì „ í¬ë¡¤ë§ í”„ë¡œì íŠ¸ ì™„ë£Œ!\n",
            "\n",
            "ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\n",
            "  - ìš”ì²­ ê°„ê²© ì¡°ì ˆë¡œ ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
            "  - ë‹¤ì–‘í•œ ì—ëŸ¬ ìƒí™©ì— ëŒ€í•œ ì ì ˆí•œ ì²˜ë¦¬\n",
            "  - êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ë° ì •ì œ\n",
            "  - JSONê³¼ HTML ë‘ ê°€ì§€ í˜•íƒœì˜ ë°ì´í„° ì²˜ë¦¬\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ ì‹¤ì „ í¬ë¡¤ë§ í”„ë¡œì íŠ¸\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "class WebCrawler:\n",
        "    \"\"\"ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤\"\"\"\n",
        "    \n",
        "    def __init__(self, delay=1):\n",
        "        self.session = requests.Session()\n",
        "        self.delay = delay  # ìš”ì²­ ê°„ê²© (ì´ˆ)\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "    \n",
        "    def fetch_page(self, url, timeout=10):\n",
        "        \"\"\"ì•ˆì „í•˜ê²Œ ì›¹í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì†Œë“œ\"\"\"\n",
        "        try:\n",
        "            print(f\"ğŸ”„ ìš”ì²­: {url}\")\n",
        "            response = self.session.get(url, timeout=timeout)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                print(f\"âœ… ì„±ê³µ (í¬ê¸°: {len(response.text):,} ë¬¸ì)\")\n",
        "                time.sleep(self.delay)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ\n",
        "                return response\n",
        "            else:\n",
        "                print(f\"âŒ HTTP {response.status_code} ì˜¤ë¥˜\")\n",
        "                return None\n",
        "                \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"ğŸ’¥ ìš”ì²­ ì˜¤ë¥˜: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def parse_html(self, html_content):\n",
        "        \"\"\"HTMLì„ íŒŒì‹±í•˜ì—¬ BeautifulSoup ê°ì²´ ë°˜í™˜\"\"\"\n",
        "        return BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "crawler = WebCrawler(delay=2)  # 2ì´ˆ ê°„ê²©\n",
        "\n",
        "print(\"ğŸ¯ ì‹¤ì „ í¬ë¡¤ë§ í”„ë¡œì íŠ¸ ì‹œì‘!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. ê°„ë‹¨í•œ HTML í˜ì´ì§€ í¬ë¡¤ë§\n",
        "print(\"\\n1ï¸âƒ£ ê¸°ë³¸ HTML í˜ì´ì§€ í¬ë¡¤ë§\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"http://www.pythonscraping.com/exercises/exercise1.html\")\n",
        "if response:\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    title = soup.find('title')\n",
        "    h1 = soup.find('h1')\n",
        "    \n",
        "    print(f\"ğŸ“„ í˜ì´ì§€ ì œëª©: {title.text if title else 'ì—†ìŒ'}\")\n",
        "    print(f\"ğŸ”¤ ë©”ì¸ í—¤ë”©: {h1.text if h1 else 'ì—†ìŒ'}\")\n",
        "\n",
        "# 2. JSON API ë°ì´í„° í¬ë¡¤ë§\n",
        "print(\"\\n2ï¸âƒ£ JSON API ë°ì´í„° ì²˜ë¦¬\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"https://httpbin.org/json\")\n",
        "if response:\n",
        "    try:\n",
        "        data = response.json()  # JSON íŒŒì‹±\n",
        "        print(\"ğŸ“Š JSON ë°ì´í„°:\")\n",
        "        print(json.dumps(data, indent=2, ensure_ascii=False))\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"âŒ JSON íŒŒì‹± ì‹¤íŒ¨\")\n",
        "\n",
        "# 3. ë³µì¡í•œ êµ¬ì¡°ì˜ í˜ì´ì§€ í¬ë¡¤ë§\n",
        "print(\"\\n3ï¸âƒ£ ë³µì¡í•œ HTML êµ¬ì¡° ë¶„ì„\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"http://www.pythonscraping.com/pages/warandpeace.html\")\n",
        "if response:\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    \n",
        "    # ì—¬ëŸ¬ ë°ì´í„° ì¶”ì¶œ\n",
        "    name_list = soup.find_all(\"span\", {\"class\": \"green\"})\n",
        "    print(f\"ğŸŸ¢ ë…¹ìƒ‰ í…ìŠ¤íŠ¸ ({len(name_list)}ê°œ):\")\n",
        "    for i, name in enumerate(name_list[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ\n",
        "        print(f\"  {i}. {name.text.strip()}\")\n",
        "    \n",
        "    if len(name_list) > 5:\n",
        "        print(f\"  ... ë° {len(name_list) - 5}ê°œ ë”\")\n",
        "\n",
        "# 4. ë§í¬ ìˆ˜ì§‘í•˜ê¸°\n",
        "print(\"\\n4ï¸âƒ£ í˜ì´ì§€ ë‚´ ë§í¬ ìˆ˜ì§‘\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = crawler.fetch_page(\"http://www.pythonscraping.com/pages/page3.html\")\n",
        "if response:\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    \n",
        "    # ëª¨ë“  ë§í¬ ìˆ˜ì§‘\n",
        "    links = soup.find_all('a', href=True)\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    \n",
        "    base_url = \"http://www.pythonscraping.com\"\n",
        "    \n",
        "    for link in links:\n",
        "        href = link['href']\n",
        "        full_url = urljoin(base_url, href)\n",
        "        link_text = link.text.strip()\n",
        "        \n",
        "        if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
        "            internal_links.append((link_text, full_url))\n",
        "        else:\n",
        "            external_links.append((link_text, full_url))\n",
        "    \n",
        "    print(f\"ğŸ”— ë‚´ë¶€ ë§í¬ ({len(internal_links)}ê°œ):\")\n",
        "    for text, url in internal_links[:3]:\n",
        "        print(f\"  ğŸ“„ {text}: {url}\")\n",
        "    \n",
        "    print(f\"\\nğŸŒ ì™¸ë¶€ ë§í¬ ({len(external_links)}ê°œ):\")\n",
        "    for text, url in external_links[:3]:\n",
        "        print(f\"  ğŸ”— {text}: {url}\")\n",
        "\n",
        "# 5. í‘œ(Table) ë°ì´í„° ì¶”ì¶œ\n",
        "print(\"\\n5ï¸âƒ£ í…Œì´ë¸” ë°ì´í„° êµ¬ì¡°í™”\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# í…Œì´ë¸”ì´ ìˆëŠ” í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
        "if response:  # ì´ì „ response ì¬ì‚¬ìš©\n",
        "    soup = crawler.parse_html(response.text)\n",
        "    \n",
        "    # í…Œì´ë¸” ì°¾ê¸°\n",
        "    try:\n",
        "        table = soup.find('table')\n",
        "        if table:\n",
        "            rows = table.find_all('tr')  # type: ignore\n",
        "            table_data = []\n",
        "            \n",
        "            for row in rows:\n",
        "                cells = row.find_all(['th', 'td'])  # type: ignore\n",
        "                row_data = [cell.text.strip() for cell in cells]\n",
        "                if row_data:  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ\n",
        "                    table_data.append(row_data)\n",
        "            \n",
        "            print(f\"ğŸ“Š í…Œì´ë¸” ë°ì´í„° ({len(table_data)}í–‰):\")\n",
        "            for i, row in enumerate(table_data[:5], 1):  # ì²˜ìŒ 5í–‰ë§Œ\n",
        "                print(f\"  {i}. {' | '.join(row)}\")\n",
        "            \n",
        "            if len(table_data) > 5:\n",
        "                print(f\"  ... ë° {len(table_data) - 5}í–‰ ë”\")\n",
        "        else:\n",
        "            print(\"ğŸ“Š í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸ“Š í…Œì´ë¸” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# 6. ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
        "print(\"\\n6ï¸âƒ£ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í˜ì´ì§€ ìš”ì²­\n",
        "error_response = crawler.fetch_page(\"http://www.pythonscraping.com/nonexistent-page\")\n",
        "\n",
        "# ì˜ëª»ëœ ë„ë©”ì¸ ìš”ì²­\n",
        "error_response2 = crawler.fetch_page(\"http://this-domain-does-not-exist-12345.com\")\n",
        "\n",
        "print(\"\\nğŸ¯ ì‹¤ì „ í¬ë¡¤ë§ í”„ë¡œì íŠ¸ ì™„ë£Œ!\")\n",
        "print(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\n",
        "print(\"  - ìš”ì²­ ê°„ê²© ì¡°ì ˆë¡œ ì„œë²„ ë¶€í•˜ ë°©ì§€\")\n",
        "print(\"  - ë‹¤ì–‘í•œ ì—ëŸ¬ ìƒí™©ì— ëŒ€í•œ ì ì ˆí•œ ì²˜ë¦¬\")\n",
        "print(\"  - êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ë° ì •ì œ\")\n",
        "print(\"  - JSONê³¼ HTML ë‘ ê°€ì§€ í˜•íƒœì˜ ë°ì´í„° ì²˜ë¦¬\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. âš ï¸ ì—ëŸ¬ ì²˜ë¦¬ì™€ ì˜ˆì™¸ ìƒí™© ëŒ€ì‘\n",
        "\n",
        "### ì›¹ í¬ë¡¤ë§ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤\n",
        "\n",
        "ì›¹ í¬ë¡¤ë§ì€ ë‹¤ì–‘í•œ ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ë“¤ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì•ˆì •ì ì¸ í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ì ì ˆí•œ ì—ëŸ¬ ì²˜ë¦¬ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "#### ì£¼ìš” ì˜ˆì™¸ ìƒí™©ë“¤:\n",
        "- **ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜**: ì—°ê²° ì‹¤íŒ¨, íƒ€ì„ì•„ì›ƒ\n",
        "- **HTTP ì˜¤ë¥˜**: 404, 403, 500 ë“±\n",
        "- **íŒŒì‹± ì˜¤ë¥˜**: ì˜ˆìƒê³¼ ë‹¤ë¥¸ HTML êµ¬ì¡°\n",
        "- **ì¸ì½”ë”© ë¬¸ì œ**: ë¬¸ì ê¹¨ì§\n",
        "- **ë™ì  ì½˜í…ì¸ **: JavaScriptë¡œ ìƒì„±ë˜ëŠ” ë‚´ìš©\n",
        "- **ë´‡ ì°¨ë‹¨**: CAPTCHA, IP ì°¨ë‹¨\n",
        "\n",
        "#### ë°©ì–´ì  í”„ë¡œê·¸ë˜ë° ì›ì¹™:\n",
        "1. **í•­ìƒ ì˜ˆì™¸ ì²˜ë¦¬**: try-except ë¸”ë¡ ì‚¬ìš©\n",
        "2. **ë°ì´í„° ê²€ì¦**: ì˜ˆìƒí•œ í˜•íƒœì˜ ë°ì´í„°ì¸ì§€ í™•ì¸\n",
        "3. **ì•ˆì „í•œ ì ‘ê·¼**: None ì²´í¬, ë°°ì—´ ì¸ë±ìŠ¤ ê²€ì¦\n",
        "4. **ì¬ì‹œë„ ë¡œì§**: ì¼ì‹œì  ì˜¤ë¥˜ì— ëŒ€í•œ ì¬ì‹œë„\n",
        "5. **ë¡œê¹…**: ë¬¸ì œ ìƒí™© ê¸°ë¡ ë° ë””ë²„ê¹…\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. âš–ï¸ í¬ë¡¤ë§ ìœ¤ë¦¬ì™€ ë²•ì  ì£¼ì˜ì‚¬í•­\n",
        "\n",
        "### ğŸš¨ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ìœ¤ë¦¬ì  ì›ì¹™\n",
        "\n",
        "ì›¹ í¬ë¡¤ë§ì€ ê°•ë ¥í•œ ë„êµ¬ì´ì§€ë§Œ, ì±…ì„ê° ìˆê²Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "#### ë²•ì  ì¤€ìˆ˜ì‚¬í•­:\n",
        "- **ì €ì‘ê¶Œ ë³´í˜¸**: ì½˜í…ì¸ ì˜ ì €ì‘ê¶Œ ì¡´ì¤‘\n",
        "- **ê°œì¸ì •ë³´ ë³´í˜¸**: GDPR, ê°œì¸ì •ë³´ë³´í˜¸ë²• ì¤€ìˆ˜\n",
        "- **ì´ìš©ì•½ê´€ í™•ì¸**: ì›¹ì‚¬ì´íŠ¸ì˜ Terms of Service ê²€í† \n",
        "- **robots.txt ì¤€ìˆ˜**: ì‚¬ì´íŠ¸ì˜ í¬ë¡¤ë§ ì •ì±… í™•ì¸\n",
        "\n",
        "#### ê¸°ìˆ ì  ìœ¤ë¦¬:\n",
        "- **ì„œë²„ ë¶€í•˜ ìµœì†Œí™”**: ì ì ˆí•œ ìš”ì²­ ê°„ê²© ìœ ì§€\n",
        "- **ëŒ€ì—­í­ ê³ ë ¤**: ë¶ˆí•„ìš”í•œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê¸ˆì§€\n",
        "- **User-Agent ëª…ì‹œ**: ë³¸ì¸ì˜ ì •ì²´ë¥¼ ìˆ¨ê¸°ì§€ ì•Šê¸°\n",
        "- **ì¬ì‹œë„ ì œí•œ**: ë¬´í•œ ì¬ì‹œë„ë¡œ ì„œë²„ ê³µê²©í•˜ì§€ ì•Šê¸°\n",
        "\n",
        "#### robots.txt í™•ì¸ ë°©ë²•\n",
        "```python\n",
        "# robots.txt í™•ì¸ ì˜ˆì œ\n",
        "import requests\n",
        "\n",
        "def check_robots_txt(domain):\n",
        "    robots_url = f\"{domain}/robots.txt\"\n",
        "    try:\n",
        "        response = requests.get(robots_url)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"ğŸ¤– {domain}ì˜ robots.txt:\")\n",
        "            print(response.text)\n",
        "        else:\n",
        "            print(f\"âŒ robots.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸ’¥ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# ì˜ˆì‹œ: check_robots_txt(\"http://www.example.com\")\n",
        "```\n",
        "\n",
        "### ğŸ’¡ ì±…ì„ê° ìˆëŠ” í¬ë¡¤ë§ì„ ìœ„í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "âœ… **ë²•ì  ê²€í† **\n",
        "- [ ] ëŒ€ìƒ ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ í™•ì¸\n",
        "- [ ] robots.txt ì •ì±… ì¤€ìˆ˜\n",
        "- [ ] ê°œì¸ì •ë³´ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
        "- [ ] ì €ì‘ê¶Œ ì¹¨í•´ ê°€ëŠ¥ì„± ê²€í† \n",
        "\n",
        "âœ… **ê¸°ìˆ ì  ê³ ë ¤**\n",
        "- [ ] ì ì ˆí•œ ìš”ì²­ ê°„ê²© ì„¤ì • (ìµœì†Œ 1ì´ˆ)\n",
        "- [ ] User-Agent í—¤ë” ëª…ì‹œ\n",
        "- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„\n",
        "- [ ] ì„œë²„ ì‘ë‹µ ì½”ë“œ í™•ì¸\n",
        "\n",
        "âœ… **ë°ì´í„° ì‚¬ìš©**\n",
        "- [ ] ìˆ˜ì§‘ ëª©ì ì— ë§ëŠ” ìµœì†Œí•œì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ\n",
        "- [ ] ê°œì¸ì •ë³´ ì²˜ë¦¬ ì‹œ ì ì ˆí•œ ë³´ì•ˆ ì¡°ì¹˜\n",
        "- [ ] ë°ì´í„° ë³´ê´€ ê¸°ê°„ ë° íŒŒê¸° ì •ì±… ìˆ˜ë¦½\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. ğŸ“ ì¢…í•© ì •ë¦¬ ë° ì‹¤ì „ í”„ë¡œì íŠ¸ ì œì•ˆ\n",
        "\n",
        "### ğŸ† í•™ìŠµ ì™„ë£Œ! ì¶•í•˜í•©ë‹ˆë‹¤!\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ ì›¹ í¬ë¡¤ë§ì˜ ì „ì²´ ê³¼ì •ì„ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ“š í•™ìŠµí•œ í•µì‹¬ ë‚´ìš©\n",
        "\n",
        "#### 1. ê¸°ì´ˆ ê°œë…\n",
        "- âœ… HTTP í†µì‹ ê³¼ ì›¹ í¬ë¡¤ë§ì˜ ì›ë¦¬\n",
        "- âœ… requests ëª¨ë“ˆì„ ì´ìš©í•œ ì›¹í˜ì´ì§€ ìš”ì²­\n",
        "- âœ… ìƒíƒœ ì½”ë“œì™€ ì—ëŸ¬ ì²˜ë¦¬\n",
        "\n",
        "#### 2. HTML íŒŒì‹±\n",
        "- âœ… BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë²•\n",
        "- âœ… find(), find_all(), select() ë©”ì†Œë“œ\n",
        "- âœ… CSS ì„ íƒìë¥¼ ì´ìš©í•œ ì •êµí•œ ìš”ì†Œ ì„ íƒ\n",
        "\n",
        "#### 3. ê³ ê¸‰ ê¸°ë²•\n",
        "- âœ… ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ ë°ì´í„° ì •ì œ\n",
        "- âœ… êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ë° ë¶„ì„\n",
        "- âœ… ì„¸ì…˜ ê´€ë¦¬ì™€ í—¤ë” ì„¤ì •\n",
        "\n",
        "#### 4. ì‹¤ì „ ì‘ìš©\n",
        "- âœ… ì•ˆì „í•œ í¬ë¡¤ë§ í´ë˜ìŠ¤ êµ¬í˜„\n",
        "- âœ… JSONê³¼ HTML ë°ì´í„° ì²˜ë¦¬\n",
        "- âœ… ë§í¬ ìˆ˜ì§‘ ë° í˜ì´ì§€ íƒìƒ‰\n",
        "\n",
        "#### 5. ìœ¤ë¦¬ì™€ ë²•ì  ê³ ë ¤\n",
        "- âœ… robots.txt í™•ì¸ ë°©ë²•\n",
        "- âœ… ì±…ì„ê° ìˆëŠ” í¬ë¡¤ë§ ì›ì¹™\n",
        "- âœ… ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ì €ì‘ê¶Œ ì¤€ìˆ˜\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì „ í”„ë¡œì íŠ¸ ì œì•ˆ\n",
        "\n",
        "ì´ì œ ë°°ìš´ ì§€ì‹ì„ í™œìš©í•´ ì‹¤ì œ í”„ë¡œì íŠ¸ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”!\n",
        "\n",
        "#### ğŸŒŸ ì´ˆê¸‰ í”„ë¡œì íŠ¸\n",
        "1. **ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ê¸°**\n",
        "   - ê¸°ìƒì²­ ë˜ëŠ” ë‚ ì”¨ ì‚¬ì´íŠ¸ì—ì„œ ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´ ì¶”ì¶œ\n",
        "   - ì˜¨ë„, ìŠµë„, ê°•ìˆ˜ëŸ‰ ë“±ì„ ì •ë¦¬í•´ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\n",
        "\n",
        "2. **ë‰´ìŠ¤ ì œëª© ëª¨ìŒì§‘**\n",
        "   - ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ ì œëª©ë“¤ ìˆ˜ì§‘\n",
        "   - ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ì •ë¦¬\n",
        "\n",
        "3. **ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ê°€ê²© ë¹„êµ**\n",
        "   - íŠ¹ì • ìƒí’ˆì˜ ê°€ê²©ì„ ì—¬ëŸ¬ ì‡¼í•‘ëª°ì—ì„œ ìˆ˜ì§‘\n",
        "   - ìµœì €ê°€ì™€ í‰ê· ê°€ ê³„ì‚°\n",
        "\n",
        "#### ğŸ”¥ ì¤‘ê¸‰ í”„ë¡œì íŠ¸\n",
        "4. **ë¶€ë™ì‚° ì‹œì„¸ ë¶„ì„ê¸°**\n",
        "   - ë¶€ë™ì‚° ì •ë³´ ì‚¬ì´íŠ¸ì—ì„œ ì§€ì—­ë³„ ë§¤ë§¤ê°€ ì •ë³´ ìˆ˜ì§‘\n",
        "   - ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜í•˜ì—¬ íŠ¸ë Œë“œ ë¶„ì„\n",
        "\n",
        "5. **ì±„ìš© ì •ë³´ ëª¨ë‹ˆí„°ë§**\n",
        "   - ì±„ìš© ì‚¬ì´íŠ¸ì—ì„œ íŠ¹ì • ì§ë¬´ì˜ ê³µê³  ì •ë³´ ìˆ˜ì§‘\n",
        "   - ìš”êµ¬ ìŠ¤í‚¬, ì—°ë´‰ ë²”ìœ„ ë“±ì„ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ\n",
        "\n",
        "6. **ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„**\n",
        "   - ê³µê°œ APIë¥¼ í™œìš©í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘\n",
        "   - ì‹œê°„ëŒ€ë³„, ì§€ì—­ë³„ ì¸ê¸° í† í”½ ë¶„ì„\n",
        "\n",
        "#### ğŸš€ ê³ ê¸‰ í”„ë¡œì íŠ¸\n",
        "7. **í•™ìˆ  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘**\n",
        "   - arXiv, Google Scholar ë“±ì—ì„œ íŠ¹ì • ë¶„ì•¼ ë…¼ë¬¸ ì •ë³´ ìˆ˜ì§‘\n",
        "   - ì €ì ë„¤íŠ¸ì›Œí¬, ì¸ìš© ê´€ê³„ ë¶„ì„\n",
        "\n",
        "8. **ì „ììƒê±°ë˜ ë¦¬ë·° ë¶„ì„**\n",
        "   - ì œí’ˆ ë¦¬ë·° ë°ì´í„° ëŒ€ëŸ‰ ìˆ˜ì§‘\n",
        "   - ê°ì • ë¶„ì„ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œì„ í†µí•œ ì œí’ˆ í‰ê°€\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ› ï¸ ì¶”ê°€ í•™ìŠµ ë¡œë“œë§µ\n",
        "\n",
        "#### ë‹¤ìŒì— ë°°ìš¸ ê³ ê¸‰ ê¸°ìˆ ë“¤:\n",
        "\n",
        "1. **Selenium WebDriver**\n",
        "   - JavaScript ë Œë”ë§ì´ í•„ìš”í•œ ë™ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§\n",
        "   - ë¸Œë¼ìš°ì € ìë™í™”ë¥¼ í†µí•œ ë³µì¡í•œ ìƒí˜¸ì‘ìš©\n",
        "\n",
        "2. **Scrapy Framework**\n",
        "   - ëŒ€ê·œëª¨ í¬ë¡¤ë§ì„ ìœ„í•œ ì „ë¬¸ í”„ë ˆì„ì›Œí¬\n",
        "   - ë¶„ì‚° í¬ë¡¤ë§ê³¼ ë°ì´í„° íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "3. **ë°ì´í„° ì €ì¥ ë° ì²˜ë¦¬**\n",
        "   - ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (MySQL, MongoDB)\n",
        "   - ë°ì´í„° ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ (pandas, numpy)\n",
        "\n",
        "4. **ì„±ëŠ¥ ìµœì í™”**\n",
        "   - ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° (asyncio, aiohttp)\n",
        "   - ë©€í‹°ìŠ¤ë ˆë”©ê³¼ ë©€í‹°í”„ë¡œì„¸ì‹±\n",
        "\n",
        "5. **í´ë¼ìš°ë“œ ë°°í¬**\n",
        "   - AWS, GCPë¥¼ ì´ìš©í•œ í¬ë¡¤ëŸ¬ ë°°í¬\n",
        "   - ìŠ¤ì¼€ì¤„ë§ê³¼ ëª¨ë‹ˆí„°ë§\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’ª ë§ˆë¬´ë¦¬ ë©”ì‹œì§€\n",
        "\n",
        "ì›¹ í¬ë¡¤ë§ì€ ë°ì´í„° ê³¼í•™, ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤, ë§ˆì¼€íŒ… ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ëŠ” ì‹¤ìš©ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤. \n",
        "\n",
        "**ê¸°ì–µí•˜ì„¸ìš”:**\n",
        "- í•­ìƒ ìœ¤ë¦¬ì ì´ê³  ë²•ì ì¸ ê¸°ì¤€ì„ ì¤€ìˆ˜í•˜ì„¸ìš”\n",
        "- ì‘ì€ í”„ë¡œì íŠ¸ë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ë³µì¡í•œ ê²ƒì— ë„ì „í•˜ì„¸ìš”\n",
        "- ì‹¤íŒ¨ë¥¼ ë‘ë ¤ì›Œí•˜ì§€ ë§ê³ , ë¬¸ì œ í•´ê²° ê³¼ì •ì—ì„œ ë°°ìš°ì„¸ìš”\n",
        "- ì»¤ë®¤ë‹ˆí‹°ì™€ í•¨ê»˜ ì§€ì‹ì„ ê³µìœ í•˜ê³  ì„±ì¥í•˜ì„¸ìš”\n",
        "\n",
        "**ğŸ¯ ì—¬ëŸ¬ë¶„ì˜ ì²« ë²ˆì§¸ í¬ë¡¤ë§ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!**\n",
        "\n",
        "---\n",
        "\n",
        "*\"ë°ì´í„°ëŠ” 21ì„¸ê¸°ì˜ ì„ìœ ë‹¤. í•˜ì§€ë§Œ ì„ìœ ì™€ ë‹¬ë¦¬ ë°ì´í„°ëŠ” ì‚¬ìš©í• ìˆ˜ë¡ ë” ê°€ì¹˜ìˆì–´ì§„ë‹¤.\"*\n",
        "\n",
        "**Happy Crawling! ğŸ•·ï¸âœ¨**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sesac_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
